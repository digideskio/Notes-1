CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Schrödinger's Equation -- Feb 9, 2012
-------------------------------------

Had the sense last time that some of you might not remember all of your
linear algebra. So this time is a hybrid of linear algebra and getting you
up to speed with this bra-ket notation.

What we are going to be doing is looking at it from three or four different
viewpoints to try to get an intuition for why it is the way it is. So when
we get around to trying to solve the SE for specific equations, it's not
just an equation; you have a feel for it.

Goal for today: Figure out why does the Hamiltonian plays a role in
Schrödinger's equation.

So basically, the way we are going about this is that last time, we had a
rather abstract formulation of Schrödinger's equation. Why? It's because
the formulation is so clean. General form: write out hamiltonian,
diagonalize it, and once you understand the eigenvalues and eigenvectors,
you understand why it has the form it does.

So why does it look the way it does? Conservation laws.

Next week, we'll look at it for concrete systems; for continuous systems;
the behavior of an unstrained particle. In each of these cases, we're
trying to build an intuition as to _why_ the Schrödinger equation is the
way it is.

Not going to get into time-dependent hamiltonians until maybe the end of
the semester.

Do Hamiltonians correspond to quantum circuits? The way you implement gates
is by implementing a suitable Hamiltonian. But a quantum circuit
corresponds to a time-varying Hamiltonian. Topics we'll get to closer to
the end of the semester.

What we are starting with are the basics of quantum mechanics. Viewing it
in the case of discrete systems, i.e. qubits. We've already started with
quantum gates, quantum circuits.

We're going back and forth between this abstract version which is very
close to axiomatic quantum theory (but also helps with the understanding of
theory), and physical systems (hamiltonians). After a few weeks of physical
systems, we'll start talking about quantum algorithms.

And then, after we have developed that for a little while, how do you
implement all this in a lab? We'll go back and look at the physics of it.

We're sort of walking this fine line between thinking about quantum devices
as abstract entities; where all you need to know is the axioms of quantum
mechanics; thinking about what you can and cannot do, and what you have to
do to make it all happen.

So let's start with the basics. What I'll do today is I'll describe in a
little more detail Dirac's bra-ket notation. We've already seen this
notation to some extent, but let's do this more systematically.

Remember if you have a k-state quantum system, then its state is described
by a unit vector in a k-dimensional Hilbert space. This is also
equivalently described in ket notation as $\alpha_j\ket{j}$. We love this
notation because it simultaneously highlights two aspects: this is a
vector, and it is information. For example, if $k=2$, this is a qubit storing
a bit of information.

The dual space (row space) of the state $\ket{\psi}$ is the bra
$\bra{\psi}$ (hermitian conjugate). The inner product (square of the length
of the vector) of two states, therefore, is simply $\braket{\phi}{\psi}$

People who love the bra-ket notation love it because you don't have to
think. You just do what seems right and everything magically works out.

So if you have a vector $\ket{\psi}$, you can talk about the projection
operator projecting onto $\ket{\psi}$. It's a linear operator. What you
want to do is design the projection operator onto $\psi$ (often denoted by
$P$) $\equiv \op{\psi}$.

$P^n$ should $\equiv P$, for obvious reasons. $\ket{\psi}
\braket{\psi}{\psi} \bra{\psi}$: $\braket{\psi}{\psi} = 1$, so multiple
applications of an operator are equivalent to a single application. ($P$ is
a projection operator, and projections are idempotent: all eigenvalues are
either 1 or 0)

Suppose $\ket{\psi}=\ket{0}$. What does P look like as a matrix?

$$
\begin{pmatrix}
1 & \hdots & 0 \\
\vdots & \ddots & \vdots\\
0 & \hdots & 0\\
\end{pmatrix}
$$

$I = \sum \ket{j}$, therefore. It doesn't have to be in terms of the
standard basis. You could write down the identity in terms of any basis in
this way. Physics refers to this as the "resolution of the identity".

Example
-------
Suppose you have a vector and you want to measure it in a general basis.

What happens if we measure $\ket{\psi}$ in the $\ket{v}, \ket{v^\perp}$
basis? Do a change of basis on $\ket{\psi}$. Project $\ket{\psi}$ onto each
of the basis vectors. This is one way of doing it.

Observables
===========

An observable is a hermitian matrix $M$ (hermitian means that $M =
M^\dag$). So now we have something called the spectral theorem, which says
that hermitian matrices can be diagonalized: you can write them in a
different basis (the eigenbasis), and you can write them out in this
eigenbasis.

Suppose $M = X$ (bit-flip). Assume we have some vector $v$ such that $Xv =
\lambda v$ for some value of $\lambda$ (i.e. an eigenvector of
$X$). $(X-\lambda I)v = 0$, so $\det(X-\lambda I) = 0$. Solve for
$\lambda$, which are your eigenvalues, and then we go back and solve for
our eigenvectors.

Here, we're going to do this by inspection. Eigenvectors of X would be
$\ket{+}, \ket{-}$; the corresponding eigenvalues are $1$, $-1$.

Why is this an observable? If you were to create the right detector, we'd
observe something. We'd measure something. What we read out on the meter is
$\lambda\ket{j}$ with probability equal to $\alpha_j^2$, and the new state
is $\ket{\Psi_j}$. What Schrödinger's equation tells us is that if you look
at the energy operator $H$, and then in order to solve this differential
equation, we need to look at it in its eigenbasis. It was not supposed to
be so frightening. You can write $U(t)$ notationally as $e^{-iHt/ℏ}$.

Why $H$?
--------

Why should Schrödinger's equation involve the Hamiltonian? Why the energy
operator? What's so special about energy? Here's the reasoning: from axiom
3 of quantum mechanics, which says unitary evolution, what we showed was
the unitary transformation is $e^{-iHt/\hbar}$. Any unitary transformation
can be written in this form. You can always write it in the form $e^{iM}$
for some Hermitian matrix $M$. The only question is, what should M be? Why
should $M$ be the energy function? The second thing that turns out (either
something that we'll go through in class or have as an assignment) --
suppose that $M$ is a driving force of Schrödinger's equation. So
$\pderiv{\Psi}{t} = M\ket{\Psi}$.

Suppose there were some observable quantity $A$ that is conserved: i.e. if
you start out with a measurement of $\ket{\psi(0)}$, and you do the same
measurement at time $t$, if $A$ is a conserved value, then this expected
value should be the same both times. If $A$ is conserved, then $AM =
MA$. $A$ has to commute with $M$. This tells us that their eigenvectors are
the same. The fact that energy is conserved, therefore, says that $HM =
MH$. Then we have one last bit: energy is very special. It is very
fundamental; these are the building blocks; it goes right to the core. $H$
commutes with $M$ not for special conditions of the system, but rather
generally.

So you can reason that $M$ is a function of $H$. And then you show that it
must be a linear function of $H$ (i.e. $aH + b$), $b$ must be 0, and $a$
must be a constant (in fact, $\hbar$).

Symmetry plays a very strong role in the way this comes about.
