CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Introduction -- January 17, 2012
--------------------------------

Course Information
-------
* Announcements on website
* Try Piazza for questions.
* GSIs:
  + Dylan Gorman
	- dgorman@berkeley.edu
  + Seung Woo Shin
	- lagnared@gmail.com
* Lecture notes on website. No textbook.
* Prerequisites: Math 54, Physics 7A/B, 7C or CS 70.
* Work and Grading:
  + Weekly homework
	- Out Tue, due following Mon @ 5 in dropbox (second-floor Soda).
  + 2 Midterms: 14 Feb, 22 Mar.
  + Final Project
  + In-class quizzes
  + Academic integrity policy
Today
-----
* What is quantum computation?
* What is this course?
* Double-slit experiment

What is Quantum Computation?
-------
* Computers based on quantum mechanics can solve certain problems
  exponentially faster than classical computers, e.g. factoring
  (Shor's algorithm).

* How to design quantum algorithms?
  + Requires different methodology than for classical algorithms
* Are there limits to what quantum computers can do? (Probably. Is not
  known to automatically solve NP-complete problems. Also, halting
  problem.)
* How to implement quantum computers in the laboratory (AQC, among
  other forms).
  + Can you design them so they're scalable?

Quantum computation starts with this realization that if we were to
base our computers on QM rather than classical physics, then they can
be exponentially more powerful.

This was really a big deal because it was believed that it didn't
really matter how you implemented computers; all that you could do was
make each step faster.

The fact that there's something like quantum computers that can be
exponentially faster, this was really a big surprise. And really on
fundamental problems, like factoring.

What this course will focus on is several questions on quantum computers.

Where we are for quantum computers is sort of where computers were
60-70 years ago.
* Size -- room full of equipment
* Reliability -- not very much so
* Limited applications

Ion traps.
------
Can trap a small handful of ions, small number of qubits. No
fundamental obstacle scaling to ~40 qubits over next two years.

Entanglement
------
Basic resource in quantum mechanics. Unique aspect of QM, and one
fundamental to quantum computing

Quantum Teleportation
------
Entanglement.

Quantum Cryptography
------
Ways to use QM to communicate securely (still safe even with Shor's).

This course
------
* Introduction to QM in the language of qubits and quantum gates.
* Emphasis on paradoxes, entanglement.
* Quantum algorithms.
* Quantum cryptography.
* Implementing qubits in the laboratory -- spin...

There are certain difficulties you can sweep away by focusing on it in
this language. It also highlights certain aspects of QM. Interesting
to focus on these aspects because they lend an alternative
interpretation of QM.

Aside:
------
There will not be programming projects. There will be a couple of
guest lectures. (not clear it will happen) would try to set things up
so we could go and play with equipment in lab. This obviously depends
on whether it scales and is set up well enough. Might be in place by
the end of the semester.

One thing that has to be done is arrange discussion sections. (under
discussion. Looks like a tentative Wed 11-12 and Fri 1-2.)

INTERIM.

Young's double-slit experiment.
----------
(particle-wave duality at quantum level. Physics completely
different. So different that it defies reason.)

There are two aspects of dealing with QM: understanding what those
rules are, and believing that nature works that way.

Hopefully you'll suspend your disbelief and just go with understanding
what the rules are.

(blah, more particle-wave duality)

(this basically boils down to interference.)

(tracking which slit each particle goes through leads to a collapse of
 the wavefunction, and we observe particles behaving like particles,
 not waves)

(talk about superposition of states; introduction of the wavefunction.
 Explained entirely by Schrödinger's cat.)

The thing that's most troubling about this from actual experience as
well as physics is that there has to be a mechanism. How did nature do
this? We are going to have a completely precise description. But it's
not going to be a mechanism unlike anything else.

Part of understanding QM is coming to terms psychologically with this
superposition of states, the existence in more than one state
simultaneously.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Qubits, Superposition, & Measurement -- January 19, 2012
--------------------------------------------------------
Quantization:
-------------
 * Atomic orbitals: Electrons within an atom exist in quantized energy
   levels. Qualitatively -- resonating standing waves. (We have Bohr to
   thank for this)
   + The important thing is that there is this quantization, and you can
	 choose to think of the ground/excited state of an electron as encoding
	 in binary. Or you can have a k-level system, where you have energy
	 levels 0 through k-1. These are the things we'll be thinking about
	 notationally.
   + There are other systems we can think of as a two-level system,
	 e.g. photons (in terms of polarization, e.g.)
	 - spin (very roughly magnetic moment associated with the charge)
 * These are very rough descriptions. For our purpose, you can think about
   k-level systems, where you have k discrete levels.

Superposition
-------------
The first axiom of quantum mechanics is the superposition principle. It
says that if a system can exist in one of two states, then it can also
exist in a linear superposition of said states. Likewise, if it can exist
in k states, it can also exist in a superposition of all k states (trivial
to prove)

In other words, $\alpha_1\ket{0} + \alpha_1\ket{1} + ... +
\alpha_{k-1}\ket{k-1}$

Our $\alpha _i$ are actually probability amplitudes. These don't sum to
one; rather the magnitudes of their intensities sum to one.

(some talk about normalization in order to satisfy said property)

What does this superposition principle correspond to? We talked about the
double-slit experiment. Electron/photon going through one of two slits,
probability of which slit corresponds to these.

Measurement
-----------
The second axiom of quantum mechanics is the measurement axiom. The
superposition is the private world of this system. As long as you're not
looking at it, it's going to be in this linear superposition. But as soon
as you make a measurement, the outcome of the measurement is one of these
levels $\ket{j}$ with probability $\abs{\alpha _j}^2$ (i.e. you collapse
the wave function).

So if you go back to our last example, there we have a qubit (a quantum
bit) -- a two-level system is called a quantum bit because it's a quantum
analogue of a bit. So we had an example where we had a superposition of two
states. (demonstration of the probabilities)

[ talk about how attempting to detect which of the slits the particle went
  through actually constitutes a measurement, which changes the state of
  the system ]

  standard basis measurement:
	 checking exactly which state the system is in.

Another way of writing the state of a quantum system (as opposed to bra-ket
notation) is just saying that it's k complex numbers and presenting them as
a vector. Should be immediately intuitive. We still have the same condition
that the summation of $\alpha _i^2 = 1$. Our vector, therefore, must sit on
the unit k-sphere in k-space.

Ket notation: invented by Dirac. The reason we are going to be so enamored
by the ket notation is that it simultaneously expreses 1) the quantum state
is a vector in a vector space and 2) this quantum state encodes
information. The fact that we are labeling our states as $\ket{0}$ and
$\ket{1}$ is indicative in itself that we are encoding information.

Two ways of rephrasing the probability of landing in a particular state,
therefore is 1) the length of the projection onto said basis vector and 2)
$cos^2θ_j$.

Generalization of the notion of measurement: in general, when you do a
measurement, you don't need to pick the standard basis; you can pick any
orthonormal basis.

There is another useful basis called the sign basis: $\ket{+}$ and
$\ket{-}$. If placed on the unit circle, we have $\ket{+}$ located at
$\theta=\frac{\pi}{4}$ and $\ket{-}$ located at $\theta=-\frac{\pi}{4}$.

[ change of basis can be done using matrices or using substitution. ]

Significance of sign basis
--------------------------

The standard basis is going to correspond to a certain quantity. The sign
basis will correspond to a different quantity. By analogy, let's assume
that we're measuring the position or momentum of the system depending on
which basis we're using (might as well be -- Heisenberg's uncertainty
principle and all).

 explanation of Heisenberg's uncertainty principle, except without actually
  attributing a name to it. basically, with two related quantities, the
  more accurately you know one, the less accurately you can know the other.

 maximal uncertainty occurs with conjugate basis. measure of uncertainty:
  spread -- $\abs{\alpha_0} + \abs{\alpha_1}$. Corresponds to maximum
  uncertainty. Uncertainty ranges from 1 to $\sqrt{2}$.

Specifically, Heisenberg's uncertainty principle: $S(\alpha_0) + S(\beta_0)
\ge \sqrt{2}$. ($\alpha_0$, $\beta_0$ correspond to a conjugate basis)
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Multiple-qubit Systems -- January 24, 2012
------------------------------------------
Snafu with the projections, so lecture will be on the whiteboard! Will
also stop early, unfortunately.

State of a single qubit is a superposition of various states (cosθ|0〉
+ sinθ|1〉). measurement has effect of collapsing the superposition
(WAVE FUNCTION).

(hydrogen atom: electron can be in ground state or excited state.)

Now we study two qubits!

TWO QUBITS
==========
Now you have two such particles, and we want to describe their joint
state, what that state looks like. Classically, this can be one of
four states. So quantumly, it is in a superposition of these four
states. Our |ψ〉, then, is α₀₀|00〉 + α₀₁|01〉 + α₁₀|10〉 + α₁₁|11〉.
Collapse of the wavefunction occurs in exactly the same manner.

Probability first qubit is 0: |α₀₀|² + |α₀₁|². New state is a
renormalization of the remaining states.

ENTANGLEMENT
============
First, let me show you what it means for two qubits not to be
entangled. Essentially, we have conditional independence.

Quantum mechanics tells us that this is a very rare event (i.e. it
almost never happens).

Bell State
----------
You have two qubits in the state (1/√2)|00〉 + (1/√2)|11〉. Impossible
to factor, so we must have some sort of dependence occurring. Neither
of the two qubits has a definite state. All you can say is that the
two qubits together are in a certain state.

Rotational invariants of Bell states -- maximally entangled in all
orthogonal bases.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Entanglement, EPR, Bell's Experiment -- January 26, 2012
--------------------------------------------------------

Ignore Q4 on HW2. Probably.

Entanglement
============

Last time, we saw a Bell state (a Bell basis state). This is a state
of two particles (i.e. two qubits), and the state of each of the
qubits is "entangled", so to speak, with the other of the qubits.

This state has a very curious property, as we saw last time: maximally
entangled qubits will remain maximally entangled regardless of the
choice of bases, as long as they are orthogonal. This is known as the
ROTATIONAL INVARIANCE OF BELL STATES.

For the bell basis state α₀₀|00〉 + α₁₁|11〉, we have rotational
invariance for _real_ rotations only. Certain bell bases additionally
have rotational invariance over complex rotations.

FACT 1:

	You get the same outcome if measure both in the v, v⊥ basis.

FACT 2:

	Independent of separation between particles. It's not because
	the particles are close to each other and talking to each
	other; it's because of their state.

Einsten, Podolsky, & Rosen '35:

	Imagine that you have a pair of particles that are emitted
	(e.g. electron, positron) that are highly entangled. They are
	emitted in opposite directions and travel far from each other. And
	then you measure Particle 1 in bit (0/1) basis ⇒ knowledge of the
	bit on the other particle. Also, measure Particle 2 in the sign
	basis ⇒ knowledge of the sign on the first particle. Contradicts
	uncertainty principle?

	Not at all. { sign information destroyed, etc. } Sign information
	measured in the second particle has nothing to do with that of the
	first particle, since measuring the bit information destroyed the
	sign information (i.e. is now entirely unknown). { more questions
	regarding the Einstein/deterministic interpretation of quantum
	mechanics. }

Bell '64:

	Take two entangled particles. Despite large separation distance,
	they are quantumly connected. What you can do is start playing
	with the notion of measuring the particles in arbitrary
	bases.

	Make one measurement with outcome u. You'd have |v〉 and |v⊥〉
	with probabilities cos²θ and sin²θ.

	Bell's idea was this: Surely, if you play with your choice of u
	and v, you're going to get something good.

	We have some input, 0 or 1, that tells us which basis to
	pick. Suppose there are two experimentalists who have these
	entangled pairs of qubits. At the last minute, Alice gets as input
	some random bit; likewise, Bob gets some other random bit.

	We want to know the two output bits.

	Goal: maximize probability that r{a}r{b} = a + b (mod 2) = a ⊕ b

		If either of the inputs is a zero, they want to output the same
		bit. But if both of the inputs are one, they want to output
		opposite bits.

	Fact: If you choose the correct angles, in the quantum world, you
	get a success probability of cos²(π/8) ≈ 0.85.

	Claim: no way to do better than 3/4, if you agree to say the same
	thing in advance. (Local) hidden variable theory ≤ 0.75. Impossible
	to do better.

	However: Quantum mechanics gives us a success rate of ≈ 0.853, or
	cos²(π/8).

Alice's protocol is as follows: if r{a} = 0, measure in basis rotated
↻ π/16.  if r{a} = 1, measure in basis rotated ↺ 3π/16.

Bob protocol is as follows: if r{b} = 0, measure in basis rotated
↺ π/16.  if r{b} = 1, measure in basis rotated ↻ 3π/16.

{ where did these angles come from? If you plot them on the number
  line, you get four points a₁, b₀, a₀, b₁. When either is zero, we
  have a distance of π/8, else we have a distance of 3π/8. }

For the cases where (a ⊕ b), we have probability cos²(π/8). for the
case where !(a ⊕ b), we have probability sin²(3π/8) = cos²(π/8).

Conclusively disproves Einstein's hidden-variable theory.

There's this remarkable aspect where over time you can refine these
concepts to the point that we can sit down in an hour and a half to
understand these concepts that Einstein would have given anything to
understand. Isn't this remarkable?

When you actually do these experiments, they turn out to refute the
entire plan Einstein had.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Quantum Gates -- January 31, 2012
---------------------------------

GATES, MORE GATES.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Revisiting Axioms of Quantum Mechanics -- Feb 2, 2012
-----------------------------------------------------

We're going to be revisiting, over the next few lectures, the axioms
of quantum mechanics and how to refine them further.

Today: first axiom: superposition principle. In general, if we're in a
system that has k distinguishable states, then in general it is in a
linear superposition of these states. Each state is a unit vector, and
the states of the system reside on the surface of the sphere.

Addendum:
=========

What happens if we have two different subsystems? Take the first to be
k-dimensional, and the second to be l-dimensional. So now, in the
addendum, the question we are asking is "what happens if we take these
two subsystems and put them together and call this our new system?"
Take a tensor product of these two states. k × l distinguishable states.

So now, if you apply our superposition principle, what does it tell
us? We can be in any superposition of states. We are in a
superposition of basis vectors of (k ⊗ l).

Separately, we have k + l amount of storage space, but when we put
them together, we have k × l. These are the fundamental underpinnings
of quantum computing: this is where entanglement comes from; this is
where the exponential speedup comes from.

It's so very different from classical physics that if you chase it
out, you have consequences. One can just keep it at the level of
formalism, and then it's just notation; it's slightly weird. But then
you look at it and try to understand it, and it really has profound
consequences. So let's try to understand these consequences further.

[ calculating angles between states; inner product actually must be ]
[ equivalent to the product of the inner product of the components. ]

So now, let's back up for a moment and ask: we've said there's this
anomaly where we get a multiplicative effect instead of additive. Why?
They could be entangled. These states we are considering are product
states and are not entangled. In general, when you have a composite
system, you won't be able to decompose it into the tensor product of
two states, i.e. general state cannot be factored. For instance, Bell
states cannot be factored. You cannot say what the first particle must
be, and what the second state must be. All you can say is what the two
particles are simultaneously.

==== Intermission ====

Two different applications of concepts we've talked about before.

# No-cloning theorem

	Suppose you've got a qubit in the state |ψ〉, in some unknown
	state. Now that you have it, you'd like to make copies of it. What
	you have in your lab is lots and lots of qubits which you can
	initialize to the state |0〉. We also have a lot of fancy
	equipment. You think to yourself, surely, given the fact that I
	have all this fancy equipment and all these post-docs running
	around, we should be able to make at least one copy of this
	quantum state.

	So we want at least to have the state |Ψ〉 ⊗ |Ψ〉. We want to
	start with |Ψ〉 ⊗ 0 and go to |Ψ〉 ⊗ |Ψ〉 using fancy
	equipment. We can do plenty of unitary transformations (third
	axiom of quantum mechanics: no matter how big your lab is, it's
	only going to perform a unitary transformation). Is this possible?
	No-cloning theorem says this is impossible.

	There's a principle called the Church of the Larger Hilbert
	space. If you really want to, you could expand your Hilbert space,
	and consider measurements to be something that happens in this
	larger Hilbert space, and you're only looking at part of your
	data. In this larger Hilbert space, this is unitary in the larger
	Hilbert space.

	Right now we're considering a closed system. Later we can make
	this theorem more general and include everything, but the
	statement will remain the same.

	All you can do is perform some rotation on your Hilbert
	space. However, we must preserve angles. Such a unitary
	transformation only exists if we know that |Ψ〉 is one of two
	known orthogonal states.

	Basically, this tells us that we cannot clone an unknown quantum
	state. There is only one exception: when you know that it is one
	of two known orthogonal states.

Quantum Teleportation
=====================

	Related is the concept of quantum teleportation. Quantum
	teleportation provides a way to transfer a particle from one party
	to another, if the two parties share an EPR state (Bell state).

	Quantum teleportation is this protocol by which the first party
	performs a joint measurement on two qubits. The result of this
	measurement is one of four results, which is shared with the
	second party. The second party then performs one of four
	operations (a series of quantum gates) on the other qubit and
	receives as a result of these operations the original quantum
	state.

	There's this property of entanglement called monogamy. A qubit
	cannot be maximally entangled with multiple qubits.

These things took a while to figure out. At first, it was completely
unclear. When this was happening in the early 90s, we'd spend a lot of
time figuring these things out. It was not easy. We'll need some more
concepts, though.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Observables, Schrodinger's equation -- Feb 7, 2012
---------------------------------------------------

Observable
==========

Operator (i.e. can be described by a matrix) that describes any quantity
that can be measured, like energy, position, or spin. You feed in a quantum
state and receive as output a real number.

Why an operator? If you have a k-level system, then an observable for this
would be a k-by-k Hermitian matrix (i.e. $A = A^\dagger$). Important thing
about hermitian matrices: spectral theorem: orthonormal eigenbasis of
eigenvectors $\phi$ that correspond to real eigenvalues $\lambda$.

The real number you get as a result of the measurement -- what you read out
in the measurement outcome -- is $\lambda$. Consider discrete energy
levels; after a measurement, we collapse the wave function into a single
eigenstate.

We already knew what a measurement was. So what happened here, how can we
have a new definition of a measurement? This isn't fair. How can you trust
a course that changes its mind every other week? No complaints? I mean,
isn't it terrible? This is completely different. So what's going on?

Our previous notion of measurement required us to choose some orthonormal
basis. We write out our state $\Psi$ in this basis. The result of the
measurement was equal to i with probability $\abs{\beta i}^2$, and the new
state was $\ket{\Psi_i}$. So how does this correspond to what we have now?

We can reconcile them by showing that our old notion was less
formalized. It's only that basis which corresponds to the basis vectors of
some Hermitian matrix.

Pick any arbitrary orthonormal set of vectors and an arbitrary set of real
numbers. Ask: is there any matrix that has these eigenvectors and these
eigenvalues? Argue: always possible. In that sense, the new definition of a
measurement is really the same as the old one.

Consider case where eigenvalues not unique: reconsider notion of orthonormal
eigenvectors as notion of orthonormal eigenspaces. We've seen an example of
this, by the way: when we had a two-qubit system and we only measured the
first qubit. Each of the two outcomes corresponded to a two-dimensional
subspace. There were two eigenvectors with the same eigenvalue. Project the
subspace onto the space spanned by eigenstates corresponding to result of
measurement.

Reasoning: in the general case, you don't project onto a basis vector; you
project onto the subspace that is consistent with the outcome of the
measurement.

What the measurement does is provide some information about the state and
change the state to reflect the outcome. It doesn't restrict itself any
more than it has to.

diagonalization: converting to a different basis, scaling appropriately,
converting back to the original basis.

A way to construct the operator (must be a hermitian matrix) is with an
outer product: you can generate the change-of-basis matrix.

==== Intermission =====

Piazza: posted question about other people wanting midterm moved. Enough
objections such that we will stick with original date: next Tuesday. Posted
yesterday a homework which is effectively a review for the midterm, which
will cover everything up until this lecture. Three problems on homework: 2
are review, 1 is on today's lecture. **Due this Friday at 5.**

Schrodinger's Equation
======================

Most basic equation in quantum mechanics; describes how a system evolves
over time. Depends on one particular operator, the Hamiltonian: the energy
operator (more specifically, kinetic energy T + potential V). When you
write out the Hamiltonian of this system, the eigenvectors correspond to
(eigen)states with definite energy. The corresponding eigenvalue $E_i$ is
the corresponding energy.

So now what Schrodinger's equation says is that the state \psi of the
system is a function of t, and it evolves according to a differential
equation which relates the energy of the system.

$i\hbar \pderiv{\psi}{t} = \hat{H} \psi$
($\hbar \equiv $Planck's constant, $i \equiv \sqrt{-1}$)

The rate of change depends on what the Hamiltonian tells us to do. You can
consider the Hamiltonian talking about interaction between parts of the
system or between subsystems. Forces. Everything.

Now, Schrodinger actually discovered this equation in 1926. This was after
many of the initial discoveries in quantum mechanics. It was after
deBroglie discovered the wave-particle duality. One of the biggest
intellectual events of the twentieth century.

So let's see what this equation tells us about the equations of motion. PDE
solving, yay.

So what we know is that if the state at time 0 was an eigenvector $\phi$,
then the state at time t must be some constant $A(t)\phi$.

precession of individual states; generalization is the summation of the
various eigenstates. If you want to write out the linear operator that
tells you how to go from $\Psi(x,0)$ to $\Psi(x,t)$, it's just the diagonal
matrix of eigenvalues. You can check that this is a unitary matrix.

The way you write this unitary matrix in notation is $\exp(-i\lambda
t/\hbar)$. Nothing to be scared by. Look, we're exponentiating a matrix,
but that's nothing to be worried about.

Suppose $\psi(0)$ = $\ket{0}$, and you wanted to know $\psi(t)$.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Schrödinger's Equation -- Feb 9, 2012
-------------------------------------

Had the sense last time that some of you might not remember all of your
linear algebra. So this time is a hybrid of linear algebra and getting you
up to speed with this bra-ket notation.

What we are going to be doing is looking at it from three or four different
viewpoints to try to get an intuition for why it is the way it is. So when
we get around to trying to solve the SE for specific equations, it's not
just an equation; you have a feel for it.

Goal for today: Figure out why does the Hamiltonian plays a role in
Schrödinger's equation.

So basically, the way we are going about this is that last time, we had a
rather abstract formulation of Schrödinger's equation. Why? It's because
the formulation is so clean. General form: write out hamiltonian,
diagonalize it, and once you understand the eigenvalues and eigenvectors,
you understand why it has the form it does.

So why does it look the way it does? Conservation laws.

Next week, we'll look at it for concrete systems; for continuous systems;
the behavior of an unstrained particle. In each of these cases, we're
trying to build an intuition as to _why_ the Schrödinger equation is the
way it is.

Not going to get into time-dependent hamiltonians until maybe the end of
the semester.

Do Hamiltonians correspond to quantum circuits? The way you implement gates
is by implementing a suitable Hamiltonian. But a quantum circuit
corresponds to a time-varying Hamiltonian. Topics we'll get to closer to
the end of the semester.

What we are starting with are the basics of quantum mechanics. Viewing it
in the case of discrete systems, i.e. qubits. We've already started with
quantum gates, quantum circuits.

We're going back and forth between this abstract version which is very
close to axiomatic quantum theory (but also helps with the understanding of
theory), and physical systems (hamiltonians). After a few weeks of physical
systems, we'll start talking about quantum algorithms.

And then, after we have developed that for a little while, how do you
implement all this in a lab? We'll go back and look at the physics of it.

We're sort of walking this fine line between thinking about quantum devices
as abstract entities; where all you need to know is the axioms of quantum
mechanics; thinking about what you can and cannot do, and what you have to
do to make it all happen.

So let's start with the basics. What I'll do today is I'll describe in a
little more detail Dirac's bra-ket notation. We've already seen this
notation to some extent, but let's do this more systematically.

Remember if you have a k-state quantum system, then its state is described
by a unit vector in a k-dimensional Hilbert space. This is also
equivalently described in ket notation as α{j}|j〉. We love this notation
because it simultaneously highlights two aspects: this is a vector, and it
is information. For example, if k=2, this is a qubit storing a bit of
information.

The dual space (row space) of this, if you write this state as |Ψ〉, is the
bra 〈ψ| (hermitian conjugate). The inner product (square of the length of
the vector) is simply 〈Φ|Ψ〉

People who love the bra-ket notation love it because you don't have to
think. You just do what seems right and everything magically works out.

So if you have a vector |Ψ〉, you can talk about the projection operator
projecting onto |Ψ〉. It's a linear operator. What you want to do is design
the projection operator onto Ψ (often denoted by P) ≡ |Ψ〉〈Ψ|.

Pⁿ should ≡ P, for obvious reasons. |Ψ〉〈Ψ|Ψ〉〈Ψ|: 〈Ψ|Ψ〉 = 1, so
multiple applications of an operator are equivalent to a single
application.

Suppose |Ψ〉=|0〉. What does P look like as a matrix?
[1...0]
[..  0]
[. . 0]
[0  .0]

I = ∑|j〉, therefore. It doesn't have to be in terms of the standard
basis. You could write down the identity in terms of any basis in this
way. Physics refers to this as the "resolution of the identity".

Example
-------

Suppose you have a vector and you want to measure it in a general basis.

What happens if we measure |Ψ〉 in the |v〉, |v^{⊥}〉 basis? Do a change of
basis on |Ψ〉. Project |Ψ〉 onto each of the basis vectors. This is one way
of doing it.

==== Intermission ====

Goals for the midterm: fluency with the basics. Purpose of the course: not
a sequence of tests as much as getting something out of it. But to get
something out of it, you should be fluent in the maneuvers presented so
far. Not enough that you can sit down and figure it out in ten minutes.

Midterm will not be open-book or open-notes, but anything that you'd need
to remember will be on the midterm itself. e.g. teleportation protocol
would be given.

Observables
===========

An observable is a Hermitian matrix M such that M = M†. So now we have
something called the spectral theorem, which says that hermitian matrices
can be diagonalized: you can write them in a different basis (the
eigenbasis), and you can write them out in this eigenbasis.

Suppose M = X (bit-flip). Xv = λv. (X-λI)v = 0. det(X-λI) = 0. Solve for λ,
which are your eigenvalues, and then we go back and solve for our
eigenvectors.

Here, we're going to do t his by inspection. Eigenvectors of X would be
|+〉, |-〉; the corresponding eigenvalues are 1, -1.

Why is this an observable? If you were to create the right detector, we'd
observe something. We'd measure something. What we read out on the meter is
λ|j〉 with probability equal to α{j}², and the new state is |Φ{j}〉. What
Schrödinger's equation tells us is that if you look at the energy operator
H, and then in order to solve this differential equation, we need to look
at it in its eigenbasis. It was not supposed to be so frightening. You can
write U(t) notationally as e^{-iHt/ℏ}.

Why H?
------

Why should Schrödinger's equation involve the Hamiltonian? Why the energy
operator? What's so special about energy? Here's the reasoning: from axiom
3 of quantum mechanics, which says unitary evolution, what we showed was
the unitary transformation is e^{-iHt/ℏ}. Any unitary transformation can be
written in this form. You can always write it in the form e^{iM} for some
Hermitian matrix M. The only question is, what should M be? Why should M be
the energy function? The second thing that turns out (either something that
we'll go through in class or have as an assignment) – suppose that M is a
driving force of Schrödinger's equation. So ∂Ψ/∂t = M|Ψ〉.

Suppose there were some observable quantity A that is conserved: i.e. if
you start out with a measurement of |Ψ(0)〉, and you do the same
measurement at time t, if A is a conserved value, then this expected value
should be the same both times. If A is conserved, then AM = MA. A has to
commute with M. This tells us that their eigenvectors are the same. The
fact that energy is conserved, therefore, says that HM = MH. Then we have
one last bit: energy is very special. It is very fundamental; these are the
building blocks; it goes right to the core. H commutes with M not for
special conditions of the system, but rather generally.

So you can reason that M is a function of H. And then you show that it must
be a linear function of H (aH + b), b must be 0, and a must be a constant
(in fact, Planck's constant).

Symmetry plays a very strong role in the way this comes about.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Continuous Quantum States, Free particle in 1D, Heisenberg Relation
--------------------------------------
Feb 14, 2012
------------

So far we've talked about discrete quantum systems. Today, we're going to
make the transition to talking about continunous quantum systems. In 1
dimension, the particle can be anywhere on the line.

Schrödinger's equation in 1 dimension, Heisenberg relation.

iℏ(∂Ψ(x,t)/∂t) = [-(ℏ²/2m)(∂²/∂x² + V(x)] Ψ(x,t) = HΨ(x,t)

Some things where won't be terribly careful about being precise for
now. Will fix later to extent that you won't be too upset about it
later. Attempt to day is to try to understand these objects on an intuitive
level. Before we do that, let's do a five-minute review of discrete
systems, just to orient ourselves when we go to continuous distributions.

Back to the familiar k-state systems. State is a unit vector in k-dim
Hilbert space, etc. And then we have this notion of an observable (given by
a Hermitian matrix M ≡ M†). What we like about this is that by the spectral
theorem, we have an orthonormal basis of eigenvectors corresponding to real
eigenvalues. Result is that discrete energy levels correspond to orthogonal
vector spaces.

Since this is an observable, the deflection of our meter is λj with
probability |〈Ψ|Φj〉|², and the new state is |Φj〉. We could ask a
couple of questions.

Before we continue: let's look at another way of considering M being
Hermitian: Mij = 〈Φi|M|Φj〉 = conj(〈Φj|M|Φi〉) for any Φi, Φj; not
necessarily just for basis vectors.

	* We can try to picture the measurement outcome. What our measurement
	  looks like is this: some probability distribution. We can
	  characterize this distribution by its moments, much like how we can
	  characterize a function by its derivatives. The more moments we have,
	  the more we know about our distribution.
		+ mean: location.
		+ standard deviation: width.
		+ skewness: symmetry.
		+ kurtosis: peakedness.
	* We can consider the mean to be 〈Ψ|M|Ψ〉.
		+ First write M in its eigenbasis, where it's a diagonal matrix.
	* We can also do the same for variance. Var(X) = E(X²) - E(x)² = E(X²)
	  - μ², for obvious reasons. E(X²) = 〈Ψ|M²|Ψ〉: intuitive after
	  considering that M² preserves eigenvectors while squaring eigenvalues
	  (result of diagonalizability of M).

Here's what we're planning to do (hopefully) for the rest of the lecture:
it'll be in the form of a sketch, which hopefully will give you a picture
of what happens when you look at a particle in 1 dimension.

Before that: σ (standard deviation) is a measure of spread. If you were
really certain about a physical quantity, you'd have σ ≡ 0.

So, let's have a particle that's free to move about in one dimension. We'll
describe approximately for now; we just want to understand its form. Later
will be much precise, but now we just want to put an image in our minds as
to just where this equation comes from. We'll want to understand position
of the particle x (how it behaves) as well as momentum p. We'll also look
at the uncertainty relation that says ΔxΔp ≥ ℏ/2 (some constant). Show
intuitively that there must be some minimum.

In this continuous picture, x and p behave like something you've already
seen: bit and sign. There's something about the spread of the two. You
cannot know both of those quantities precisely, either in the one, or the
other, or both. The more certainly in one, the less certainly in the
other. Rather than do it by formula and precisely, we'll do it more
intuitively.

Often, when you think you want to explain something so that people really
understand, you want to go slowly. Paradoxically, it's sometimes better to
go fast. Explanation: it's easier to put all the pieces together when you
see the big picture all at once. See big picture first, then observe
individual bits later.

We want to talk about a lot of stuff.

Once again, what we are trying to do is describe the state of the particle
on an infinite line. So now, before you describe it, let's do an
approximation. Let's consider this as not infinite, but very long (take a
limit). Likewise, not continuous but very fine (also a limit). Could be at
one of various positions. Describe your state as this superposition of
states. What we're saying is that Ψ(j) is αj. When we generalize this to a
particle being anywhere on the line, the way to describe it is Ψ being a
continuous function, so Ψ(x) is a complex-valued function on the real
line. As in the discrete case, we want our distribution to be normalized.

Now, suppose we wanted to measure the position of this particle. Out here,
we'd have an observable, M. The corresponding observable in the continuous
case, let's call it x. We'd just do 〈Ψ|x|Ψ〉. Our inner product now is
defined with integrals.

Our observable now takes as input a wave function Ψ and spits out another
function as output.

More fuel for intuition: you should know one of the big discoveries: nature
is described by local differential equations. Every point in space only
considers its own neighborhood. It's concerned only with itself, and
nothing else. So now let's apply that principle here. How that wave
function evolves over time. So the point x is minding its own business and
its own infinitesimal neighborhood. So what does it do? The simplest thing
it does is compare itself to its neighbors, say the average of its
neighbors. (consider perceptron, maybe?) But this yields the second
derivative with respect to x, and the function smooths itself out. So we
must move in an orthogonal direction to avoid collapsing the wave
function, i.e. multiply by i.

Let's now try to understand where the uncertainty principle comes in.

Momentum we can measure in the quantum case; it's sort of a proxy for
velocity, since velocity doesn't really make sense.

Let's consider a fairly standard wave exp(i(kx-omegat)). Now you ask what the
equation of motion says if it's going to evolve. Let's say Ψ(x,0) ≡
exp(ikx). omega is the rate at which it twists over time. A twisting seems to
correspond to some sort of translation. The rate of translation is directly
proportional to k.

So now we have a function of definite momentum. We can then decompose this
wave function in terms of these functions. These are our Fourier basis
functions. So if you want to go from the position basis to the momentum
basis, you take a Fourier transform. In other words, this is roughly
equivalent to what we do when we go from the sign basis to the bit basis,
or vice versa (what a Hadamard gate does).
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Free particles, Uncertainty, Quantization
-----------------------------------------
Feb 21, 2012
------------

We talked about things informally last time, and what we were going to do
was see Schrödinger's equation from various viewpoints.

Today, again we'll look at a free particle in one dimension, a somewhat
more rigorous of the uncertainty relation, quantization, and use that as
our first implementation of a qubit.

What we want to do is describe how this state evolves in time. This is what
was described by Schrödinger's equation. Last time, we derived what this
equation should look like (approximately) as a result of certain
considerations.

Clasically, the energy of this particle (which we'll write as a function of
two parameters p and x). ∂ψ/∂t = p²/2m + V(x).

So, first of all, we want to figure out what corresponds to the position in
quantum mechanics? How do you measure the position of this particle? We
said we had a position operator, and when you applied it to ψ(x), what you
got back was xψ(x). Same idea of measurables as before: we're measuring
position, so our eigenvalues correspond to the position of the
particle. What we are requiring from our position operator is exactly the
same thing.

So the momentum operator \hat{p} is ℏ/i (∂/∂x). ℏ is our normalized
Planck's constant. Again, the discrete analogue of this is a
measurable. Once again, we're ignoring constants, since we only really care
about constants.

What the correspondence principle says is that what you should do (when you
know what the classical situation looks like) is to just substitute \hat{x}
and \hat{p} instead of x and p, and put that down for your energy
operator. So why do you do this? Better people than us have done this in
the past, and it seemed to work out for them.

So clearly, what Schrödinger's equation must say is:
iℏ ∂ψ/∂t = -ℏ²/2m ∂²ψ/∂x².

(ignoring potential energy as a result of it being a free particle – not
subject to any potential / potential is equivalent to 0.)

Uncertainty
===========
Let's just try to see how our function (given as a function of x) looks as
<a function of p. What we said last time was that was the Fourier
transform. Fourier basis and standard basis are "like oil and water" – if
one is maximally certain, then the other is maximally uncertain. So you
cannot come up with any wave function that is localized in both spaces. So
we can try to quantify this. We had nice pictures, but now we're going to
work directly with operators. It's going to not vague and very precise, so
for some, it'll be great. But for others, it'll be a disaster since there
are no pictures.

Remember: the thing about an observable that we care about most is the
eigenvector decomposition. So the question is: what do the eigenvectors
look like? That determines how nicely they play with each other.

Discrete case first. Remember you had your phase-flip operator Z, bit-flip
X. Considering the eigenvectors and eigenvalues, the claim is that this is
as good as you are going to get.

Another way to measure how different these eigenvectors are is to see
whether these matrices commute or not. If they commute, they have a common
set of eigenvectors. Commuting means XZ - ZX = 0. So we want to look at XZ
- ZX (a commutator, denoted by [X,Z]).

So what does this look like between \hat{x} and \hat{p}? We have product
rule coming into play... yielding [x,p] ≡ iℏ.

We'll use this to derive ΔxΔp = ℏ/2. We'll do this quickly so we'll have
time to talk about the particle in a box.

Recall: given an observable A and a state |Ψ〉, the expected value is
〈Ψ|A|Ψ〉. We also saw that the variance was E(x²) - E(x)², so in this case
σ² = 〈Ψ|A²|Ψ〉 - 〈Ψ|A|Ψ〉². 

For now, assume 〈Ψ|A|Ψ〉 = 0. Makes derivation simpler, and we're just
asserting that we don't really lose much (anything, really).

Take Ψ ≡ A|ψ〉, Φ ≡ B|Ψ〉. By Cauchy-Schwarz, we have that this is greater
than or equal to 〈Ψ|Φ〉 = 〈ψ|AB|ψ〉. By symmetry, it's also greater than
or equal to 〈Φ|Ψ〉 = 〈ψ|BA|ψ〉. As a result, (ΔA)²(ΔB)² ≥
|〈ψ|AB-BA|ψ〉/2|² = |〈ψ|[A,B]|ψ〉/2|² = |〈ψ|iℏ|ψ〉/2|² = ℏ²/4. So the
square of the spread of x + square of spread of momentum is at least
ℏ²/4. If you take square roots, you get the proper result. (you can get a
better bound by being more careful by also using the anticommutator. We
were being sloppy for the purpose of making things simpler.)

Particle in a box
=================

This is basically just the infinite square well. We want to figure out our
eigenfunctions just by looking at the operator. Again the way we do this is
guess and come up with the right answer by chance. You expect the
eigenfunction to be an exponential, so we guess and check. Life is nice
that way.

We are trying to solve for H|φ〉 + E|φ〉. Guess that |φ〉 = e^{ikx}. Let's
figure this out. Our maneuver is to say that we're dealing with a separable
equation ("decompose this problem" by looking at the eigenvectors of H).

Suppose our state was one of these eigenvectors. We then know what the
Hamiltonian does to it: it simply applies a scalar (the corresponding
energy level). So of course we want to write our function in this basis,
since we know how to solve the simpler differential equation.

The operator affects each eigenvector separately. So we can tack on the
time dependence as an afterthought (to each eigenvector).

COMING UP
=========

We'll solve this problem using this very strategy. We know what the
eventual answer looks like: ∑e^{-iEt/ℏ}|ψ(x)〉
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Quantization, Particle in a box, Implementing Qubits
----------------------------------------------------
Feb 23, 2012
------------

A more precise review
=====================
By this point, we've talked about a number of things. Discrete/continuous
quantum systems, measurements, and so forth. So what I'd like to do for the
first half of the lecture is give you a slightly more formal overview of
everything we've talked about before: about the model. In some sense what
we've been talking about has been challenging, but in some sense it's also
rather simple (i.e. we can do a more precise, more formal review of what
we've covered quickly).

Multiple-qubit systems
----------------------
So let's start from the beginning. If our state is a discrete system, $\psi$
is an element of a $k$-dimensional vector space.

The second thing we want to say about quantum states is the following: what
happens if you have two quantum systems, $a$, a $k$-level system, and $b$,
an $l$-level system? Now we want to understand what happens when we put $a$
and $b$ together and look at them as a composite system? When you put states
together, you need to take tensor products. If we happen to be in the state
$|i\rangle$ in one system and $|j\rangle$ in the other system, then
corresponding to that we have a state in the composite system as $|i\rangle
\otimes |j\rangle$, which can be written as $|ij\rangle$, if you're being
especially lazy.

The number of dimensions we get is $kl$, which is the dimensionality of the
space in which the composite system lives.

This is called taking tensor products.

The new system inherits the properties of the old one; for instance, how do
you compute inner products? Suppose a-system happened to be in the state
$|00\rangle$, and another was in $|++\rangle$. What is the angle between
these states? If you were computing the inner product, $\langle 00|11
\rangle$, then you would just take the inner products of the pieces
separately.

What is more interesting to us is measuring the probability of a state, and
that corresponds to its magnitude.

Evolution of states
-------------------
We had our Hilbert space, and our evolution was just the rotation of our
Hilbert space. This was a rigid body rotation in that it preserved
angles / distances. So the inner product is not going to change as you do
this rotation.

And then we had our favorite gates, which consisted of things like the
bit-flip (X), phase-flip (Z), Hadamard (H), controlled not (CNOT, a
two-qubit gate).

So now, here's what I wanted to get to. Suppose you have two qubits, and
you apply a gate on each of them. Now you want to understand what operation
was applied. So first we must understand the form of the answer: it will be
a 4x4 matrix. And then to understand how to write out this 4x4 matrix:
effectively, we take the tensor product of the indivdual gate matrices.

$$A \otimes B = \begin{pmatrix}
a_{00}B & a_{01}B \\ a_{10}B & a_{11}B
\end{pmatrix}
$$

Observables
-----------
So then you have measurements, and we said that measurements correspond to
an observable M, which is a Hermitian operator, i.e. $M_{ij} = M_{ji}^{*}$.

So now, why is an observable M? Because we said when you have a Hermitian
matrix, by the spectral theorem, you have an orthonormal eigenbasis that
correspond to real eigenvalues. What you get as the outcome of a
measurement is some $\lambda_j$. This occurs with probability equal to
the square of the length of the projection onto the eigenspace
corresponding to $\lambda_j$. The new state is $|j\rangle$.

There is a special observable known as the Hamiltonian $\hat{H}$, the
energy observable. In order to solve the Schrodinger equation, which looks
very complex, if you write it in terms of the eigenvectors, we can neatly
partition it into a number of simpler differential equations, one for each
eigenvector. Since these are eigenvectors, the evolution of the system
leaves the direction alone, and all it does is change the amplitude and the
phase. There is a very nice short-hand for writing this: $e^{-i\hat{H}t}$:
this is our evolution operator, and you can check that it is unitary.

Continuous quantum states
-------------------------
So now let's fill in the corresponding picture for continuous quantum
states.

No longer finite, and it's even continuous. So, like a particle on a line,
and now you have a probability distribution representing your amplitude
(sort of; more like intensity). Usually you talk about what the probability
is of being in some range (of being in the neighborhood of x). If you were
looking at x itself, the amplitude would be zero (excepting Dirac
deltas). So now $\Psi$ is a function mapping $\mathbb{R}$ to
$\mathbb{C}$. It's normalized such that $\int |\Psi|^2dx = 1$. Another way
of saying this is that the inner product of $\Psi$ with itself,
$\langle\Psi|\Psi\rangle$, is 1.

So what's the corresponding vector space we have for a continuous vector
space? We need some set of eigenfunctions that span all complex
numbers.

Then we have this notion of observables in these continuous systems, which
is going to be just like it was in the discrete case. So what does it do?
It takes a state $|\Psi\rangle$ and maps it to $M|\Psi\rangle$. So we'll
just have some operator that maps a wave function to a not necessarily
normalized wave function.

And so we have two examples that we saw: $\hat{x}$, the position
observable, which maps $\Psi(x)$ to $x\Psi(x)$, and then we had $\hat{p}$,
the momentum observable, which maps $\Psi(x)$ to $i\pderiv{\Psi(x)}{x}$.

So we need these to have some notion of Hermitian. We must have
$\langle\phi|M|\psi\rangle = \bar{\langle\psi|M|\phi\rangle}$. We call this
sort of operator "self-adjoint".

Remember: integration by parts.

The momentum matrix would be skew-Hermitian ($M^\dagger = -M$), so we had to
multiply by a factor of i.

On this particular homework, all you have to do is work your way through
things like this (whether certain operators are Hermitian or not) and
compute the commutators of certain matrices. Should be an easy or useful
exercise, depending on how used to this sort of thing you are.

So now, let's talk about a particle in a box. We assume there is a box of
length L with infinitely high walls (i.e. infinite square well). Basically,
consider behind the boundaries of these walls there is a potential so large
that the particle cannot afford to leave.

So we want to solve Schrodinger's equation. What are the states? $H =
\frac{\hat{p}^2}{2m} \Psi = \frac{-\hbar^2}{2m}\pderiv{^2}{x^2}
\psi$. Rather than carry this potential around with us, we'll just impose
boundary conditions. $\Psi(0) = \Psi(l) = 0$. So this is H. Remember how we
solved Schrodinger's equation; we solved the eigenvalue problem. We tried
to figure out the eigenvalues $\phi_j$ and the corresponding energies
$E_j$.

So now we want to understand what these eigenvalues look like for
corresponding energies. So what's an eigenfunction of this? The guess (what
we want) is for the eigenfunctions to look like e^{ikx}. Just evaluating
the right-hand-side, we get $E_k = \frac{\hbar^2 k^2}{2m}$. This is both
the energy of $e^{ikx}$ as well as $e^{-ikx}$. So we guessed what the
eigenfunction looked like, and then we checked.

So we checked that $\psi_E(x)$ is going to be of the form $Ae^{ikx} +
Be^{ikx}x$. When you take linear combinations of the two exponentials listed
above, you might as well take linear combinations of $\cos(kx)$,
$\sin(kx)$. This form is nicer because it is easier to impose the boundary
conditions. Enforcing boundary conditions, we get that $C = 0$ (which makes
sense; cosine is even, and this function is 0 at x=0) and that $D =
\frac{k\pi}{l}$.

We now want to find C, which we can get by enforcing that our wave function
is normalized. use the $\int (sin^2 + cos^2) = 2\int (sin^2)$ trick.

Finally, let's just go back and make two nice observations. Now you finally
see how to implement a qubit. To implement a qubit, you would restrict the
energy to be small enough to be in the first two modes. And then you would
let zero be one qubit and one be the other.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Quantum Algorithms
==================
Feb 28, 2012
------------
Introduction
============
Today we're going to make a transition to quantum algorithms. But first, a
brief review of particle-in-a-box.

Particle in a Box
=================
The particle in a box is sort of a toy model for a hydrogen atom. In what
sense? In a hydrogen atom, you have a proton and an electron, and the main
force it is subject to is Coulomb attraction. So for our purposes, as a
very first, very simple model, we'll think of writing out Schr\"odinger's
equation in the radial direction.

We'll think of this electron as not three-dimensional, but
one-dimensional (radial). And instead of dealing with the potential as it
is, we'll approximate it by saying what the potential really does is
confine the electron to within some range $\ell$. What we're going to do is
model the situation by saying that the electron is a free particle in a
box.

* Aside

	It's worth thinking about this: once we solve this, we get a first
  picture what a hydrogen atom looks like. When we plot out the solution,
  what does this tell us about an electron and where it sits? This gives us
  a first approximation; really inexact. But for our purposes, since we are
  not so much wanting an understanding of the hydrogen atom so much as
  wanting a general understanding, this is a great model.

Normally we consider this particle as free to move anywhere, but now we
impose an infinite potential outside of a particular region. Writing out
Schr\"odinger's equation where $H \equiv \frac{-\hbar^2}{2m}
\pderiv{^2}{x^2}$. We initially did this by guessing that the eigenvectors
were of the form $e^{ikx}$. We guessed this, figured out that the
eigenvalues (energies) corresponding to this must be $\frac{\hbar^2 k^2}
{2m}$, and then we said look, what this means is that $e^{ikx}$ and
$e^{-ikx}$ have the same energy, so we might as well just use the sinusoids
$\cos(kx)$ and $\sin(kx)$. We solve for coefficients by imposing the
boundary conditions.

We also went ahead and normalized our wave function by saying $\int_0^\ell
\abs{\psi_n(x)}^2dx = 1$. What this further gives us is that $D =
\sqrt{\frac{2}{\ell}}$. So our general solution is $\psi_n(x) = \sqrt{\frac{2}
{\ell}}\sin\frac{n\pi x}{\ell}$. This tells us about the quantization of
energies. The fact that it takes on these discrete values means that you
get these standing waves of definite energy.

There's also a way of making this analogy a little more quantitative. We
can attempt to go ahead and do that. If we look at $E_2 - E_1$ (the energy
difference between the first excited state and the ground state), for our
model, we got $\frac{3\hbar^2 \pi^2}{2m\ell^2}$. Assume $\Delta E \approx
10$ eV; let's find out what $\ell$ is (compared to $\ell_H \approx 1
\AA$, the actual value). We could substitute and solve, and we get roughly
$3.4 \AA$. As long as you make sure your energy is small, what you know is
that your particle is always in some superposition of the $n=1$ and $n=2$
states.

Precession of phase
-------------------
Remember that you are in always in some superposition of states, which
evolves over time. So I hope you can see what your qubit is, now. The
associated value of a qubit corresponds to a particular superposition. If
you do an inner product between any two of these superpositions, you get
complete cancellation.

We get a changing phase, yes, but this is no big deal; we can simply
account for a moving reference frame.

Because we have this phase precession, if you look at what the frequency
corresponds to, it's whatever the frequency of visible light is, let's
say. The fact you have this precession at this frequency allows you to
control this qubit from the outside using light (lasers). So somehow, in
this formulation, what you have is an implementation of qubits, and
secondly, you have a hint of how you're going to control this qubit from
the outside. How do you apply various gates? You now have this method of
reaching in and constraining the evolution of the quantum state.

* Aside

	One other thing which we may or may not get to is this whole notion of
  cooling. This is a big deal in quantum computing and quantum systems. How
  do you cool your system down to a desired level? Once you're down to the
  quantum level, cooling itself looks like an algorithm. So there's this
  notion of algorithmic cooling. Just wanted to point this out as a lead as
  to where we are going to get to in a few weeks.

* Intermission

Quantum Algorithms
==================
Inspiration
-----------
A one-qubit system belongs to $\mathbb{C}^2$; a two-qubit system belongs to
$\mathbb{C}^4$; an n-qubit system belongs to $\mathbb{C}^{2^n}$. This is a
result of repeated applications of the tensor product.

Note that even a 500-qubit system requires $2^{500}$ complex numbers to
describe. This number is larger than the number of particles in the
universe; larger than the estimated age of the universe in femtoseconds;
even larger than the product of these two numbers. This just comes from the
basic axioms of quantum mechanics. In order to even remember the state of
this 500-qubit system -- just to remember that state, it's almost as if
nature has $2^{500}$ pieces of scratch paper lying around somewhere.

Moreover, if the system interacts with something else, with the outside
world, the state needs to be updated -- it needs to be updated, even, at
every moment in time, at every smallest time step. If this were not a
theory which we read about in a science class, it's insane to think that
this is how hard nature must work to keep this tiny system going.

You have a choice, now. You can either get stuck in this philosophical
mode, or ignore how nature stores this information and just focus on how to
make use of this. A computer, once you get past the packaging, is just an
experiment in physics. The only difference between a computer and an
experiment in physics is that in an experiment in physics, you're trying to
understand how physics works, whereas when you're using a computer, you
already understand the physics and are just tricking nature to solve
interesting problems for you.

Furthermore, remember that entanglement disrupts the principle of locality.

So why use classical computers, when nature is working so hard to keep
track of these quantum systems?

It's almost as if quantum systems have the ability to store and process
exponential amounts of information. It's rather exciting: quantum mechanics
tells us that nature is exponentially extravagant. So you could solve
problems exponentially faster. But nature is extremely private. When you
make a measurement, all you see is one of these values. So this is the main
problem in quantum computing. You have this entity who, behind the scenes
is carrying out these totally extravagant gyrations, computations, and you
think to yourself, if we took our most difficult problems and mapped it
onto that? Except that nature also guards all of this very
jealously. Whenever you try to query, nature, tries to pretend nothing much
was going on.

So what quantum computation is about is trying to tease this information
out of nature. How do you design algorithms where you can really map the
question, the problem of interest, into this space such that even the
outcome of the measurement tells you a lot about the system?

We somehow take this 500-qubit system, put it into some state, and set it
to evolve over time. At the end of this time evolution, we make a
measurement and just get back 500 measly classical bits. We say to
ourselves, we couldn't have gotten them ourselves. Either these are the
solution to our problem, or we can post-process them very quickly to get
our solution. Even though this $x$ doesn't seem like much, there was a lot
going on to create $x$, and we can extract this information from $x$.

What we can solve in this manner are questions in the form of
puzzles. [ Description of NP-complete problems. ]

For example, the quantum algorithm for factoring, where you're given some
semiprime $n \equiv pq$ (although not really necessary) as input. We'll create some
sequence of superpositions, and we'll get $x$.

If $n$ were a few hundred bits, $x$ would be a few hundred bits, and we'd
be able to extract $p$ and $q$ in a fairly straightforward manner from
here. Except classically, this would take exponential time to compute (in a
few hundred bits).

What are the tools we'll be using? Hadamard gate, which is a one-qubit gate
corresponding to the two-dimensional DFT, or changing from the bit basis to
the sign basis (namely, between conjugate bases). This is our quantum
Fourier transform, most likely.

What we'll do next time is understand the transformation corresponding to
performing a Hadamard on every individual qubit.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Quantum Algorithms
==================
Mar 1, 2012
-----------

Now, let me tell you what quantum algorithms look like. You have your
qubits feeding in, and you might have your input x, have a number of work
qubits initialized to $\ket{0}$, and you have your outputs. What we might
do is measure some subset of your output bits. And then what you might do
some classical post-processing.

So what happens inside the box? Inside the box, you have quantum gates that
form some sort of circuit.

For us, the star is the Hadamard transform (for now). Later, it'll be the
QFT, but the Hadamard is a good place to start.

Remember what we said was that we had n qubits. We'll call this $H^{\otimes
n}$ (an n-fold tensor product of 1-fold Hadamards). Suppose $H^{\otimes
n}\ket{0^n}$. What's the output? $\sum_{\ket{x}\in \{0,1\}^n} \frac{1}
{\sqrt{2}^n}\ket{x}$. Suppose $u$ is some n-bit string. Now what's the
output?
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Quantum Algorithms
==================
Mar 6, 2012
-----------
Today, we are going to resume our study of quantum algorithms, but I'll
start with the basics of what it means to have a quantum circuit in the
first place, and then we'll move on to a quantum algorithm.

Let's start with the first part, which is about the basics of designing
quantum algorithms. What circuits does one use, what does it mean to design
a circuit; things like that.

First question: Quantum computers are a) digital, b) analog. Why digital?
Input is a sequence of zeroes and ones; we input our qubits in some digital
form. Output is also in the form of zeroes of ones -- concept of
measurements.

Internally, qubits look both digital as well as analog: coefficient looks
analog; ket is digital. Really the coefficients are not analog, since we do
not care too much about infinite precision (cannot care, really). Where
else does it come from? It is unitary.

So it's not as though these are going to have chaotic dynamics; being off
by a little step doesn't spell the end of the world. No butterfly effect.

So that's lesson number one.

Lesson number two. So how did we actually design classical qubits,
classical circuits? There is this concept that certain gates that are
universal, e.g. NAND. Talk about how to construct all possible two-input
gates with NAND.

So: quantum gates. We have various examples of quantum gates, e.g. CNOT, H,
X, Z, $\frac{\pi}{8}$ rotations. So what do these mean? Suppose you have an
arbitrary gate U that we want to implement. Maybe it's a two-qubit gate;
maybe three; maybe one.

Remember, in this case, we can specify U by 16 complex numbers. But these
have infinite precision. We are not going to be able to implement infinite
precision with finitely many bits. So what we want to do is implement
something that is $\epsilon$ close to this. Instead use universal gates to
implement $U_\epsilon$. We'll measure this $\epsilon$, and we'll claim that
these two transformations are very close to each other (i.e. identical
states are sent to similar states; only changes distance by at most
$\epsilon$).

Also, the cost in terms of $\epsilon$ is not going to be very large. The
number of universal gates will scale just as $\log^2\frac{1}{\epsilon}$. So
all this, we're just going to take for granted. In fact, in our quantum
algorithms, we'll use relatively simple-looking gates.

So far, what we've learned is that quantum computers are digital, and
there's a universal family of gates.

These were both ways in which quantum computers were similar to classical
computers. Now let's start exploring the differences.

The first difference is that quantum computers are reversible.

You have some quantum circuit. It takes as input some number of qubits,
outputs the same number of qubits. Inside you have quantum gates. That is
your quantum circuit. $U_c$ is unitary, which means that $U_c^{-1} =
U_c^\dag$. In general, you will still get the mirror image, except you will
replace each gate by its Hermitian conjugate.

So now we have a problem. Suppose we have a classical circuit $C_n$ for
computing f. Now suppose $f$ is a boolean function, so $f(x)$ is just a
bit. Imagine there was a quantum circuit for doing this. We really are
computing all n bits. These other bits, together with f, allow us to
reconstruct $x$. Since quantum computers are reversible, any arbitrary
circuit is just a injective map.

Remember our basic classical circuit, our NAND gate. This was not injective
-- it mapped four possible inputs to two possible outputs.

Let's remember this qwuestion: why can't you just discard the bits that you
don't need? It turns out, this is a very crucial issue in quantum computing.

If this is a constraint that we have on our quantum circuits (unitary), we
have to do something: we have to make our classical circuits look like
this. We'll show that we can take any classical circuit and make it
reversible. So given a classical circuit, first make it reversible, then
we'll simulate it quantumly. That is our answer.

So how to make classical reversible computation? What you must do, if
you're doing classical computation reversibly, the first thing you must do
is increase the number of inputs. Now, being reversible means that it has
to be true at every step. You can't possibly throw away anything. You can't
ever discard information: you can transform it, but you cannot discard
it.

So now, what this shows is that you can take any classical circuit $C$
which has NOT gates and AND gates, and you can transform it into a
reversible circuit $R_c$ with more outputs (we need these to maintain
injectivity). We don't like this junk. What we'd like is to further
transform this into a circuit of the kind that we really like, $\hat{R_c}$,
that preserves the inputs as well as the desirable outputs.

The secret lies with the $CNOT$.

Back to what we did last time: suppose you had a boolean function $f$ and a
circuit $C$ that represented it. Now suppose you have a quantum equivalent
$U_f$. What we said was that since this is now a quantum circuit, we can
run it over a superposition of inputs.

Let's say you have a qubit that you feed into this circuit. What the
circuit does for you is it creates junk. It happens to be a CNOT
gate. Meaning, this was a circuit that on input $\ket{x0}$ outputs
$\ket{xx}$. However, suppose that we intended to do a Hadamard
transform. The "junk" prevents the interference that we desire. This is why
we do not leave junk lying around: we generally do not know what
interference it will cause, or where.

There's a corollary to all of this: **throwing away / discarding is the
same thing as measurement.**

So in other words, what all this shows you is that if you had a qubit and
wanted to measure it, one way to measure it would be to throw it away and
never interact with it again.

Claim: Measuring in the standard basis is equivalent to doing a CNOT, and
then continuing.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Simon's Algorithm
=================
Mar 8, 2012
-----------
We're still working in this toy model. Last time, we showed that we can get
some sort of speedup for Fourier sampling. While this was on the order of
n, we can actually get a speedup of $2^n$

Review of Fourier sampling
--------------------------
Applying a Hadamard transform to all inputs, we get somme sort of analogue
to the FFT.

Adding slit patterns (going back and considering double-slit
experiment). Input corresponds to the slit pattern. And then, given the
slit pattern, we say it is hard to figure out what the interference pattern
looks like.

Simon's Algorithm
-----------------

We are given a circuit that computes a function. f is 2-1. Namely, there
are $2^n$ possible inputs, and there are $2^{n-1}$ possible outputs in a
particular way: there is some secret s of length n, and we know that $f(x)
= f(x \oplus s)$. Other than that, it could be anything. What you want to
do is find $s$.

Let $n = 3$. Let $s = 101$. So:

$$f(000) = f(101) \equiv 000
\\ f(001) = f(100) \equiv 010
\\ f(010) = f(111) \equiv 001
\\ f(001) = f(110) \equiv 100
$$

So how do you find s? Classically, you'd find two inputs that map to the
same string. You'd expect this to take roughly $2^{n/2}$ tries: consider
birthday paradox. What this is saying is classically, this is a hard
problem. And now we are going to see how to solve this quantumly in $n$ or
$n^2$ steps.

The first thing we are going to do is take this circuit (remember that if
we can compute something, we can also compute it reversibly) and write down
the reverse of this circuit.

Once this is a reversible circuit, we can also implement every gate
quantumly. We then map this to a quantum circuit which does the exact same
thing.

With the quantum circuit, you can input a superposition, and you'll get a
superposition of the outputs as your result. We've seen this before in the
form of phase states, but we'll use this in a different manner today.

Example:

We start with the Hadamard transform, $H^{\otimes n}$, and we feed in
zeroes. So what's the state at this point? It's a total superposition of
all states. After we feed this into our circuit, our state is a
superposition of the sum over all x of f(x).

Working through the math, we find that we get a random $y$ such that $s
\cdot y = 0$. Thus we have a single linear equation, and so we just need
$n-1$ equations to uniquely solve for s.

Okay. So let's go back and look at a general circuit. So we start with n
zeroes, do a Hadamard to get into a superposition of all possible
inputs. Now we are in the state where we have every $n$-bit state with equal
amplitude, but now it's entangled with this other register $f(x)$. At this
point, we measure these n bits (and we see some $f(z)$) -- and so we got
some random input.

Now we do another Hadamard transform and another measurement. We want to
figure out what we get as the result of this second measurement. We claim
that our output is a random $y$ such that $y \cdot s = 0$.

This is what we said before, but now more general.

Let's prove this. After the Hadamard, let's say we're in a state $\sum_y
\alpha_y \ket{y}$; $\alpha_y = \frac{1}{\sqrt{2}}(-1)^{z \cdot y} +
\frac{1}{\sqrt{2}}(-1)^{s \ cdot y}$. Now, we can see that this is equal to
zero if $s \cdot y = 1$ and two if $s \cdot y = 0$. Therefore the only
values remaining in this superposition are those such that $s \cdot y = 0$.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Quantum Algorithms
==================
Mar 13, 2012
-----------
Today, we are going to set things up for the quantum factoring algorithm,
which we will cover on Thursday. To do that, we have to generalize the
Hadamard transform into the quantum Fourier transform. What we are going to
cover is the quantum Fourier transform: its properties as well as how to
use it.

Today, we'll regard period-finding as an abstract problem we want to
solve. Next time, we'll show how to use period-finding to factor.

So what's a quantum Fourier transform? It's really a quantum implementation
of the discrete Fourier transform.

So if $\omega$ is a primitive nth root of unity, then all of the powers
$\omega^j$ are also roots of unity.

So the quantum Fourier transform is going to look just like the Hadamard
transform except there are phases in it.

The discrete Fourier transform
------------------------------
$$
F = \frac{1}{\sqrt{n}}\begin{pmatrix}
1 & 1 & .. & 1
\\ 1 & \omega & \omega^2 & .
\\ . & . & . & . & .
\\ 1 & . & . & . & .
\end{pmatrix}
$$

$F_{jk}  = e^{ijk2\pi/n}$

Primitive square root of unity: $e^{i2\pi n}$. When we do $F_n$, it's the
same, but with phases.

Showing that $\braket{F_i}{F_j} = \delta_{ij}$...Properties of the Fourier Transform
-----------------------------------
This is a proper circuit, and in fact, you can implement it very very
efficiently.

The QFT is very efficient to implement by a quantum circuit.

Let's talk about what you'd get if you tried to implement it.

Considerations with regards to cyclic shifts: effect only shows up in
phases.

So by the way: in quantum computing, what is the significance of phases?
They don't matter in the context of a measurement.

To eliminate effects of relative phase, we do a Fourier transform and then
measure. Basically, multiplication-convolution property.

One more property of Fourier transforms, and then big picture.

The next property is the periodicity property of Fourier transforms. Part
of a much more general property that we won't talk about.

Suppose that $k\divides N$. Suppose that our superposition has period
k. The claim is that if you do a Fourier transform, you get something
that's periodic with period $N/k$ (time-frequency uncertainty
principle).

Note that this quantum Fourier transform can be implemented in $O(\log n)$
qubits, but only gives us the value at one index.

What I'll do for next time is to sweep some things under the rug; try to do
these things so very cleverly that you don't notice. Furthermore,
sometimes, if you see things too slowly, it's hard to understand. So next
time, I'll work hard to make sure that you see the big picture. So if
there's one lecture where you come in and think that you're going to be
fully alert, next time should be it.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Quantum Factoring
==================
Mar 15, 2012
-----------
Introduction
============
Turns out that if you want to factor a number, it is sufficient to show
that we just need to split it up into its components.

The length of the input is $\log N$. That is, the number of bits.

So let's say that we've got a 1024-bit semiprime N. Linear is not at all
feasible -- larger number than age of universe in femtoseconds. So what we
want is to factor this in $\log N$. The best classical algorithms we have
run in some sort of exponential time.

So what we can see is a quantum algorithm that runs in time $\log^3 N or
\log^2 N$. All that matters is that it's polynomial with respect to $\log
N$. Now, factoring is extremely important because it is used in
cryptography -- RSA (which is behind various cryptographic schemes like
TLS).

Two things we'll need to understand: quantum subroutine -- finding the
period of a periodic function, and show how we can solve factoring using
this subroutine. For that, we'll need to understand something about modular
arithmetic. We'll use these to show how to go to factoring in polynomial
time.

Period-finding
==============
Given a periodic function f with period k such that $\{0, 1, ..., M-1\}
\mapsto S$ (i.e. $f(x) = f(x+k)$), assuming that we are working $\mod M$,
and k divides N, the challenge is to find k.

To appreciate this problem, think of M and k as being very large. You are
given some circuit for computing f: you input x, and you get as output
$f(x)$. This is a little like Simon's algorithm.

Now we're going to use M as the dimension of our quantum Fourier
transform. We'll do this quickly and somehow output k. So let's see how we
do that.

Recall properties of Fourier transform: convolution-multiplication property
(this manifests as phases applied to the output, and upon measurement, this
phase conveniently drops out), treatment of periodic functions ($\sum \ket
{jk} \fourier \sum \ket{j\frac{N}{k}}$) -- output is also periodic.

Our period-finding circuit will look very familiar. What we do is start
with a Fourier transform of dimension M, and input the state zero into this
Fourier transform. The result is a superposition over all states.

Now, let's feed this into $U_f$ (with enough scratch zeroes to fill all the
inputs), then measure the second register. Suppose we get some z as the
result of our measurement. What must the first register of $U_f$ be? A
superposition over all the states equal to $z \mod k$ ($\sqrt{\frac{k}{m}}
\sum_j\ket{jk + z}$). Now, if you measure this, you get a random number
$mod M$, which gives us nothing.

So what we want to do, therefore, is put this z into the phase so we can
ignore it, i.e. feed this into the quantum Fourier transform. Now we want
to measure. Now, measuring, we get some $r\frac{M}{k}$, where r is some
random integer between 0 and $k-1$. Recall, what we want is k. We can
consider the gcd of different measurements. Presumably if we sample a few
times and take the gcd, then this will readily lead to our goal. (note that
our samples must be collectively relatively prime in order for us to get
the correct answer, so $\log N$ samples is an approximate upper bound).

Note that the gcd can be found quickly using Euclid's algorithm.

Shor's Algorithm
================
So now we want to factor. This algorithm works for any number, but for now,
we're concerned only with semiprimes: $N = pq$, for $p,q$ prime.

Here's a claim: Given that you can solve this problem (order-finding), then
you can factor. Let $ord_N(x) =$ order of $x$. (the minimum $r > 0$ such
that $x^r = 1 (\mod N)$ -- this exists iff x relatively prime to N
(Fermat's Little Theorem). This assumes that $\mathtt{gcd}(x,N) = 1$ -- if
it isn't, then we can trivially factor.

The claim is that if you can find the order of $x \mod N$, you can
factor. Classically, this wasn't even considered because finding the order
of $x$ was potentially more difficult than factoring. But now we have this
completely magical period-finding algorithm.

Claim: if the order is even and $x^{r/2} \neq -1$, one factor is either
$gcd(N, x^{r/2} - 1)$ or $gcd(N, x^{r-2} + 1)$. Somehow, once you've
figured out the order, it's easy to find the factors: this is for a very
good reason.

There's a secret fact: if N is a composite, when you take the square root
of one, you have more than just $\pm 1$. These square roots are great,
because once you've found them, you've factored the number.

Consider this:

$$\newcommand{\divides}{\big|}
x^{r/2} \neq \pm 1
\\ y \neq \pm 1
\\ y \pm 0
\\ N \not\divides\;\; y
$$

Thus

$$
x^r = 1
\\ y^2 = 1
\\ y^2 - 1 = 0
\\ N \divides y^2 - 1
\\ N \divides (y+1)(y-1)
$$

We exploit the periodicity of modular exponentiation: f is periodic with
period r (the order). Thus we can use the period-finding algorithm to
determine the order of x.

Thus to factor N, we pick x at random mod N and use the function $f(a) =
x^a$ in our period-finding subroutine. What does a subroutine do? We invoke
the subroutine many times, and each time we get some number, and we take
the gcd... and eventually, from the subroutine, we get r.

Repeat this until r is even and $x^{r/2} \equiv -1$.

Compute $gcd(x^{r/2} + 1, N)$ and output this.

Recap
=====

Note that what we stressed here was the quantum part of this algorithm. If
you wish, you can see the details of the modular arithmetic: chapter 0
in CS170 text. For those of you who have the background, last chapter.
CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Quantum Search
==============
Mar 20, 2012
------------

This is the main problem with quantum algorithms, that you're sort of
searching for a needle in a haystack.

So how long does it take? Usually a function of how large the haystack
is. One way to think of it is that you have a table with n entries, and one
of these is marked, and you're looking for the marked entry.

So classically, you could of course try every entry until you find the
marked one, which takes on the order of n steps. Randomized, you could
expect to find it halfway through your search.

What we're going to see today is called Grover's algorithm. It's a quantum
algorithm that does this in $\sqrt{n}$ steps. It's very strange in that you
get to know without looking at all of the steps.

So... what does this have to do with anything? In general, when we are
trying to find an algorithm for something (when we are trying to solve a
computational problem), our problem is as follows: given some input x, we
want to find something. Whether this is 3-SAT, TSP, factoring; we want to
find some output (answer).

Often finding the answer is difficult, but once you have it, you can
convince yourself that it's a good answer (i.e. it's in NP).

There's a real dichotomy: searching for an answer might be exponentially
hard, but checking said answer is easy (i.e. verifiable in polynomial
time).

It turns out that even though checking the answer is easy, we *believe*
that finding the answer is hard. There are all sorts of problems in
here. We strongly believe that $\mathrm{P} \neq \mathrm{NP}$. What we can
show is that if there are hard problems, then the NP-complete problems are
definitely hard.

Remember, all NP-complete problems are just search problems. So this
searching for a needle in a haystack is actually at the crux of these
problems.

Thus with Grover's algorithm, we can get a quadratic speedup for these
NP-complete problems.

Given a function $f \{1 .. N\} \mapsto \{0, 1\}$, find x such that $f(x) =
1$. We'll focus on the hardest case where we have only one possible x.

We could also convert this into a quantum circuit. Now, we can feed in a
superposition of states, and you'll get the same superposition of
corresponding outputs.

What the algorithm will do
==========================

Let's consider $N = 2^n$. The way our algorithm will work is that it'll
start with this superposition of all these states. How do we do that? A
Hadamard.

Then we apply the function, then apply a bit flip (X). All of the states of
our superposition that correspond to zeroes are now ones, and the one state
that corresponds to one is now zero.

If you were to consider these amplitudes and compute the arithmetic mean,
it's approximately $\frac{1}{\sqrt{N}}$. So what we're going to do is this
really funny operation; we're going to reflect all these amplitudes about
the mean. If it's above the mean, it goes below the mean by an equal
amount; if it's below the mean, it goes above the mean by an equal
amount. We can iterate this until roughly the halfway point.

This particular algorithm is called the quantum algorithm for searching a
database, which is nonsense. In order to use a quantum algorithm, you must
first feed the entire database into the quantum algorithm. Even if
searching took a single step, you've already taken n steps to convert it.

The entire point of this algorithm is to search for a Travelling Salesman
path, for instance.

Implementing
============

We'll start by doing a Hadamard on our original state of zeroes, and now
we'll have our superposition over all states.

For the second step, the suggestion is that we should use the circuit for
$f(x)$.

The third step is strange: it's a reflection about the mean. So what's the
mean? $\mu = \sum x\alpha^2_x \ket{\alpha_x}$. Let's first understand what
it means to reflect about the state $\ket{0^n}$. We leave alone the
component in the $\ket{0^n}$ direction and flip the phase of every other
component. So a reflection about $\ket{0^n}$ would look like
$\begin{pmatrix} 1 & 0 & 0 & ... \\ 0 & -1 & 0 & ... \\ 0 & 0 & -1 & ... \\
... & ... & ... & ...\end{pmatrix}$. So how do we do this for the uniform
vector? We simply do a change of basis such that this is our zero vector,
do this operation, then revert our basis. This is usually called D, our
diffusion operator. $D = H^{\otimes n} Z^\prime H^{\otimes n}$.

Replacing each $\alpha_j$ with $\beta_j \equiv 2\mu - \alpha_j$ is just the
reflection about the mean.

Actual circuit
==============

We know how to do the first two stages. Then we want this special
transformation: first do a Hadamard, then reflect about $\ket{0^n}$. We
have a function that outputs one whenever this isn't $\ket{0^n}$ --
something we can implement classically, so this is not a problem -- then we
can feed this into our conditional phase flip.

The important thing about these algorithms is that they have special
structure, and so they have small corresponding quantum circuits.
