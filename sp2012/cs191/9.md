CS 191: Qubits, Quantum Mechanics and Computers
===============================================
Continuous Quantum States, Free particle in 1D, Heisenberg Relation
--------------------------------------
Feb 14, 2012
------------

So far we've talked about discrete quantum systems. Today, we're going to
make the transition to talking about continunous quantum systems. In 1
dimension, the particle can be anywhere on the line.

Schrödinger's equation in 1 dimension, Heisenberg relation.

iℏ(∂Ψ(x,t)/∂t) = [-(ℏ²/2m)(∂²/∂x² + V(x)] Ψ(x,t) = HΨ(x,t)

Some things where won't be terribly careful about being precise for
now. Will fix later to extent that you won't be too upset about it
later. Attempt to day is to try to understand these objects on an intuitive
level. Before we do that, let's do a five-minute review of discrete
systems, just to orient ourselves when we go to continuous distributions.

Back to the familiar k-state systems. State is a unit vector in k-dim
Hilbert space, etc. And then we have this notion of an observable (given by
a Hermitian matrix M ≡ M†). What we like about this is that by the spectral
theorem, we have an orthonormal basis of eigenvectors corresponding to real
eigenvalues. Result is that discrete energy levels correspond to orthogonal
vector spaces.

Since this is an observable, the deflection of our meter is λj with
probability |〈Ψ|Φj〉|², and the new state is |Φj〉. We could ask a
couple of questions.

Before we continue: let's look at another way of considering M being
Hermitian: Mij = 〈Φi|M|Φj〉 = conj(〈Φj|M|Φi〉) for any Φi, Φj; not
necessarily just for basis vectors.

	* We can try to picture the measurement outcome. What our measurement
	  looks like is this: some probability distribution. We can
	  characterize this distribution by its moments, much like how we can
	  characterize a function by its derivatives. The more moments we have,
	  the more we know about our distribution.
		+ mean: location.
		+ standard deviation: width.
		+ skewness: symmetry.
		+ kurtosis: peakedness.
	* We can consider the mean to be 〈Ψ|M|Ψ〉.
		+ First write M in its eigenbasis, where it's a diagonal matrix.
	* We can also do the same for variance. Var(X) = E(X²) - E(x)² = E(X²)
	  - μ², for obvious reasons. E(X²) = 〈Ψ|M²|Ψ〉: intuitive after
	  considering that M² preserves eigenvectors while squaring eigenvalues
	  (result of diagonalizability of M).

Here's what we're planning to do (hopefully) for the rest of the lecture:
it'll be in the form of a sketch, which hopefully will give you a picture
of what happens when you look at a particle in 1 dimension.

Before that: σ (standard deviation) is a measure of spread. If you were
really certain about a physical quantity, you'd have σ ≡ 0.

So, let's have a particle that's free to move about in one dimension. We'll
describe approximately for now; we just want to understand its form. Later
will be much precise, but now we just want to put an image in our minds as
to just where this equation comes from. We'll want to understand position
of the particle x (how it behaves) as well as momentum p. We'll also look
at the uncertainty relation that says ΔxΔp ≥ ℏ/2 (some constant). Show
intuitively that there must be some minimum.

In this continuous picture, x and p behave like something you've already
seen: bit and sign. There's something about the spread of the two. You
cannot know both of those quantities precisely, either in the one, or the
other, or both. The more certainly in one, the less certainly in the
other. Rather than do it by formula and precisely, we'll do it more
intuitively.

Often, when you think you want to explain something so that people really
understand, you want to go slowly. Paradoxically, it's sometimes better to
go fast. Explanation: it's easier to put all the pieces together when you
see the big picture all at once. See big picture first, then observe
individual bits later.

We want to talk about a lot of stuff.

Once again, what we are trying to do is describe the state of the particle
on an infinite line. So now, before you describe it, let's do an
approximation. Let's consider this as not infinite, but very long (take a
limit). Likewise, not continuous but very fine (also a limit). Could be at
one of various positions. Describe your state as this superposition of
states. What we're saying is that Ψ(j) is αj. When we generalize this to a
particle being anywhere on the line, the way to describe it is Ψ being a
continuous function, so Ψ(x) is a complex-valued function on the real
line. As in the discrete case, we want our distribution to be normalized.

Now, suppose we wanted to measure the position of this particle. Out here,
we'd have an observable, M. The corresponding observable in the continuous
case, let's call it x. We'd just do 〈Ψ|x|Ψ〉. Our inner product now is
defined with integrals.

Our observable now takes as input a wave function Ψ and spits out another
function as output.

More fuel for intuition: you should know one of the big discoveries: nature
is described by local differential equations. Every point in space only
considers its own neighborhood. It's concerned only with itself, and
nothing else. So now let's apply that principle here. How that wave
function evolves over time. So the point x is minding its own business and
its own infinitesimal neighborhood. So what does it do? The simplest thing
it does is compare itself to its neighbors, say the average of its
neighbors. (consider perceptron, maybe?) But this yields the second
derivative with respect to x, and the function smooths itself out. So we
must move in an orthogonal direction to avoid collapsing the wave
function, i.e. multiply by i.

Let's now try to understand where the uncertainty principle comes in.

Momentum we can measure in the quantum case; it's sort of a proxy for
velocity, since velocity doesn't really make sense.

Let's consider a fairly standard wave exp(i(kx-ωt)). Now you ask what the
equation of motion says if it's going to evolve. Let's say Ψ(x,0) ≡
exp(ikx). ω is the rate at which it twists over time. A twisting seems to
correspond to some sort of translation. The rate of translation is directly
proportional to k.

So now we have a function of definite momentum. We can then decompose this
wave function in terms of these functions. These are our Fourier basis
functions. So if you want to go from the position basis to the momentum
basis, you take a Fourier transform. In other words, this is roughly
equivalent to what we do when we go from the sign basis to the bit basis,
or vice versa (what a Hadamard gate does).
