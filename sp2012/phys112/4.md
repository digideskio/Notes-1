Physics 112: Statistical Mechanics
==================================
Kinetic Theory 3 / Probabilities. January 27, 2012
--------------------------------------------------

Various probability stuffs. Continuous distirbutions. Sum/average of
independent events. Central limit theorem.

  P(s) ≥ 0, ∑ P(s) = 1
  〈y〉 = ∑y(s)P(s) [ this is the first moment. ]

  y(s) = 35 δ{i,k}. 〈y〉 = ∑ 35δ{i,k} = 35/38.

Variance:
  σ² = 〈(y(s) - 〈y〉)²〉 = 〈y²〉 - 2〈y〈y〉〉+ 〈y〉² = 〈y²〉 - 〈y〉²

root mean square (rms) ≡ standard deviation

Independence. Usually, if I have two random variables, x and y, the
probability of P(x,y) = P(x|y)P(y) ≠ P(x) × P(y). We say we have
independence iff P(x,y) = P(x)P(y). In other words, P(x|y) = P(x).

You can define a correlation coefficient to be
  (x - 〈x〉)(y - 〈y〉)/(σxσy).

Independence ⇒ ρ = 0. Converse not necessarily true.

Continuous distributions. Histograms. The case where a variable is
continuous. We now have probability _densities_.

  f(x)dx = g(y)dy = f(x,y)(dx/dy)dy.
  f(x) ≥ 0, ∑f(x)dx = 1.

Moments: we can define moments in exactly the same way as before.
The moment of a variable y(x)

〈y(x)〉 = ∫y(x)f(x)dx.
  μ = 〈x〉 = ∫xf(x)dx.
  σ² = 〈x²〉 - 〈x〉² = ∫x²f(x)dx - (∫xf(x)dx)².

f(x,y)dxdy = g(x)dx h(y)dy. Factoring works the same way, if our
variables are independent.

Normal distributions: Gaussian.

It is very important to put the differential element (dx or dy) because the
function changes depending on what the differential element is. The
histogram depends on what variable you choose to plot. If I choose x or x²,
my histogram will be different. Usually.

The mean is in the middle of a normal distribution because the third
moment is 0.

Full-width half maximum (FWHM) ≈ 2.3σ

A is called the mode, i.e. the maximum. In a distribution with nonzero skew
(like the Maxwell-Boltzmann), the mode is different from the mean.

mean: location.
standard deviation: width.
skewness: symmetry.
kurtosis: peakedness.

Fourier transform is the characteristic function. The log of this is stuff
with cumulants.

Sum of random variables:
x ≡ y + z
〈x〉 = 〈y〉 + 〈z〉.
σ²x = σ²y + σ²z + 2ρσyσz.
Independence ⇒ ρ = 0; σ²x = σ²y + σ²z

h(x)dx = (f*g)(x)dx ⇒ the cumulants add!

Proof: Convolution in original space is equivalent to product in Fourier
space. Hence for a sum, the characteristic functions multiply and the logs
of characteristic functions add.

Central limit theorem: Cool if our variables actually are independent.
