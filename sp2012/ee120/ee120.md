EE 120: Signals and Systems
===========================
January 17, 2012.
-----------------
* BABAK
* email: AYAZIFAR@EECS.BERKELEY.EDU
  + Best way to contact.
	- If time-sensitive, make subject line explosive. Literally.
	- Does his best to respond. So that's that.
* Office: 517 Cory

Grading:
--------
* Three midterms.
  + MT1: 20%. Date: Feb 14. Week of drop date (almost certainly)
  + MT2: 25%. Date: TBA. Percentage tentative (alt. 20%)
  + MT3: 25%. Date: Last lecture.
* Homework: 15%, drop lowest two scores.
  + 4-6 homeworks.
  + Work in groups of 3-5 people.
  + Each member of group turns in separate document.
	- Must constitute primarily own work.
	- Write own name at top, names of collaborators beneath.
  + Tentatively due Tuesdays so Babak can have OH on Monday evenings.
* Pop quizzes (often obvious when): 15%. Drop lowest one.
* Possible research paper project. 10% => MT2, MT3 weigh 20% each.
  + Just get feet wet with journals.
  + Same for everybody.

Since the course is not HW-heavy, there are weeks we don't have
homework. So get together with groups and plow through old exams.

The other thing you can do is look at a couple of books. Most of them
are with this title: Signals & Systems. One of them is by Oppenheim,
Willsky, Nawab. This one's an expensive book, actually. Don't
recommend you go out and buy it unless you can find one printed
overseas. Excellent problems -- mostly MIT exams. Taught out of this
book previously. (both are second-edition)

There's another one with the same title by Hwei P. Hsu (Schaum's
Outline) -- great for self-study because every problem is solved.

Basic Properties of Systems
===========================
* Linearity
  + $x\to[F]\to y$
  
	if:

	$x_1\to[F]\to y_1$
	$x_2\to[F]\to y_2$

	then:

	$\alpha x_1\to[F]\to\alpha y_1$   (scaling / homogeneity property)
	$x_1+x_2\to[F]\to y_1+y_2$ (additivity property)
	  
	if:

	($\forall x_1,x_2\in X, \alpha_1 x_1\to\alpha_2x_2\to[F]
	\to\alpha_1 y_1+\alpha_2y_2$),

	then:

	($\forall x_1,x_2\in X, \alpha_1 x_1+\alpha_2x_2\to[F]
	\to\alpha_1y_1+\alpha_2y_2$),
		
	(where:

	  $x \in X$    input signal space
	  
	  $y \in Y$    output signal space
	  
	  $y = F(x)$
	  
	  $y(t)$     output at time t
	  
	  $y(n)$     output of sample n)
	  
  + example: resistor and stuff. Ohm's law. $\vec{J} = \sigma \vec{E}$ and
	whatnot.
	- By changing the system into a time-variant system, it doesn't
	  lose linearity.
	- ZIZO.
	  - converse not true.
	  - contrapositive/logical equiv: $\not \text{ZIZO} \implies \not
        \text{L}$.
* Modulation
  + Parallel interconnection of linear systems produces a
	multiple-input linear system.
	- Strictly speaking, this is a lie. It's not really parallel.
  + Two-point moving average:
	- Scaling and additivity properties.
* Time Invariance
  + $\hat{x}(t) = x(t-T) \implies \hat{y}(t) = y(t-T)$
	$\forall x\in X, \forall T \in \mathbb{R}$

	Hidden assumption: X is closed under shifts. (i.e. $x\in X \implies
	\hat{x}\in X$)
  + occurs, e.g. when something that isn't an input is time-dependent.
* Causality
  + Basically, no dependence on future values.
	- If two inputs are identical to some point, then their outputs
	  must also be identical to that same point.

EE 120: Signals and Systems
===========================
January 19, 2012.
-----------------

More on causality
-----------------
(proof by contradiction of previous example)

(input zero up to a certain point, but the output is nonzero before said
point. output preceding input. In itself, it does not tell non-causality,
but if you know the system is either linear or time-invariant, then you
know it can't be causal.)

L and C implies not P

If the system is linear AND causal, output cannot precede the input.

contrapositive: P implies not (L and C) = not L or not C.

for time invariance, we don't have to insist that the input is zero up to a
point; it just has to be (any) constant. Output also doesn't have to be
zero; it just has to be constant (not necessarily the same constant,
obviously).

(TI and C) implies not P
contrapositive: P implies not TI or not C.

anti-causality is defined as the exact opposite of causality. Everything
we've said about causality, the inequalities change sign.

Bounded-input bounded-output (BIBO) stability.
----------------------------------------------

if $\abs{x(n)} \le Bx (\exists Bx < \infty) \implies \exists 0 < By <
\infty \text{ s.t. } \abs{y(n)} \le By \forall n$

every FIR (finite-duration impulse response filter) is BIBO stable.

LTI (linear time-invariant) systems
-----------------------------------

$x(n)$ can in general be decomposed into into a linear combination of
impulses. (Kr\"onecker deltas). $x(n) = \sum_{m=-\infty}^\infty
x(m)\delta(n-m)$. (sifting property)

If you know the response of the LTI system to the unit impulse, you know
the response to all discrete-time inputs -- decompose it into unit
impulses.

convolution sum.
$$
x(n) = \sum_m x(m) \delta(n-m)
\\ y(n) = \sum_m x(m)f(n-m)
\\ y(n) = (x * f)(n)
\\ y(n) = \sum_m^\infty x(m)f(n-m) = \sum_{k=-\infty}^\infty f(k)x(n-k)
\\ = (f*k)(n)
\\ \therefore x * f = f * x
$$.

You can reverse the roles of the impulse response and the input signal.

Let $x(n) = e^{i\omega n}$. $y(n) = F(\omega) = (\sum_k f(k)e^{-i\omega
k})e^{i\omega n}$.  Frequency response of this LTI system.

Converges nicely iff the system is BIBO-stable. The frequency response of
the system will be a very smooth function of omega.

BIBO stability for LTI systems
------------------------------

An LTI system F is BIBO stable iff its impulse response converges
(absolutely summable in discrete case or integrable in continuous case).

IF the impulse response is absolutely summable, then F is BIBO stable
(every bounded input produces a bounded output).

$$y(n) = \sum_k f(k)x(n-k) \text{ (I/O relation for an LTI system)}.
\\ \abs{y(n)} = \abs{\sum_k f(k)x(n-k)} \le \sum_k \abs{f(k)}\abs{x(n-k)}
\\ \abs{x(n)} \le Bx \forall n.           \le (\sum_k \abs{f(k)})Bx$$

F is BIBO stable $\implies \sum_n \abs{f(n)} < \infty$

contrapositive. i.e. we can find at least one bounded input that produces
an unbounded output.

(fudge something to cancel out signs; we create our absolute sum over all
integers @ n=0. convolve signal with time-reversed signal;
auto-correlation. convolution of a signal with itself.)

$\frac{f(n)^2}{\abs{f(n)}} = \abs{f(n)}^2/\abs{f(n)} = \abs{f(n)}$

IIR Filters
-----------
$y(n) = \alpha y(n-1) + x(n)$.  causal, $y(-1) = 0$.

(geometric sum; BIBO stability depends on magnitude of \alpha being less
than
1.)

EE 120: Signals and Systems
===========================
January 24, 2012.
-----------------

LTI Systems and Frequency Response
==================================
Freq. response: $H(\omega)$
(continuous-time?)

Will learn set of systems for which sum doesn't converge, so we'll need a
new transform: laplace (Z) transform

Two-point moving average:
-------------------------

$h(n) = (\delta(n) + \delta(n-1))/2$

$H(\omega) = (1 + e^{-i\omega})/2 = e^{-i\omega /2}(e^{i\omega /2} +
	 e^{-i\omega /2})/2 = e^{-i\omega/2}cos(\omega/2)$

frequency response of discrete-time systems is periodic: adding a multiple
of $2\pi$ will not change the result.

$2\pi$-periodicity of discrete-time LTI frequency responses. This means,
naturally, that we don't have to plot our functions everywhere; rather, we
only need to worry about a single period.

(looks like $\abs{cos(\frac{\omega}{2})})$

eigenfunction property of discrete LTI systems -- $e^{i\omega} \to
H(\omega)e^{i\omega}$

$H^*(\omega) = \sum h^*(k)e^{i\omega k}$

conjugate symmetry: $H^*(\omega) = H(-\omega)$. (generalization of "even"
functions)

if $h(n) \in \mathbb{R} \forall n$, $y(n) = Re{H(\omega)e^{i\omega n}} =
|H(\omega )|cos(\omega n+\angle H(\omega))$

$x\to H\to y$

$h(n) = (1-\alpha ^n)/(1-\alpha)$

Ways to determine H(\omega):

Method 1:
---------

$H(\omega) = \sum h(n)e^{-i\omega n} = \sum (\alpha e^{-i\omega})^n$. Use
usual formula for geometric series, since this converges.

Method 2:
---------

Use eigenfunction property of the complex exponential. Frequency response
is probably defined. Let $x(n) \equiv e^{i\omega n}$ (a pure tone).

$y(n) = H(\omega)e^{i\omega n}. y(n-1) = H(\omega)e^{i\omega n}e^{-i\omega}$.
$H(\omega) = 1/(1-\alpha e^{-i\omega})$.

How we can plot the magnitude response
--------------------------------------

eliminate all negative complex exponentials. $\alpha$ is in the unit
circle. $e^{i\omega}$ is a point on the unit circle, so we can represent it
with a vector.

Consider graphically.

EE 120: Signals and Systems
===========================
January 26, 2012.
-----------------

numerator is 1, denominator is length of vector $e^{i\omega} -
\alpha$. Task: plot ratio as $\omega$ varies.

$1/(1+\alpha), 1/(1-\alpha)$.

frequency response curve should be monotonically increasing from $-\pi$ to
0 and decreasing from 0 to $\pi$. [ talk about inflection points, concavity,
etc. Second derivatives. ]

# Low-pass filter for $0<\alpha<1$. If we understand the geometry for this
  particular case, we can answer design-oriented questions. One question
  is, what if I want to sharpen the peak of this low-pass filter?

  * Answer: have $\alpha$ approach 1. Can't get too close -- real-world has
	noise. Also, in trouble if $\alpha$ jumps onto or outside of the unit
	circle. Algebraic argument available, as well.

	: move $\alpha$ toward the unit circle (but still keep it inside)

# Can I make a high-pass filter out of this?

  * Yes. Set $-1 < \alpha < 0$.
	: take $\alpha$ to $-1 < \alpha < 0$

  * By taking $\alpha$ to be negative, we have the equivalent of a phase
	shift by $\pi$.

# Sharper high-pass filter?

  * Bring $\alpha$ closer to -1.

# What if we want the filter to peak @ an arbitrary frequency, say
  $\omega_0$ = $\pi/4$?

  * Place $\alpha$  along the pole $\theta = \omega_0$.

  $1/(1-\abs{\alpha})$

  observation: this filter is a complex filter (i.e. one whose impulse
  response $h(n)$ is complex valued). This has its uses but is not always
  desirable.

  $h(n) = \alpha^n u(n) = R^n e^{i\omega_0n}$

  $g(n) = h(n)e^{i\omega_0n} \implies G(\omega) = H(\omega -\omega _0)$

Example:
--------
LCCDE: linear constant coefficient difference equation

$y(n) = \alpha y(n-N) + x(n)$. N is a positive integer, not necessarily 1.
$y(-N) = ... = y(n) = 0$

$(\abs{\alpha} < 1)$

$$h_{N}(n) = \alpha h_{N}(n-N) + \delta(n) = !(n % N) && \alpha^{n/N}.
H_{N}(\omega) = \sum\alpha^{k}e^{-i\omega Nk} (k from 0 to \infty)
  \sum(\alpha e^{-i\omega N})^k = 1/(1-\alpha e^{-i\omega N})

\abs{H_{N}(\omega)} = \abs{e^{i\omega N}/(e^{i\omega N} - \alpha)}
$$

Use $\omega \equiv \omega N$, then use a change of variables (after
plotting) to recover the initial basis and label axes and whatnot.

Or use some arbitrary argument that it's a contracted version by going back
to $H(\omega)$. Roughly equivalent either way.

Graph of $\abs{H(2\omega)}$, for $0<\alpha <1$.

(No more graphs.)

Comb filter. (pick off values that are multiples of \pi/N)

design-oriented analysis. layers like an onion. they fit together very
well.

Analog system
-------------
$g(t) = e^{-\alpha t}u(t), \Re{\alpha} > 0$

$G(\omega) = \int g(t)e^{-i\omega t}dt
   \\ = 1/(\alpha  + i\omega ).
   \\ = 1/(i\omega  - (-\alpha ))$

$\sqrt{\omega ^2 + \alpha ^2}$

EE 120: Signals and Systems
===========================
January 31, 2012.
-----------------

Homework should be out today. Yes, there are homeworks in this class.

analog system procedure for figuring out frequency response of
systems.

Fourier analysis today. Some of this will be review, but we'll dive
more deeply into the linear algebra in this class.

Roadmap:
 * Fourier Analysis
   "A way of decomposing signals into their constituent frequencies.
   Kind of like the way a prism splits light into its components.
   Fourier analysis gives us the tools we need to analyze signals."
   + Periodic [ in time domain. ]
	 - Discrete time, discrete time Fourier series.
	 - Continuous time, continuous time Fourier series.
   + Aperiodic
	 - Discrete-time Fourier transform.
	 - Continuous-time Fourier transform
   We can put blocks around these. Discrete-time signals are periodic
   in the frequency domain.

   Before I do that, I want to review an abstraction that we use for
   signals (and the way we look at signals) as vectors.

Periodic DT Signals
===================

$x(n+p) = x(n) \forall n \exists p \in \{1,2,3,4,...\}$

p is called the period of x. The smallest p for which this is true is
called the fundamental period.

With discrete-time signals that are periodic, you always have to find out
the period before you can talk about frequencies. $\omega_0\equiv2\pi/p$ is
the fundamental frequency.

Let's say we have a signal defined as ...4/2/4/2/4/2...

We can abstract and represent this signal as a cartesian vector. A
euclidean vector. Namely, I can go to p-space and draw $\mathbf{x}$. I
basically take the values in that one period and stack them
up. $x=[4, 2]$. I can ask you a couple of questions: what the
representation of this vector is in terms of two canonical vectors:
$\psi_0=[1,0]$, $\psi_1=[0,1]$. $\ket{x} = 4\ket{\psi_0} +
2\ket{\psi_1}$. We do this via projection. Or a change of basis. $[4,2] =
4[1,0] + 2[0,1]$. Inner products and stuff. $\braket{x}{\psi_0} +
\braket{x}{\psi_1}$.

$\braket{x}{\psi_0} = (x_0\psi_0 + x_1\psi_1)^T\psi_0 = x_0(\psi_0)^T\psi_0
+ x_1(\psi_1)^T\psi_0 = x_0\psi_0 + 0$.

$x^T\psi_0 = x_0\psi_0^T\psi_0$

$x_0 = x^T\psi_0/(\psi_0^T\psi_0)$

Just use inner product with normalized basis vectors.

In this case, it was easy to find out what the coefficients were. I can
repose the question by changing the $\psi_k$. I'm going to rotate the basis.

Basically, non-normalized sign basis. [3, 1].

[ talk about how we only need two orthogonal vectors, since our fundamental
  period is 2. Don't need a third vector. ]

Now, there's something special about these two frequencies we found.

Harmonics. Integer multiples of the fundamentals. Number of terms equals
period. Certainly what this toy example is indicating.

Now the question is, how do we find these coefficients? Oh wait, we've
already solved that problem.

INTERMISSION
============

I'm going to recast what we just did in matrix-vector form. We said that x
as a vector can be represented as a linear combination of $\psi_0$ and
$\psi_1$. I chose the period over the interval 0 to 1. I can do the same
thing with $\psi_0$ and $\psi_1$.

This is just the matrix for the FT. I have $x = x_0\psi_0 + x_1\psi_1 =
F[x_0,x_1]$, where $F = \begin{bmatrix}1&1\\1&-1\end{bmatrix} =
[\psi_0, \psi_1]$. This is a 2x2 matrix.

I can write this as $x = \Psi X$. Solve for X: left-multiply by inverse Fourier
transform. $F^{-1} = \frac{1}{n}F^\dagger$.

INTERMISSION
============

Once you have complex numbers, you can no longer use the dot product. We
now need the inner product. $\braket{a}{b} = a(T)b^*$. Fails otherwise, since
$\braket{x}{x} \neq \abs{x}$.

$x$ is $p$-periodic.

$\vec{x} = [x(0), ... , x(p-1)]$. I have $\psi_k \perp \psi_l$, where
$b\braket{\psi_k}{\psi_l} = p\cdot\delta_{kl}$.

Proof of orthogonality of $\psi_k$, $\psi_l$, $k \neq l$. Geometric sum,
not very interesting.

$x$ is the linear combination of our $\psi_k$.

$\braket{x}{\psi_l} = X_l\braket{\psi_l}{\psi_l} \implies X_l =
\frac{1}{p} \braket{x}{\psi_l}$.

synthesis equation, analysis equation:

$x(n) = \sum X_k e^{ik\omega_0n}$
$X(n) = \sum x_k e^{-ik\omega_0n}$

EE 120: Signals and Systems
===========================
February 2, 2012.
-----------------
DTFS
====
$x(n) = \sum X_k e^{ik\omega_0n}$. The complex exponentials form a basis
for $\mathbb{C}^2$. The way we define inner products for signals is exactly
as you'd imagine. $\sum \psi_k(n) \psi^*_l(n)$. Frequency periodicity:
$\psi_{k+p} \equiv \psi_k$. Only in discrete-time periodic case are our
functions guaranteed to be both periodic in time as well as frequency.

$x(n) = cos(n)$ not periodic in discrete time $\implies$ no DTFS.

Evidently there will be a quiz on Tue, since pset is due on Wed.

Consider what it means to send discrete-time periodic signals through
LTI systems.

EE 120: Signals and Systems
===========================
February 7, 2012.
-----------------
CTFS:

periodic if $x(t+p) = x(t) \exists p \in R$. Smallest positive $p$ is
called the fundamental period. Fundamental frequency $\omega_0 \equiv
\frac{2\pi}{p}$.

Can also write $p = 2\pi/\omega_0$. In discrete-time, writing it this way
was dangerous, since depending on $\omega_0$, $p$ may or may not be an integer.

For discrete-time signals, the constant signal was periodic with $p=1$.
$x(t) = 1 \forall t \in Z$?

What about the signal $x(t) = 1 \forall t \in R$? The fundamental period is
undefined: any $p>0$ can serve as a period.

So there are subtleties in each story. In the discrete-time story, there
were some sinusoids that looked periodic but weren't, and the constant
signal has no fundamental period in continuous-time.

We're going to jump immediately into the Fourier series. He said you can
decompose any continuous signal as a linear combination of complex
exponentials that are related to each other by virtue of being at
frequencies that are integer multiples of the fundamental period.

$x(t) = \sum X_k e^{ik\omega_0t} = \sum X_k\psi_k$.

We know the procedure for finding the kth coefficient. Before we go there,
there's something you ought to pay attention to in this expression. When I
draw a typical periodic signal, when I look at one period, how many points
do I have? Uncountably infinite. Also range is potentially a set of
uncountably many values. So this is a bold claim: we can represent these
with a countable number of eigenfunctions.

Unlike the discrete-time story, this equality will not always be a
pointwise equality. There are different gradations of convergence. Whenever
you have an infinite sum, you have to worry about convergence in the back
of your head. For well-behaved signals, the left and the right converge,
and this is true for every t. The less well-behaved signals will no longer
hold pointwise. Strange things happen, e.g. Gibbs phenomenon.

You'll have a reasonable understanding of Fourier series. We're not going
to worry too much about convergence in this class. The only time it doesn't
arise is in the discrete-time Fourier series.

Claim: Fourier analysis works. One path we can take is for you to take my
word for it. Or we could prove it. Since last time was hilarious, we're
going to take this for granted, for now. Assume orthogonality of $\psi_k$
for some definition of the inner product. $\psi_k = \exp(ik\omega_0 t)$.

I am now going to determine $X_l$. Take the inner product of $x$ with
$\psi_l$.

The procedure is exactly the same. We're just swapping out our definition
of inner product. Exploit the orthogonality.

For discrete-time p-periodic signals, we defined the inner product as
$\braket{f}{g} = \sum fg^*$. Guess what the continuous-time inner product
is for $p$-periodic signal!

And if they're non-periodic, we'll do the same, but over all time.

Show that our eigenfunctions are orthogonal.

Synthesis equation: $x(t) = \sum X_k \exp(ik\omega_0 t)$
Analysis equation:  $X_k = (1/p)\int x_k\exp(-ik\omega_0 t)$.

How do I show that $\braket{\psi_k}{\psi_l} = 0 (k\neq l)$? Just evaluate
the integral. We get an exponential with period p, integrated over a period
p? Looks like 0 to me.

Example: $X(t) = cos(\pi t/3)$. $(\exp(i\pi t/3) + \exp(-i\pi
t/3))/2$. $\psi_1 = \psi_{-1} = \frac{1}{2}$.

$q(t) = \sum\delta(t-\ell p) = \sum Q_k \exp(ik\omega_0t)$
$\delta(t) = \deriv{u(t)}{t}$

Poisson's identity. $\sum\delta(t-\ell p) = \frac{1}{p}\sum\exp(ik\omega_0t)$

$R_k = \frac{1}{p}\int r(k)\exp(-ik\omega_0t) = \frac{1}{p}$ if $k=0$ else
$\frac{\sin(k\omega_0\Delta/2)}{pk\pi\Delta/2}$

What happens if I want to approximate a signal that has finite energy? What
should the coefficients $\alpha_k$ be?

orthogonal projection! Least squares!

EE 120: Signals and Systems
===========================
February 9, 2012.
-----------------

Discrete-time Fourier transform
===============================

Discrete aperiodic signals. The DTFT can also handle discrete-time periodic
signals, provided that we make use of Dirac deltas in the frequency
domain. You all should remember that the frequency response of a
discrete-time LTI system $H(\omega) = \sum h(\ell)\exp^{-i\omega\ell}$
[DTFT analysis equation]. It turns out this is the DTFT of the impulse
response.

How to go from frequency domain to time domain, i.e. how to derive impulse
response from frequency response.

Our goal is to determine h from H. That's what we want to do. And I'm going
to do this in two ways, both of which rely on things we've already done. So
there's nothing new that I'm going to go through.

Recall: With discrete LTI systems: frequency response has fundamental
period $2\pi$. $H(\omega+2\pi)=H(\omega)$. We already possess the
mathematical machinery to handle periodic continuous variables (i.e. we've
seen this in the CTFS). Then, our continuous variable was time. Now, it's
frequency.

Recall CTFS. $x(t)$. $p$: fundamental period. $\omega_0 = \frac{2\pi}{p}$:
fundamental frequency. We said we can express x as a linear combination of
complex exponentials that are harmonics (integer multiples) of the
fundamental frequency. We had an expression for these. $X_k =
\frac{1}{p}\int x(t)e^{-ik\omega t}$.

I'm going to draw parallels now with the current scenario: $x\to
H$. $t\to\omega$. $p\to2\pi$. $\omega_0=\frac{2\pi}{p}\to\Omega_0 =
\frac{2\pi}{2\pi}=1$. $\therefore X_k = h(-k)$.

Let $k\equiv-\ell$ in equation ($\star$). $H(\omega)=\sum
h(-k)e^{ik\Omega_0\omega}$.

Our coefficients are the $h(-k) = \frac{1}{2\pi} \int_{\avg{2\pi}} H(\omega)
e^{-ik\omega} d\omega$. This is exactly parallel to the previous
expression.

$h(\ell) = \frac{1}{2\pi}\int H(\omega)e^{i\ell\omega}d\omega$. Synthesis
equation.

DTFT equations:
$H(\omega) = \sum h(n)e^{-i\omega n}$.
$h(n) = \frac{1}{2\pi}\int H(\omega)e^{i\omega n}d\omega$.

$h(n) = \int \frac{d\omega H(\omega)}{2\pi}e^{i\omega n}$. Linear combination of
complex exponentials. $H(\omega)$ is a measure of the contribution of
frequency $\omega$ to the function $h$.

For now, we're working with the universe of functions of the continuous
variable $\omega$ which happen to be periodic with period $2\pi$.

Ideal discrete-time Low-pass Filter
===================================

Impulse response $h(n) = \frac{1}{2\pi}\int H(\omega)e^{i\omega n}d\omega$.

$\frac{B}{\pi} n \sin(An)$

EE 120: Signals and Systems
===========================
February 16, 2012.
-----------------

Discrete-Time Fourier Transform: continued
------------------------------------------

Recall the ideal low-pass filter. In time-domain, it had some
discontinuities. In freq domain, it was NOT absolutely summable. However,
it is square-summable. We have names for signals that behave in these
particular ways.

$\ell_1$ signals: absolutely summable. $\sum \abs{x(n)} < \infty$ (abuse of
notation, but useful at that).

$\ell_2$ signals: Square-summable. $\sum \abs{x(n)}^2 < \infty$. Finite
energy.

For discrete-time, if a signal is $\ell_1$, it is $\ell_2$. Converse not
true.

$\sum\abs{h(n)} = \sum_{n=0 \to \infty} \abs{\alpha}^n =
\frac{1}{1-\abs{\alpha}} < \infty$. $h \in \ell_1 \implies$ finite
$\abs{H(omega)}$ and smooth.

If $x \notin \ell_1$ but $x \in \ell_2$, you run into a bit of a problem:
cannot use analysis equation, since summation will not converge. But we can
define the Fourier transform in that case to be the limit as $N \to \infty$
of $\sum_{-N \to N} x(n)e^{-i\omega n}$. DTFTs are continuous (not
necessarily smooth / infinitely differentiable).

Turns out we get convergence in energy of the two signals. It's just for
you to bear in mind, since you have an infinite sum. The most well-behaved
functions are $\ell_1$. They have nice DTFTs. Next level up in terms of
misbehavior is $\ell_2$. Fourier transforms cannot be obtained through
analysis equation, but can be reverse-engineered or otherwise. DTFTs have
discontinuities.

We have yet another level up in terms of misbehavior. We have signals of
"slow growth". (zero growth) Examples: $x(n) = 1$, $x(n) = n$, $x(n) =
e^{i\omega_0 n}$. Basically, these are signals that grow no faster than
polynomially in time. (and signals that neither grow nor decay). Notice
these signals $\cap (\ell_1 \cup \ell_2) = null$. Cannot use analysis
equation, but can use synthesis equation. And their DTFTs have Dirac
deltas.

Example:

$x(n) = e^{i\omega_0n} X(omega)$? $\alpha\delta(\omega-\omega_0$). Strictly
speaking, we'd have to write the spectrum as a sum: we have a delta every
$2\pi$, starting at $\omega_0$. But that's not particularly interesting,
since that looks messy. I'm just interested in the interval here. As long
as we know that it's $2\pi$-periodic.

To get our $\alpha$, we can and will use our synthesis equation:
$\frac{\alpha}{2\pi}\int \delta(\omega-\omega_0 e^{i\omega
n}d\omega$. $\frac{\alpha}{2\pi} e^{i\omega_0n} = e^{i\omega_0n} \implies
\alpha \equiv 2\pi$.

$\pi\delta(\omega-\omega_0) + \pi\delta(\omega+\omega_0)$ ($2\pi$-periodic,
again)

For signals that belong to none of the previous categories, we have to
learn a new transform -- Z-transform! Z-transform! Coming in March, to
theaters near you.

Why do $\ell_1$ signals have finite DTFTs?

triangle inequality, following directly from the synthesis equation.

INTERMISSION
============
DTFT Properties
---------------
# Time-shifting

If I have a signal that's Fourier-transformed? What do I get in the
frequency domain if I shift the original signal?

We get a phase shift: if we shift x(n) by N, our new $X(\omega)$ is
multiplied by $e^{-i\omega N}$.

## Example / curveball

$H(\omega) = e^{-i\omega/2}4$
which yields $h(n)= \frac{(-1)^{n+1}}{\pi}(n-\frac{1}{2})$

Remember: convolution and multiplication are duals over the time and
frequency domains.

Not too long from now, we are going to start studying the sampling
theorem. One way to consider this as a half-delayed sample of X is to think
of X as having samples. We map this to a sequence of samples in continuous
time, split apart by T seconds (sampling period). Interpolation scheme that
we'll learn about, etc. I can shift to the left or right by an arbitrary
amount. So in continuous time, I take this signal and I shift it to the
right by T/2 seconds. So I take whatever this was, take samples spaced T
apart, then convert it back into Kr\"onecker deltas.

This filter is called, for the reason described, a half-sample delay. You
essentially have to interpolate between the frequencies in discrete time
and sample the halfway points.

What happens if I multiply in the time domain by a complex exponential
(modulation)? The frequency domain is convolved with said exponential.

Using the analysis equation,we have $X(\omega) = \sum x(n) e^{i (\omega -
\omega_0) n} = X(\omega-\omega_0)$.

# Multiplying the time domain by $e^{i\omega_0}$ results in a frequency shift.

Multiplying by a complex exponential yields a shift in the other domain.

There is one property that is distinct, and that is the modulation
property (multiplication property).

$q(n) = x(n)y(n) \iff Q(\omega)$. Not an ordinary convolution.

$Q(\omega) = \sum x(n)y(n)e^{-i\omega n}$

In particular, for $x(n)$, use synthesis equation to replace $x(n)$ with
$X(\xi)$ (where $\xi$ is a dummy variable).

$$
Q(\omega) = \frac{1}{2\pi} \sum_n\int_\avg{2\pi}
X(\xi)e^{i\xi n}y(n)d\xi e^{-i\omega n}.
= \frac{1}{2\pi} \int_\avg{2\pi}d\xi X(\xi)\sum_n e^{-i(\omega-\xi)n}y(n)d\xi
= \frac{1}{2\pi} \int_\avg{2\pi}d\xi X(\xi)Y(\omega-\lambda)
$$

What's special about this? It's only over a value of $2\pi$. This is a very
special kind of convolution. We call this a circular convolution of $x$ and
$y$. So $x(n)y(n) \iff \frac{1}{2\pi} (X*Y)(\omega)$.

Think of two functions plotted in terms of $\lambda$. One function, let's
say, is a square pulse wave thingy.

easy way of carrying out circular convolution: take one of the two
functions, keep one replica (e.g. the one from $-\pi$ to $\pi$), and then
do a regular convolution with the other function. advice: choose the
flipped + shifted signal for elimination of replicas.

EE 120: Signals and Systems
===========================
February 21, 2012.
-----------------

Circular convolution wrap-up
============================

$h(n) = \frac{B}{\pi n} \sin(An) \fourier H(\omega) = u(n+A)-u(n-A)$.
(ideal low-pass filter and its impulse response)

What if I had a filter whose frequency response G(\omega) is basically the
signal (circularly) convolved with itself (with some extra factors)?
description: ramp going from 0 to $4AB^2$ from $-2A$ to $0$, then back down
to $0$.

* Recall: To perform circular convolution:
  + Keep one function fixed (in terms of a dummy frequency variable)
  + Keep only one period of the other signal.
  + Perform regular convolution.

* Example: $(Q * R)(\omega)$.
+ Plot on a dummy-variable axis $Q(\lambda)$ with its replicas.
  + Plot R only in one period (zero out all replicas).
  + Flip \& slide $R$. $R(\lambda-\omega)$

What's the brute-force way to find the impulse response? Synthesis
equation.

Inspection method? It looks like it's $2\pi h^2(n) = \frac{2B^2}{\pi
n^2}\sin^2(An)$ (circular convolution must yield the $2\pi$ scalar)

* Recall: $q(n)r(n) \fourier 2\pi (Q*R)(\omega)$.

* _Note: this filter is BIBO stable, and its Fourier transform is
  continuous._

Now, we can spend a lot of time on the various properties of the DTFT (what
happens when you time-reverse a signal, shift, multiplication properties,
modulation, etc.), and this'll probably show up in a problem set.

As the last example on the DTFT: $f(n) = e^{i\omega_0n}h(n)$. Let one of
these be a sinusoid. What do you do?

It's effectively shifted by $\omega_0$, after the math clears.

Intermission
============
Tentative midterm date: March 13.

Continuous-Time Fourier Transform
=================================

$X(\omega)$: roughly the frequency response in continuous-time.

$$X(\omega) = \infint x(t)e^{-i\omega t}dt
\\ x(t) = (1/2\pi) \int X(\omega)e^{i\omega t}d\omega$$$

Slight shorthand: $X = \infint x(\tau)\phi{\tau} d\tau (\phi{t}(\omega)
\defequals e^{-i\omega \tau})$

To determine $x(t)$: (assume for now that $\braket{\phi_t}{\phi_\tau} =
\delta{\tau t})$

$\braket{X}{\phi_t} \defequals \infint X(\omega) \phi_t^*(\omega) d\omega$.

$\braket{X}{\phi_t} = \int X\braket{\phi_\tau}{\phi_\tau}d\tau$

_NB: Inner products are in frequency domain._

* Recall:
  + In the discrete-time case, we found that for $\braket{\phi_m}{\phi_n}$,
	we got $2\pi\delta_{mn}$. Turns out, we have exactly the same
	relationship in the continuous-time case.
  + In the continuous-time case, we have $2\pi\delta(m-n)$.

* Recall Poisson's identity: $\sum \delta(t-\ell p) = \frac{1}{p}\sum
  \exp(ik\omega_0t)$.

Now we are desperate to have $\infint e^{i\omega t}d\omega =
2\pi\delta(t)$. Which means we must have $\delta(t) \equiv \frac{1}{2\pi}
\int e^{i\omega t}d\omega$. New definition for $\delta(x)$.

Mutual orthogonality of the CTFT takes on this nasty form.

Plausibility argument:
----------------------

The right-hand side is proportional to $\int e^{i\omega t}d\omega = \int
\cos(\omega t)d\omega + i\int \sin(\omega t)d\omega$. Imaginary part is
guaranteed to be 0. By the same argument, the cosine portion is 0 if $t
\neq 0$. Otherwise, we're integrating 1 over all reals, so we have
$\infty$.

Constructively interfere at $t = 0$, but destructively interfere at $t \neq
0$.

Therefore the right-hand side must be proportional to $\delta(t)$.

The claim is that the constant of proportionality is $2\pi$. The hard part is
to recognize this is actually a Dirac delta.

Intermission
============
Steam coming out of our heads, evidently. Remember, this is the only
barrier to studying the DTFT.

One way to show $\delta(t) \equiv 1/2\pi \int e^{i\omega t}d\omega$
==============================================================

This method you will see, hear, etc. in engineering contexts for figuring
out something that doesn't converge, since Riemann integrals don't really
work here.

$\delta_\Delta(t) = 1/2\pi \int e^{i\omega t}e^{-\Delta\abs{\omega}}$. This
product is supposed to tame the function such that it is now integrable.

(take $\Delta > 0$)

Multiply what you've got with something that makes the product
converge. Then take the limit as said function goes to 1?

In other words, I am perturbing the problem slightly to make it stable and
taking the limit as stability goes to 0.

NB: half-width, half-maximum: where we're at half the width of our function
and also half the maximum value.

Perturbation theory!

$\alpha \defs $ perturbation parameter. Chosen such that area is 1.

Result of the integral: $\delta_\Delta(t) = \Delta\alpha/\pi 1/(\Delta^2 +
t^2)$. Turns out $\alpha=1$.

This is a Cauchy probability density function.

(names for dirac delta: generalized function, distribution. Look up theory
 of distributions)

EE 120: Signals and Systems
===========================
February 23, 2012.
-----------------
Continuous-time Fourier transform
---------------------------------

Recall:
$$X(\omega) = \int_{-\infty}^\infty x(t)e^{-i\omega t}dt
\\ x(t) = \frac{1}{2\pi}\int_{-\infty}^\infty x(t)e^{i\omega t}dt$$

We developed the inverse Fourier transform using a bizarre identity for
which we established a way to derive (i.e. perturbation theory).

Example: Ideal low-pass filter. In the continuous-time story, you do not
assume any periodicity. Figure out the impulse response of this filter.

$$\frac{1}{2\pi}\int_{-A}^A Be^{i\omega t} = \frac{B}{\pi t}\sin(At)$$ (valid
for $t = 0$ as well)

Okay. Now, how would you plot this? maximum of $\frac{AB}{\pi}$, zeroes at
$\frac{k\pi}{A} \forall k \in \mathbb{Z}$.

$$H(\omega) = \int h(t) e^{-i\omega t}dt\bigg|_{\omega=0} = \int h(t)dt =
H(0) = B$$.

Largest triangle has the same area as the integral over all space.

Another question. Let $B=1$. What is $\lim_{A\to\infty} h(t)$? Yet another
Dirac delta (yet another Dirac delta definition).

Take $g(t)$ as a block. What is $G(t)$? It is a sinc function.

If you dilate in the time domain, you squish the frequency domain, and vice
versa.
$$
\delta(t) \fourier 1
\\ 1 \fourier 2\pi\delta(\omega)
\\ e^{i\omega_0t} \fourier 2\pi\delta(\omega-\omega_0)
\\ \cos(\omega_0t) \fourier \pi(\delta(\omega-\omega_0) + \delta(\omega+\omega_0))
\\ \sin(\omega_0t) \fourier i\pi(\delta(\omega+\omega_0) - \delta(\omega-\omega_0))
\\ x(t) \fourier X(\omega)
\\ x(t-T) \fourier e^{-i\omega T}X(\omega)
\\ e^{i\omega_0t}x(t) \fourier X(\omega - \omega_0)
$$

Multiplication in Time
----------------------

$$h(t) = f(t)g(t) \fourier H(\omega)
\\ f(t) \fourier F(\omega)
\\ g(t) \fourier G(\omega)$$

Since transforms are not periodic, we have a regular convolution with an
extra $1/2\pi$ term. $H(\omega) = \frac{1}{2\pi}(F * G)(\omega)$.

Reasoning for convolution leading to multiplication in frequency domain:
cascade two systems, choose input to be complex exponential. This is an
eigenfunction, so we have the output response being $F(\omega)G(\omega)$. Now
choose input to be an impulse. Output is $(f*g)(t)$.

INTERMISSION
------------

Amplitude modulation
--------------------

I'm going to start what looks like a new phase of this course, but it's
hardly anything new and unpredictable. If you understand the CTFT and its
fundamental properties (what we've talked about so far), you should have no
problem understanding amplitude modulation.

One of the things you've experienced by now is that the atmosphere is quite
unforgiving to audible frequencies. Your voice only transmits over some
short distance.

Obvious solution is to shift spectrum to a much higher frequency range. The
way we do it is to multiply by either a complex exponential or a
sine/cosine (called a carrier signal). Carries signal of interest. So
$Y(\omega) = \frac{1}{2\pi}(X * C)(\omega) = X(\omega - \omega_0)$. What
happens is that $\omega_0$ is usually large enough to make transmission
through the atmosphere possible.

Ignoring all degradation going through the atmosphere, to retrieve the
original signal, multiply by the negative carrier frequency (complex
exponential). Frequencies about zero: baseband frequencies. Shifted
spectrum of received signal back to baseband.

aliasing; completely garbles signal.

Apply low-pass filter at end to capture baseband spectrum. Scale by two to
recover original signal, complete with amplitude.

Final observations: we first of all made an assumption that the received
signal is the same as the transmitted signal. Whole area of communication
theory that deals with deterioration. Other assumption is that the
oscillator at the receiver that can generate the exact same frequency as
the transmitter oscillator and at the same phase.

EE 120: Signals and Systems
===========================
February 28, 2012.
-----------------
AM Continued
============

(review of what we just did)

Recall:
-------
There are two major assumptions that we made. First of all, no
transmission degradation. Second of all, the receiver has the exact
same phase and frequency as the transmitter.

Question:
---------
What if $\hat{c}(t) = \cos(\omega_0t + \theta)$? (still keeping the
assumption that we can somehow match the frequency. New assumption: phase
is constant and not time-varying)

Thoughts:
---------

If the phase is off by $\pi/2$, then you lose your signal entirely. $r(t) =
\frac{1}{2}\cos(\theta)x(t) + \frac{1}{2} x(t)\cos(2\omega_0t +
\theta)$. If $\theta$ is relatively small (compared to $\pi/2$), then we
are safe, since we have our original signal. However, we lose our signal
as $\cos\theta\to0$.

Note:
MT2 date: Tues, 13 Mar 2012.

Also: when $\theta=0$, this is referred to synchronous demodulation
(transmitter and receiver in sync)

Instead of sending $y(t)$ into a low-pass filter, what we do is send it
through the diode parallel RC circuit. This is one way to do asynchronous
demodulation. This is technically cheating, since we assume that our signal
is entirely positive. However, we can simply apply a DC offset if we know
the bounds on our values.

Suppose $|x(t)| \le A \forall t$. Then, transmit $\hat{x}(t) \equiv x(t) +
A$.

Why is this method of transmitting $\hat{x}(t)$ called AM with large
carrier? We're actually also transmitting the carrier: $\hat{x}(t)
\cos(\omega_0 t) = x(t)\cos(\omega_0 t) =A\cos(\omega_0 t)$.

If $\abs{x(t)} \le K$, we want $K < A$. In fact, $\frac{K}{A}$ is referred
to as the percent modulation or modulation index.

One thing you should know is that there is redundancy in information double
side-band suppressed carrier.

Frequency Division Multiplexing
===============================

Each player is allocated a piece of real estate along the frequency axis.

Quadrature multiplexing
-----------------------

The way we can do this is by exploiting the orthogonality of cosine and
sine. What's being transmitted is the sum of the two.

EE 120: Signals and Systems
===========================
March 1, 2012.
-----------------
Sampling of CT Signals
======================
Now we have to differentiate between frequency in radians per second
($\omega$), and frequency in radians per sample ($\Omega$). Our sampling
interval we will represent with $T_s$, our sampling period.

There are associated with these boundary blocks ($C \to D$, $D \to C$)
periods. $T_r$ : reconstruction period. $T_r$ and $T_s$ may or may not be
the same.

The new part really is sampling theory.

So let's begin.

Let's say I have a continuous-time signal $x_c(t)$. We sample
periodically. $x_d(0)=x_c(0)$. $x_d(1)=x_c(T_s)$. Basically,
$x_d(n)=x_c(nT_s)$. A basic question is whether or not it is possible to
reconstruct the original signal? In general, the answer is no.

There must be (we hope so, at least) a set of conditions that guarantees
recoverability. These actually happen to be sufficient conditions. That is
the whole subject of the Sampling theorem, which we will develop.

So let me open up this first box, the process of sampling. We can model the
process of sampling this signal as one that involves multiplying the
original input signal by an impulse train, where the impulses are separated
$T_s$ second apart. Remember: multiplying a function that is locally
continuous with an impulse is equivalent to rescaling the impulse.

Once you extract $x_q$, there's a block we're not going to worry about that
converts Dirac deltas to Kr\"onecker deltas. Believe it or not, there is
nothing new here.

What happens to the spectrum of $x_c$ as it is multiplied by the impulse
train?

Our fundamental frequency of this signal is $\frac{2\pi}{T_s}$. It's also
called, in this context, our sampling frequency.

Considering the DTFT of periodic signals, we have a bunch of uniform
impulses separated by $\omega_s$, and each of them has strength
$\omega_2$. What happens when $T_s$ gets smaller? The sampling frequency
increases, and so we have stronger (and more separated) impulses as our
spectrum.

Our CTFT is simply $2\pi\sum_k Q_k\delta(\omega-k\omega_s)$. Multiplication
in the time domain is convolution in the frequency domain, so we can
consider the triangles.

So what happens? In order for these triangles to be recoverable, we need $2A
\le \omega_s$, or $A \le \frac{\omega_s}{2}$. $\omega_s$ must be large
enough that adjacent triangles do not overlap (and this is, indeed, the
crux of the sampling theorem) -- in this case, we can certainly recover our
original signal by applying a low-pass filter. 

The low-pass filtering is an interpolation operation -- we're interpolating
between adjacent values of the signal.

(explanation: You've got your samplings of the signal, so we've actually
got a set of impulses. We're applying a low-pass filter on this and filling
in the missing portions of the signal. Remember that our ideal low-pass
filter is the sinc, and so we're taking the linear combination of a
countably infinite set of sincs)

Whittaker-Nyquist-Kotelnikov-Shannon Sampling Theorem
=====================================================
(1915, 1928, 1933, 1949)

Most frequently known as the Shannon Sampling theorem. This is the set of
sufficient conditions for recovering a signal from a sequence of samples.

 * $x_c$ is band limited. Namely, $\abs{X_c(\omega)} = 0 (\abs{\omega} >
   A)$.
 * Sampling rate $\omega_s$ is large enough such that $2A \le \omega_s$

There is no universal definition of bandwidth. In some circles, the highest
frequency A is called the bandwidth. In other circles, the frequency's
footprint 2A is called the bandwidth. Does not really matter.

As mentioned, nothing mentioned today is new; we've just used very basic
properties of the CTFT of periodic signals and the convolution property.

Thoughts
--------
What if $\omega_s < 2A$? (by the way, $2A$ is called the Nyquist
rate). There has been a big push lately. Look up sub-Nyquist in a group of
papers. There are many people trying to figure out how to sample at a
lower-than-Nyquist rate.

So what happens if $\omega_s = 2A$? Our triangles are tangent. In the ideal
case, this is still acceptable, but it is impossible to build an ideal
low-pass filter, so it doesn't actually matter.

Oversampling: you exceed the Nyquist rate by a certain amount to
compensate. Past a certain amount of oversampling, it doesn't really mean
anything.

If $\omega_s < 2A$, our triangles overlap, and so the spectrum is
significantly different.

Higher frequencies get folded into lower frequencies. This is called
aliasing: the artifact of folding of higher frequencies into lower
frequencies. Once you have that, you have irrecoverably lost your original
signal.

In image processing, we have anti-aliasing (our next topic).

Notice that if the signal is band-limited, we can make $\omega_s$ large
enough such that we do not get overlap. If $x_c$ is **not** band-limited,
then no matter how high we make $\omega_s$, we're going to have some
overlap. There's one thing we can do in that case, but that's a
compromise. We call this:

Anti-aliasing
-------------
The idea is we discard frequencies above a certain value. With aliasing, the frequency
region is distorted, which is reflected everywhere in the time domain. This
is generally not good, unless it's really small and you can ignore it.

Anti-alias filtering is a preprocessing stage. $x_c(t) \to LPF \to
q(t)$. Namely, apply a low-pass filter (with cutoffs @ $-\frac{\omega_s}
{2}$, $\frac{\omega_s}{2}$) before sampling. We eliminate the aliasing we
know will happen (in the event that we don't do this).

The result is that you don't lose quite as high of frequencies. There is no
more aliasing.

So why anti-alias first instead of allowing it to alias, if you lose
information either way?

 - Anti-aliasing allows for the preservation of more of the original
   signal.

The set of signals that does not produce aliasing with a fixed $\omega_s$
are those that are band-limited by $\left(-\frac{\omega_s}{2},\frac{\omega_s}
{2}\right)$.

(talk about Parseval's theorem)

Carriage-wheel effect
---------------------
Aliasing is what causes the phenomenon that some of you may have noticed:
when on the highway, and you stare at the wheels of the car passing by you,
and they seem to be moving much slower in the opposite direction. Used to
show up at PhD qualifying exams at MIT. You may also find this described as
the carriage-wheel effect. Since the frame rate of old movies was 24 frames
per second, the wheels looked to be moving backward.

Mark a point on the unit circle. I strobe this wheel at a rate $\omega_s$
(strobing it is like sampling it). So I've got this strobe gun with a
sampling rate of $\omega_s$. $\omega_s$ happens to be $\frac{3}{2}
\omega_0$ (so not exceeding the Nyquist rate). If I sample exactly at
$\omega_0$, the point looks stationary. So the only way to capture the
motion properly is to capture at $2\omega_0$ and higher. But I'm not doing
that.

Its spectrum is a single Dirac impulse of strength $2\pi$ centered at
$\omega_0$. The sampling signal $Q(\omega)$ will have an impulse train
separated by $\omega_s$. When we convolve this and apply a low-pass filter,
we have just one remaining frequency at $\omega_0-\omega_s = -\frac
{\omega_0}{2}$.

EE 120: Signals and Systems
===========================
March 6, 2012.
--------------
Sampling Cont'd
===============
We are still in the first of three blocks (where we take a continuous-time
signal and create a discrete-time signal).

This end-to-end system is effectively a continuous-time system.

This kind of processing we refer to (for obvious reasons) as discrete-time
processing of continuous-time signals. The opposite is also possible, where
you start out with a discrete-time signal, process in continuous-time, and
spit out a discrete-time signal.

Last time, we opened up the first box ($C \to D$). We didn't even talk
about the entire box -- there's still some stuff to discuss.

So, consider an impulse train. We then take this through a Dirac $\to$
Kr\"onecker block to produce $x_d(n)$.

Question: How is $X_q(\omega)$ related to $X_d(\Omega)$?

etc, work with moving around between coords.

All of this assumes that there has been no aliasing. For us to have had no
aliasing, remember the Nyquist sampling theorem.

Considerations of LTI for $Y_c(\omega) \equiv X_c\parens{\frac{\omega
T_r}{T_s}}$.

The only way your end-to-end system will have an LTI-equivalent is if you
fulfill the conditions of the Nyquist sampling theorem (no aliasing), and
your reconstruction period is the same as your sampling period.

Then you know that your output is equal $\frac{G}{T} H_d\parens{ \omega T}
X_c(\omega)$. The equivalent LTI filter is simply $\frac{G}{T} nH_d \parens{
\omega T}$
