EE 120: Signals and Systems
===========================
March 1, 2012.
-----------------
Sampling of CT Signals
======================
Now we have to differentiate between frequency in radians per second
($\omega$), and frequency in radians per sample ($\Omega$). Our sampling
interval we will represent with $T_s$, our sampling period.

There are associated with these boundary blocks ($C \to D$, $D \to C$)
periods. $T_r$ : reconstruction period. $T_r$ and $T_s$ may or may not be
the same.

The new part really is sampling theory.

So let's begin.

Let's say I have a continuous-time signal $x_c(t)$. We sample
periodically. $x_d(0)=x_c(0)$. $x_d(1)=x_c(T_s)$. Basically,
$x_d(n)=x_c(nT_s)$. A basic question is whether or not it is possible to
reconstruct the original signal? In general, the answer is no.

There must be (we hope so, at least) a set of conditions that guarantees
recoverability. These actually happen to be sufficient conditions. That is
the whole subject of the Sampling theorem, which we will develop.

So let me open up this first box, the process of sampling. We can model the
process of sampling this signal as one that involves multiplying the
original input signal by an impulse train, where the impulses are separated
$T_s$ second apart. Remember: multiplying a function that is locally
continuous with an impulse is equivalent to rescaling the impulse.

Once you extract $x_q$, there's a block we're not going to worry about that
converts Dirac deltas to Kr\"onecker deltas. Believe it or not, there is
nothing new here.

What happens to the spectrum of $x_c$ as it is multiplied by the impulse
train?

Our fundamental frequency of this signal is $\frac{2\pi}{T_s}$. It's also
called, in this context, our sampling frequency.

Considering the DTFT of periodic signals, we have a bunch of uniform
impulses separated by $\omega_s$, and each of them has strength
$\omega_2$. What happens when $T_s$ gets smaller? The sampling frequency
increases, and so we have stronger (and more separated) impulses as our
spectrum.

Our CTFT is simply $2\pi\sum_k Q_k\delta(\omega-k\omega_s)$. Multiplication
in the time domain is convolution in the frequency domain, so we can
consider the triangles.

So what happens? In order for these triangles to be recoverable, we need $2A
\le \omega_s$, or $A \le \frac{\omega_s}{2}$. $\omega_s$ must be large
enough that adjacent triangles do not overlap (and this is, indeed, the
crux of the sampling theorem) -- in this case, we can certainly recover our
original signal by applying a low-pass filter. 

The low-pass filtering is an interpolation operation -- we're interpolating
between adjacent values of the signal.

(explanation: You've got your samplings of the signal, so we've actually
got a set of impulses. We're applying a low-pass filter on this and filling
in the missing portions of the signal. Remember that our ideal low-pass
filter is the sinc, and so we're taking the linear combination of a
countably infinite set of sincs)

Whittaker-Nyquist-Kotelnikov-Shannon Sampling Theorem
=====================================================
(1915, 1928, 1933, 1949)

Most frequently known as the Shannon Sampling theorem. This is the set of
sufficient conditions for recovering a signal from a sequence of samples.

 * $x_c$ is band limited. Namely, $\abs{X_c(\omega)} = 0 (\abs{\omega} >
   A)$.
 * Sampling rate $\omega_s$ is large enough such that $2A \le \omega_s$

There is no universal definition of bandwidth. In some circles, the highest
frequency A is called the bandwidth. In other circles, the frequency's
footprint 2A is called the bandwidth. Does not really matter.

As mentioned, nothing mentioned today is new; we've just used very basic
properties of the CTFT of periodic signals and the convolution property.

Thoughts
--------
What if $\omega_s < 2A$? (by the way, $2A$ is called the Nyquist
rate). There has been a big push lately. Look up sub-Nyquist in a group of
papers. There are many people trying to figure out how to sample at a
lower-than-Nyquist rate.

So what happens if $\omega_s = 2A$? Our triangles are tangent. In the ideal
case, this is still acceptable, but it is impossible to build an ideal
low-pass filter, so it doesn't actually matter.

Oversampling: you exceed the Nyquist rate by a certain amount to
compensate. Past a certain amount of oversampling, it doesn't really mean
anything.

If $\omega_s < 2A$, our triangles overlap, and so the spectrum is
significantly different.

Higher frequencies get folded into lower frequencies. This is called
aliasing: the artifact of folding of higher frequencies into lower
frequencies. Once you have that, you have irrecoverably lost your original
signal.

In image processing, we have anti-aliasing (our next topic).

Notice that if the signal is band-limited, we can make $\omega_s$ large
enough such that we do not get overlap. If $x_c$ is **not** band-limited,
then no matter how high we make $\omega_s$, we're going to have some
overlap. There's one thing we can do in that case, but that's a
compromise. We call this:

Anti-aliasing
-------------
The idea is we discard frequencies above a certain value. With aliasing, the frequency
region is distorted, which is reflected everywhere in the time domain. This
is generally not good, unless it's really small and you can ignore it.

Anti-alias filtering is a preprocessing stage. $x_c(t) \to LPF \to
q(t)$. Namely, apply a low-pass filter (with cutoffs @ $-\frac{\omega_s}
{2}$, $\frac{\omega_s}{2}$) before sampling. We eliminate the aliasing we
know will happen (in the event that we don't do this).

The result is that you don't lose quite as high of frequencies. There is no
more aliasing.

So why anti-alias first instead of allowing it to alias, if you lose
information either way?

 - Anti-aliasing allows for the preservation of more of the original
   signal.

The set of signals that does not produce aliasing with a fixed $\omega_s$
are those that are band-limited by $\left(-\frac{\omega_s}{2},\frac{\omega_s}
{2}\right)$.

(talk about Parseval's theorem)

Carriage-wheel effect
---------------------
Aliasing is what causes the phenomenon that some of you may have noticed:
when on the highway, and you stare at the wheels of the car passing by you,
and they seem to be moving much slower in the opposite direction. Used to
show up at PhD qualifying exams at MIT. You may also find this described as
the carriage-wheel effect. Since the frame rate of old movies was 24 frames
per second, the wheels looked to be moving backward.

Mark a point on the unit circle. I strobe this wheel at a rate $\omega_s$
(strobing it is like sampling it). So I've got this strobe gun with a
sampling rate of $\omega_s$. $\omega_s$ happens to be $\frac{3}{2}
\omega_0$ (so not exceeding the Nyquist rate). If I sample exactly at
$\omega_0$, the point looks stationary. So the only way to capture the
motion properly is to capture at $2\omega_0$ and higher. But I'm not doing
that.

Its spectrum is a single Dirac impulse of strength $2\pi$ centered at
$\omega_0$. The sampling signal $Q(\omega)$ will have an impulse train
separated by $\omega_s$. When we convolve this and apply a low-pass filter,
we have just one remaining frequency at $\omega_0-\omega_s = -\frac
{\omega_0}{2}$.

