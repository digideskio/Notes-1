EE 120: Signals and Systems
===========================
February 7, 2012.
-----------------
CTFS:

periodic if $x(t+p) = x(t) \exists p \in R$. Smallest positive $p$ is
called the fundamental period. Fundamental frequency $\omega_0 \equiv
\frac{2\pi}{p}$.

Can also write $p = 2\pi/\omega_0$. In discrete-time, writing it this way
was dangerous, since depending on $\omega_0$, $p$ may or may not be an integer.

For discrete-time signals, the constant signal was periodic with $p=1$.
$x(t) = 1 \forall t \in Z$?

What about the signal $x(t) = 1 \forall t \in R$? The fundamental period is
undefined: any $p>0$ can serve as a period.

So there are subtleties in each story. In the discrete-time story, there
were some sinusoids that looked periodic but weren't, and the constant
signal has no fundamental period in continuous-time.

We're going to jump immediately into the Fourier series. He said you can
decompose any continuous signal as a linear combination of complex
exponentials that are related to each other by virtue of being at
frequencies that are integer multiples of the fundamental period.

$x(t) = \sum X_k e^{ik\omega_0t} = \sum X_k\psi_k$.

We know the procedure for finding the kth coefficient. Before we go there,
there's something you ought to pay attention to in this expression. When I
draw a typical periodic signal, when I look at one period, how many points
do I have? Uncountably infinite. Also range is potentially a set of
uncountably many values. So this is a bold claim: we can represent these
with a countable number of eigenfunctions.

Unlike the discrete-time story, this equality will not always be a
pointwise equality. There are different gradations of convergence. Whenever
you have an infinite sum, you have to worry about convergence in the back
of your head. For well-behaved signals, the left and the right converge,
and this is true for every t. The less well-behaved signals will no longer
hold pointwise. Strange things happen, e.g. Gibbs phenomenon.

You'll have a reasonable understanding of Fourier series. We're not going
to worry too much about convergence in this class. The only time it doesn't
arise is in the discrete-time Fourier series.

Claim: Fourier analysis works. One path we can take is for you to take my
word for it. Or we could prove it. Since last time was hilarious, we're
going to take this for granted, for now. Assume orthogonality of $\psi_k$
for some definition of the inner product. $\psi_k = \exp(ik\omega_0 t)$.

I am now going to determine $X_l$. Take the inner product of $x$ with
$\psi_l$.

The procedure is exactly the same. We're just swapping out our definition
of inner product. Exploit the orthogonality.

For discrete-time p-periodic signals, we defined the inner product as
$\braket{f}{g} = \sum fg^*$. Guess what the continuous-time inner product
is for $p$-periodic signal!

And if they're non-periodic, we'll do the same, but over all time.

Show that our eigenfunctions are orthogonal.

Synthesis equation: $x(t) = \sum X_k \exp(ik\omega_0 t)$
Analysis equation:  $X_k = (1/p)\int x_k\exp(-ik\omega_0 t)$.

How do I show that $\braket{\psi_k}{\psi_l} = 0 (k\neq l)$? Just evaluate
the integral. We get an exponential with period p, integrated over a period
p? Looks like 0 to me.

Example: $X(t) = cos(\pi t/3)$. $(\exp(i\pi t/3) + \exp(-i\pi
t/3))/2$. $\psi_1 = \psi_{-1} = \frac{1}{2}$.

$q(t) = \sum\delta(t-\ell p) = \sum Q_k \exp(ik\omega_0t)$
$\delta(t) = \deriv{u(t)}{t}$

Poisson's identity. $\sum\delta(t-\ell p) = \frac{1}{p}\sum\exp(ik\omega_0t)$

$R_k = \frac{1}{p}\int r(k)\exp(-ik\omega_0t) = \frac{1}{p}$ if $k=0$ else
$\frac{\sin(k\omega_0\Delta/2)}{pk\pi\Delta/2}$

What happens if I want to approximate a signal that has finite energy? What
should the coefficients $\alpha_k$ be?

orthogonal projection! Least squares!

