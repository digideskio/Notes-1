DRAM, Asynchronous; DRAM, Synchronous
=====================================
November 8, 2012
----------------

Chipscope feature: "trigger on data". Super-valuable; if you do it this
way, you don't spend that much time on synthesis.

Last time, we talked about the history of what DRAM used to look like, up
through multi-megabit DRAMs. You had address, data, row-address strobe,
column-address strobe, write-enable, and chip-select. You'd have your
row-address with RAS going to that (all active low signals),
column-address, etc.

$t_{rcd}$: row to column delay. Used to be RAS to CAS, but that's the same
thing.

$t_{CL}$: column / CAS latency.

$t_{rp}$: row precharge.

Total delay from rising edge to $t_{rc}$, which is either the row cycle or
the (random) read cycle (depending on who you talk to). Row cycle is a
better term.

How about writing? If you think of your row register and sense amps up
here, what does a slice through this thing look like? I've got my bit line
coming down, and it goes into a sense amp, which then goes into a MUX -- I
want to get this into my register -- but I also have an input line coming
up that I'd like to be able to write to; I need some strobe on the
register, and I need to be able to write back into the array, and I need to
be able to write out to the outputs.

In these days, these lines get called DQ lines for obvious reasons.

The standard refresh interval is 64 ms; for automotive temperature grade
(higher), there's more leakage, so this has to be lower (typically 32 ms).

In the good old days, that used to be outside the chip; now that's built
into the chip.

So I've got this row register, now, and it seems like a waste to only have
one output from this thing. It turns out that $t_{CL}$ is often comparable
to $t_{rcd}$. The time to decode (select) one of these lines is pretty
large. The time it took a signal to travel has to do with how long your
lines are and how much capacitance it has. In order to minimize that,
you've got the whole row there; why not clump it in larger groups? I'll
have a group of $k$ bits that come down to a MUX, and I'll get $k$ bits out
of it, and pipeline that process.

My column address therefore gets broken up into two pieces.

If $t_{cl}, t_{rcd}, t_{rp}, t_{rc} \gg T_{clk}$, share pins among 2,4,8
identical banks. All of these have the same address lines, and they have
separate RAS and CAS lines. Become more efficient with pins, but that makes
things messier.

Getting messy, so throw more logic at the chip.

Synchronous DRAM (SDRAM) is pretty much what everyone uses these days. Yes,
you're just adding a clock, but you're changing the entire interface. You
still have CS, WE, RAS, CAS, but now these plus some bank address BA0/1 all
become control inputs to a FSM. There's a (mostly) separate FSM for each
chip.

As an example, RAS=1,CAS=0,WE=0 yields "activate", 010 is "read", 011 is
"write", 101 is "precharge", which also writes back. Tried best to make it
look like old days.

So. The state machine, you start in IDLE; obviously there's some bootup
sequence. You go to the active state when you're given that active command
input, and that takes you $t_{rcd}$, which for our chip is 15ns. From there
you go to a write state, a read state, a write and precharge state,
read and precharge state, or precharge with writeback state.

200MHz bus clock (5ns), most things take 3 clock cycles. The minimum time
if you want to go around the loop and back to idle again, is 11. 256 MB,
which is 32M $\times$ 64b = 32M $\times$ 8B. 32M $\times$ 16b per chip,
8M $\times$ 16b per bank.

How does a bank work? We have 64b-wide columns. That comes down into our
row register, and then coming off of that, we come down to another register
that goes into a multiplexer to give us our 16-bit output. The column
address (which is sitting in the column address register) gets split up.

Double-data rate: pump on rising as well as falling edge. Every 2.5ns (with
200MHz clock), you get an edge.

Reads can happen in parallel (since we have 4 banks). DDR2 does burst
reads. You're going to read 16 bits out of a given chip on every edge for
four edges (and there are four chips). So 256 bits per burst read / write.

They all showed up in the space of two clock cycles. 32B in 10ns is 3.2
GB/s. Latency is $6 T_{clk} = 30$ ns; at the end of this, I get a precharge
command, when I can finally given another activate command and a new row
command. So if I'm just doing truly random transfers (writes then reads), I
get one transfer per 55 ns, which is something like 18 Mtransfers/sec for
completely random access. 

talk about graphics processor, should be able to do ff and pf much faster
than software because we can read/write more than 32 bits at a given time.

vertical blanking / sync; horizontal blanking / sync. If your pixel feeder
gives you a DONE interrupt, the ISR should change the pixel feeder source
address should change to reflect the current buffer. Various other things
should change here.
