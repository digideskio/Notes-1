Midterm Review, Cache
=====================
October 16, 2012
----------------
Midterm Thursday, OBONNS. Review session tonight. Project checkpoint 1 is
behind us (or most of us). Checkpoint 2 and 3, we've rearranged things a
little bit to allow TAs to get the interrupt one done in time. Last time I
said exceptions and interrupts would be next, but that'll be CP3; the next
will be integrating cache.

MT review, Cache. Material will be based on homework and labs/project. Not
going to throw anything at you that you haven't seen before, no need to
invent anything new, not going to make it tricky. Basically, set of things
I will choose from is K-maps, MIPS datapath/pipeline/assembly, ready/valid
interface, UART, Verilog, RAM + ALU datapath, FF and CLB internals.

All this is stuff that's either been on a homework assignment or is part of
your lab. So not on exam: exceptions, cache.

K-maps, broadly: what I want you to do is given a description either as
minterms or maxterms or f(A,B,C,D) or truth table, or maybe even gates, I
want you to be able to go to drawing the K-map (four variables max), find
the minimal sum of products or product of sums, and then implement with
NOR/NAND/whatever. Nowhere up there does it say accurate definitions of
terms. It's open-book and open-note.

MIPS, for sure single-cycle datapath: add some new piece of hardware or add
an instruction; how would you change it if you had to add something to it?
(add a new register, floating point unit). Pipeline is obviously what we've
covered most; sequencing of instructions (how they work their way through
the pipeline as other instructions go through, and that's predominantly for
3-stage pipeline (don't think I'll give you a five-stage pipeline, but I
might, since that's been on your homework); hazards and
forwarding. Calculating minimum clock period given some particular
arrangement of pipeline registers and propagation delay of various
elements; ff stuff, logic stuff, topology of the implementation, etc. What
else can I ask about pipelines? Probably it.

Ready/valid: draw traces (suppose I give you a clock and what the ready
line does; what is the valid line and what shows up on the datapath?). For
you, that's related to the UART and certainly understanding the UART
interface there. Revamp to lab next time around; still not ideal. At this
point, now that you've done your checkpoint 1, you ought to know what's
happening cold. What happens at the bit-level? Bits going over wire; how
does it work?

Verilog for simple FSM. "Design a FSM that outputs a 1 when XYZ".  Your job
is to take that and turn it into state encoding, state register; next-state
logic; output logic. Do it all cleanly and clearly so we can say that you
know how Verilog works.

RAM + ALU + registers question: "find the min/max/XYZ of values in
SRAM". Draw datapath, draw state transition diagram. A big part is figuring
out these are separate and make sure you aren't putting control into your
datapath and being clever by throwing gates over the control.

FF internals: down to FETs. You should know how to if I tell you to give me
a D-flipflop with synchronous load and asynchronous reset, you ought to be
able to draw the gates that do that, and how the gates are implemented with
FETs. Add some functionality to the flipflop (as on the homework). Trace
output.

Similarly, CLB internals: probably not going to ask you to take that down
to transistors. Will be provided with any datasheet info necessary. Related
to this, I may have something on the registered PAL or something like that.

Caches
------

Why is it we have registers in the MIPS CPU? Immediate access. Faster,
lower energy per operation. Multiport (we can have several read inputs at
the same time as a write input) -- that's a bit win. Shorter addresses, so
we can actually encode these with single instructions.

First two: sort of a first version of cache. So if we look at the hierarchy
from registers to cache (which is SRAM) to what I'll call main memory
(DRAM), and then I guess all the way down here, we've got disk (and disk
cache), it's all about binding. At the end of the day, an add refers to
variables, which had to be bound to something. Whether that's a memory
address or a register address, there's some kind of binding.

The binding coordination here to registers happens either by the compiler
(or whoever's writing the assembly coder, in general). The cache is
managed by the cache controller. Main memory is handled (mostly) by the
operating system.

In terms of size of transfers, registers are 1B to 8B, depending on the
machine you have (for our MIPS, that transfer is 4B). Cache is on the order
of 8B to 128B (we'll be doing 16B chunks), and then transfers between main
memory and disk are block sizes from 512B to 4KB (and we don't got any).

And speed: registers are on the order of 0.1 - 1ns today (you can get
regfiles that operate faster than 10GHz, but the problem is they dissipate
too much power per unit area, so we've stopped turning up the clock and
figured out how to take advantage of the scaling). Cache is somewhat
slower; on the order of 1ns, and main memory is around 10-100ns, depending
on what's going on.

There are lots of analyses you can find and lots of figures that relate the
cache size to the cache miss rate, and let me draw your project
architecture before we talk more about that: you've done your MIPS core at
this point, which has an instruction and data memory interface. The next
step is we're going to put in our DDR2 SDRAM (256MB), and then a memory
controller called a memory arbiter. There's actually another block in here
called the Xilinx memory interface generator (XMIG) that lives between
the arbiter and the SDRAM. At the end of the day, that interface just has
one address going in and data going back and forth. And each one of these
is going to be talking to that memory arbiter, and our block RAMs become the
instruction cache and the data cache.

When all things go well, this looks no different. But if the address you're
looking for isn't in your instruction cache (both of which are going to be
8KB). Those are cache misses. The miss rate on a log scale (for 1 kB to 1
MB) looks different between the cache type, the code being executed, the
archiecture of your processor, and just about everything else you can think
of. Generally fairly linear on a log-log plot for some point, and then it
drops drastically. Register access on the order of a hundred times faster
than a memory access.

Big caches: take more chip area, and in an era where you can put literally
billions of transistors on a chip, that's not the problem: big caches are
slower and more energy-costly.

One of the things you'll be doing is talk to a video chip, which will
generate the video output that drives your monitor, and it'll use
memory-mapped IO, so it'll read out a megapixel every thirtieth of a
second, which you'll have to write out to the arbiter. Haven't yet talked
about why it's slower and costs more power; we'll get to that at the end.

If you look at the Intel i7 architecture, it's 1-8 cores on a chip, and
each core has an instruction cache and data cache (L1); those are 32KB
each. We then have an L2 (unified instruction and data) cache, and then
finally an L3 cache that's shared by all processors (6MB), which then goes
off to DRAM.

L1 cache is about 0.5 ns, L2 is about 2.5 ns, and L3 is 10 ns. Off-chip is
on the order of 40-100 ns, depending on what you put down there.

Three layers of cache is not the end of the story. It turns out that in
fact, we can keep playing this game that very often, your PC goes into your
L1 instruction cache, and you get an instruction word out of this thing
that might be 128 bits wide (in some sense, L0), and then you've got 2 bits
of control that decides which word you're going to grab, which is also
controlled by the PC, broadly.

On the other end, you've got your register file and your ALU, and this
might be like ours, on the MIPS, 32 by 32; Bill Dally wrote a paper that
showed that in 28nm CMOS, the energy cost of doing the computation for a
fused multiply accumulate operation (where you read out three 32-bit
operands and write back AB + C), that the register reads/write is roughly
two times the energy of the FMAC (floating point multiply accumulate). So
he put in 4 32-bit registers inside the ALU so that he didn't have to do
that, and that cut the overall power consumption by a factor of two
(basically, he added a register cache, analogous to L-1).

So how about von Neumann (Princeton) and Harvard?

Harvard Architecture
--------------------
You have your program counter address going to some instruction memory, and
you get some instruction out, and you have some kind of registers and ALU
giving you your address and exchanging data with some data memory.

Princeton Architecture
----------------------
You have your PC, RF, ALU, and peripherals, all of whom talk to an address
bus and a data bus. Shared memory, any of them can use it.
