Graphics Processors
===================
November 20, 2012
-----------------
Guest lecture by John Lazzaro.

What we're going to be talking about here is how the GPUs in your computer
and iPad work. Giving you the 80-minute version. Help you figure out
whether you want to go and work for NVidia.

Can't talk about this if you don't know anything about graphics. We'll
start with the basics of how we made them until about 2006, then move into
state of the art and what came out this year.

Most of this block diagram lives on one chip. On this one integrated die,
for the desktop, we have 4 cores, a GPU that's fast enough to handle your
day-to-day use cases of simple games. What used to be on a separate chip,
the north bridge, is up here (what talks to DRAM, PCI). In addition to the
GPU, it has a lot of logic to drive the DVI port (aside from the high
voltage parts).

The GPU that comes with the chip doesn't have its own dedicated graphics
memory; it shares memory with the CPU.

There are people for whom this won't be enough; that's what the PCIe bus is
for. Discrete GPU. They put the very best GPU you could make as opposed to
this simpler one.

Why do we make this specialized hardware at all? You can answer this for
yourself on the back of the envelope. Double buffering to prevent
artifacts. Simplest thing you could build. How much work can you do on each
pixel? On the order of tens of instructions.

What are we accelerating? Now, 3-D games. Back in the day, 2-D acceleration
-- fast windowing systems, games like pacman.

Why should we use a special processor for graphics? Programmers generally
use a certain style.

Triangle: simplest closed shape that may be defined by straight edges. With
enough triangles, you can make anything.

3-D model: includes faces we can't see in the current view. Arbitrary
smoothing based on granularity of triangles. Canonical example: teapot;
wireframe outlines the triangles.

That's idea #1: whatever our processor is going to do, it's going to do on
triangles.

Affine transformations to scale, rotate, translate, etc. Can apply these to
the ensemble of triangles to apply the transformation to the (aggregate)
object as a whole.

Wireframes are boring; vertex shading -- smooth gradient, we want to use
interpolation here. Real graphics actually takes this a step further and
considers light sources; what you specify on the edges is how these things
absorb light as functions of angle of incidence, intensity, etc; how the
light hits the vertices.

We see a 2-D window into the 3-D world (orthogonal projection; 4-D?). This
story will only really be true for the next decade or so. You have 3-D TVs
that don't need the glasses. These days, they can do something like 50-60
views. Once that happens, we won't be seeing a 2-D window so much as an
actual 3-D world. That isn't here yet; this talk is about the hardware we
need today.

To go from this three-dimensional world to this two-dimensional space, we
must project each triangle that might face the eye onto the image
plane. Then, create fragments on the boundary of the image plane triangle
(rasterization).

Went from three-dimensional world to two-dimensional world on the
screen. Called pixel fragments because a screen pixel color might depend on
many triangles (e.g. a glass teapot). Need a system general enough to
process many triangles.

Need to figure out color -- shading. There's thousands of SIGGRAPH papers
saying how to do this, but there are two basic approaches: (a) physics of
what copper looks like when you shine light on it (purely simulated physics
-- you know the material and its properties, lots of math, but you get nice
pictures when it all works -- only recently has this become feasible), or
(b) you hire an artist to paint a surface of the teapot; we map this
texture onto each fragment during shading.

The final step is reassembly / output merge.

Lugo Jr: first Pixar movie.

Graphics acceleration (back to hardware for the rest of the talk --
couldn't talk about hardware until we understood the motivation).

Algorithms were generally hardwired, programmable CPUs for vertex and pixel
shaders. Specialized programming languages for specialized CPUs.

First difference is a set of input registers (read-only). Outside world
puts this in. Only one vertex at a time is placed in; you do that; the
program that gets put in is run once. Short code (e.g. 128 instructions),
the same code runs on every vertex. Then the result gets put in the output
registers, which are write-only. This goes out to the rasterization
hardware. And there's some constant registers the general-purpose CPU could
change; also read-only for the shader CPU. Very specialized thing;
optimized in many ways. Working on 4 floating point numbers.

What really makes this whole thing work is that it's trivial to
parallelize: processing of each vertex is independent. The only thing you
have to worry about is ordering: the order of incoming vertices should
be the same as outgoing vertices.

Shader CPUs may be multithreading. Completely hide memory latency this
way.

Pixel shader works similarly: takes one fragment at a time; outputs one
fragment at a time. Memory system that handles mathematics; built into
memory system.

Basic idea: replace specialized logic with many copies of one unified CPU
design. Consequence: you no longer see the graphics pipeline when you look
at the architecture block diagram. Only way to make graphics work well with
DirectX 10.

New pipeline features: geometry shader lets a shader program create new
triangles: one vertex in, many vertices out. Things you no longer need the
CPU to do.

The other thing you could do was use recurring algorithms that lived
entirely on the chip. Most of the specialized instructions have more or
less gone. Shader CPUs are more like RISC machines.

More complicated things possible without CPU.
