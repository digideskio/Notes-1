Computing the Matrix Exponential; Dyadic Expansion
==================================================
October 2, 2012
---------------

Today: example computing $x(t)$ using $e^{At}$. Sastry guest lecturing on
Thursday; LN11 is linear quadratic optimization.

Date for midterm: in class, Tuesday Oct. 16 (two weeks from today). It's
going to cover material up to and including what we cover in homework
assignments (e.g. won't have anything on linear quadratic optimization;
will probably be everything up to what we finish this week: up to lecture
notes 13, not including lecture notes 11). Sample midterms, which will be
posted; next Friday, Oct. 12, will be midterm review in section. On Monday,
Oct. 15, Insoon will have extended office hours. One more homework: #4,
posted tonight, due before midterm (so likely next Thursday).

In general, you'll compute the matrix exponential via the inverse Laplace
transform of $(SI - A)^{-1}$. Similarity transformation: $TJT^{-1}$. We
don't in general talk about $e^{At}$, but rather functions of a matrix $A$
which can be represented in its Jordan form.

Continuation of example. We recall that we're working in the realm of LTI
systems described by the matrices $\dot{x} = Ax + Bu, \dot{y} = Cx +
D$. Remember the solution given by the convolution (so to speak) of the
input with the matrix exponential.

We set up this model with the inverted pendulum, and we considered the
open-loop representation. We proposed a controller, $k(s)$, and then we
wrote out the system dynamics. We then defined a new state variable to
represent the dynamics of the controller, so we had three first-order
differential equations where the right-hand side is just a function of the
inputs.

We then began to consider a closed-loop representation of the system. If we
write the system in terms of the transfer functions, what we're doing is
measuring the output and feeding it back through negative feedback to the
input. If we look at the new state update equation and the output equation,
we recognize that even though the matrices is different, they still have
the same dimensions.

Computed a few things in MATLAB: CL step response. What we see is that it
has a step response indicative of a nice, stable system, but that's the
closed loop zero-state response. If we consider the zero-input response, we
find that the solution diverges.

So what's going on here? We can go back and look at the system. At some
times, we can compute $x_{CL}(t)$.

Once we compute the matrix's inverse, we consider the partial fraction
decomposition of the elements of the inverse. In general, you would have to
do that for all nine terms (but in this case, there are repeated terms, so
you'd have to do this just three times). You can perform this partial
fraction expansion, and then you have to remember what the inverse Laplace
transform is.

In our problem, we're asking for less: we only need to compute the first
column (which is actually still everything).

One of the things you're tempted to do as an undergraduate in controls is
attempt to cancel out your poles, but it is impossible to get perfect
pole-zero cancellation.

Back to the main point: we're in general faced with computing the matrix
exponential, and taking the inverse Laplace transform is hard. The idea is
to try to represent A in a canonical form to simply compute what that
Laplace transform is, if possible.

General: Functions of a matrix $A \in \mathbb{C}^{n\times n}$
-------------------------------------------------------------
(Goal: find simpler ways to compute $e^{At}$, but in general, functions of
a matrix)

We started with LTI systems; we know the solution in terms of the state
transition matrix is given by $e^{At}x_0$. More for terminology more than
anything, we're going to write the inverse of a matrix $sI - A$ in the
following form: the ratio of something called the adjugate of $SI - A$ over
the determinant, i.e. $\frac{\mathrm{adj} (sI - A)}{\det (sI - A)}$. I'm
going to define the **characteristic polynomial** of $A$ as
$\hat{\chi}_A(s) \defequals \det (SI - A)$ (so it will look like $s^n +
\alpha_1 s^{n-1} + ... + \alpha_n$. The **adjugate** is given by a
polynomial $B_0 s^{n-1} = B_1 s^{n-2} + ... + B_{n-2} s + B_{n-1}$, where
$B_i \in \mathbb{C}^{n \times n}$. We define this matrix as follows:
$\mathrm{adj} (sI - A) = C^T$, where $C_{ij} = )-1)^{i+j} M_{ij}$ (where
$M_{ij}$ is the determinant of the $n-1\times n-1$ matrix where row $i$ and
column $j$ are eliminated).

What we really want to do is define a notation here for understanding the
Cayley-Hamilton theorem. If we had to, we could compute it, but let's go
back and use the notation.

The theorem we're going to state here is the **Cayley-Hamilton theorem**,
which states that every $n \times n$ matrix satisfies its own
characteristic polynomial. What does that mean? If you set the
characteristic polynomial of the matrix to 0, and everywhere you see $s$
you plug in the matrix, that will sum up to 0.

Given the setup here, we can easily prove this. With the definition of an
inverse, let's multiply both sides (of our definition of the inverse) by
$sI - A$ and the characteristic polynomial. This will yield the equivalent
expression that $\mathrm{adj}(sI - A)(sI - A) = \hat{\chi}_A(s) I$. Just
matching coefficients, we can write out the $B_i$. From that, we can write
out the polynomial and simply work through the math. All we used in the
proof of that is just the general form of the matrix in terms of $sI - A$.

Really important result, since we can use this to say general things. This
tells us something immediately about polynomials of matrices. If you were
looking at some polynomial and wanted to simplify that, we can relate
higher powers of the matrix A which are less than or equal to $n$. If you
had general k-degree polynomials in $s$, $\hat{p}_1, \hat{p}_2$, then if we
divide $\hat{p}_1$ by $\hat{\chi}_A$, then we can write these as $\hat{q}_1
\chi_A + \hat{r}_1$ and $\hat{q}_2 \chi_A + \hat{r}_2$. Even if the two are
not equal, if the remainders are the same (i.e. $\hat{r}_1 = \hat{r}_2$),
if you evaluate the two on the matrix A, $\hat{p}_1(A) = \hat{p}_2
(A)$. Just a result of the Cayley-Hamilton theorem. That tells us that
every polynomial function of $A$ can be written as a function of $I, A,
A^2, ..., A^{n-1}$.

In general, interesting functions are not necessarily polynomials, and we
have an infinite series representation. How do we compute functions of a
matrix A? In order to do that, we're going to have to think about two
cases: we're going to look at the matrix A and compute its eigenvalues and
eigenvectors. We've mentioned that before; we'll have a quick review of
computing eigenvalues and eigenvectors. We'll break this up into two cases:
we have $n$ linearly independent eigenvectors, and we have less than $n$
linearly independent eigenvectors. In the first case, $A$ is
diagonalizable, meaning there exists a similarity transformation that can
compute a diagonalization of the matrix where the diagonal elements are the
eigenvalues, and in the latter case, $A$ can be represented in Jordan form.

If $A$ has $n$ distinct eigenvalues, that implies that $A$ has $n$ linearly
independent eigenvectors (but the reverse implication is not true!).

Next: Sastry will talk about diagonalizable case, and he'll probably get
through part of lecture notes 13.
