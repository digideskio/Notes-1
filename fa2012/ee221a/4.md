Matrix Representation of Linear Maps
====================================
September 4, 2012
-----------------
Today
-----
Matrix multiplication as a representation of a linear map; change of basis
-- what happens to matrices; norms; inner products. We may get to adjoints
today.

Last time, we talked about the concept of the range and the nullspace of a
linear map, and we ended with a relationship that related properties of the
nullspace to properties of the linear equation $\mathcal{A}(x) = b$. As
we've written here, this is not _matrix_ multiplication. As we'll see
today, it can be represented as matrix multiplication, in which case, we'll
write this as $Ax = b$.

There's one more important result, called the rank-nullity theorem. We
defined the range and nullspace of a linear operator. We also showed that
these are subspaces (range of codomain; nullspace of domain). We call
$\text{dim}(R(\mathcal{A})) = \text{rank}(\mathcal{A})$ and
$\text{dim}(N(\mathcal{A})) = \text{nullity}(\mathcal{A})$. Taking the
dimension of the domain as $n$ and the dimension of the codomain as $m$,
$\text{rank}(\mathcal{A}) + \text{nullity}(\mathcal{A}) = n$. Left as an
exercise. Hints: choose a basis for the nullspace. Presumably you'd extend
it to a basis for the domain (without loss of generality, because any set
of $n$ linearly independent vectors will form a basis). Then consider how
these relate to the range of $\mathcal{A}$. Then map $\mathcal{A}$ over
this basis.

Matrix representation
---------------------

**Any linear map between finite-dimensional vector spaces can be
represented as matrix multiplication.** We're going to show that it's true
via construction.

$\fn{\mathcal{A}}{U}{V}$. We're going to choose bases for the domain and
codomain. $\forall x \in U, x = \sum_{j=1}^n \xi_k u_j$. Now consider
$\mathcal{A}(x) = \mathcal{A}(\sum_{j=1}^n \xi_k u_j) = \sum_{j=1}^n \xi_k
\mathcal{A}(u_j)$ (through linearity). Each $\mathcal{A}(u_j) =
\sum_{i=1}^n a_{ij} v_i$. Uniqueness of $a_{ij}$ and $\xi_j$ follows from
writing the vector spaces in terms of a basis.

$$
\mathcal{A}(x) = \sum_{j=1}^n \xi_j \sum_{i=1}^m a_{ij} v_i
\\ = \sum_{i=1}^m \left(\sum_{j=1}^n a_{ij} \xi_j\right) v_i
\\ = \sum_{i=1}^m \eta_i v_i
$$

Uniqueness of representation tells me that $\eta_i \equiv \sum_{j=1}^n
a_{ij} \xi_j$. We've got $i = \{1 .. m\}$ and $j = \{1 .. n\}$. We can turn
this representation into a matrix by defining $\eta = A\xi$. $A \in
\mathbb{F}^{m \times n}$ is defined such that its $j^{\text{th}}$ column is
$\mathcal{A}(u_j)$ written with respect to the $v_i$s.

All we used here was the definitions of basis, coordinate vectors, and
linearity.

Let's do a couple of examples. Foreshadowing of work later in
controllability of systems. Consider a linear map $\fn{\mathcal{A}}
{(\Re^n, \Re)}{(\Re^n, \Re)}$. Try to derive the matrix $A \in \Re^{n
\times n}$. Both the domain and codomain have as basis $\{b,
\mathcal{A}(b), \mathcal{A}^2(b), ..., \mathcal{A}^{n-1}(b)\}$, where $b
\in \Re^n$ and $A^n = -\sum_1^n -\alpha_i \mathcal{A}^{n-i}$. Your task is
to show that the representation of $b$ and $\mathcal{A}$ is:

$$
\bar{b} = \begin{bmatrix}1 \\ 0 \\ \vdots \\ 0\end{bmatrix}
\\ \bar{A} = \begin{bmatrix}
\\ 0 & 0 & ... & 0 & -\alpha_n
\\ 1 & 0 * ... & 0 & -\alpha_{n-1}
\\ 0 & 1 * ... & \vdots & -\alpha_{n-2}
\\ \vdots & \vdots&  \ddots & \vdots & -\alpha_{n-2}
\\ \vdots & \vdots & \ddots & \vdots & -\alpha_{n-2}
\\ 0 & 0 & \dots & 1 & -\alpha_1
\end{bmatrix}
$$

This is really quite simple; it's almost by definition.

Note that these are composable maps, where composition corresponds to
matrix multiplication.

Change of basis
---------------

Consider we have $\fn{\mathcal{A}}{U}{V}$ and two sets of bases for the
domain and codomain. There exist maps between the first set of bases and
the second set; composing those appropriately will give you your change of
basis. Essentially, do a change of coordinates to those in which $A$ is
defined (represented this as $P$), apply $A$, then change the coordinates
of the codomain back (represented as $Q$). Thus $\bar{A} = QAP$.

If $V = U$, then you can easily derive that $Q = P^{-1}$, so $\bar{A} =
P^{-1}AP$.

We consider this transformation ($\bar{A} = QAP$) to be a **similarity
transformation**, and $A$ and $\bar{A}$ are called **similar**
(**equivalent**).

We derived these two matrices from the same linear map, but they're derived
using different bases.

Proof of Sylvester's inequality on homework 2.

One last note about the dimension of the rank of a linear map, which
corresponds to the rank of the associated matrix representation: that is
$\text{dim}(R(A)) = \text{dim}(R(\mathcal{A}))$. Similarly, $\text
{nullity}(A) = \text{dim}(\text{nullspace}(A)) = \text{dim}(\text
{nullspace}(\mathcal{A}))$.

Sylvester's inequality, which is an important relationship, says the
following: **Suppose you have $A \in \mathbb{F}^{m \times n}$, $B \in
\mathbb{F}^{n \times p}$, then $AB \in \mathbb{F}^{m \times p}$, then
$\text{rk}(A) + \text{rk}(B) - n \le \text{rk}(AB) \le \min(\text{rk}(A),
\text{rk}(B)$.** On the homework, you'll have to show both
inequalities. Note at the end about elementary row operations.

Next important concept about vector spaces: that of norms.

Norms
-----

With some vector spaces, you can associate some entity called a norm. We
can then speak of a **normed vector space** (more commonly known as a
**metric space**). Suppose you have a vector space $(V, \mathbb{F})$, where
$\mathbb{F}$ is either $\Re$ or $\mathbb{C}$. This is a metric space if you
can find $\fn{\mag{\cdot}}{V}{\Re_+}$ that satisfies the following axioms:

$\mag{v_1 + v_2} \le \mag{v_1} + \mag{v_2}$

$\mag{\alpha v} = \abs{\alpha}\mag{v}$

$\mag{v} = 0 \iff v = \theta$

We have some common norms on these fields:

$\mag{x}_1 = \sum_{i=1}^n \abs{x_i}$ ($\ell_1$)

$\mag{x}_2 = \sum_{i=1}^n \abs{x_i}^2$ ($\ell_2$)

$\mag{x}_p = \sum_{i=1}^n \abs{x_i}^p$ ($\ell_p$)

$\mag{x}_\infty = \max \abs{x_i}$ ($\ell_\infty$)

One of the most important norms that we'll be using: the **induced norm**
is that induced by a linear operator. We'll define $\mathcal{A}$ to be a
continuous linear map between two metric spaces; the induced norm is
defined as

$$ \mag{\mathcal{A}}_i = \sup_{u \neq \theta}
\frac{\mag{\mathcal{A}u}_V}{\mag{u}_U} $$

From analysis: the **supremum** is the least upper bound (the smallest
$\forall y \in S, x : x \ge y$).
