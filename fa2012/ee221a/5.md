EE 221A: Linear System Theory
=============================
September 6, 2012
-----------------
Induced norms of matrices
-------------------------

The reason that we're going to start talking about induced norms: today
we're just going to build abstract algebra machinery, and at the end, we'll
do the first application: least squares. We'll see why we need this
machinery and why abstraction is a useful tool.

The idea is that we want to find a norm on a matrix using existing norms on vectors.

Let 1) $\fn{A}{(U,F)}{(U,F)}$, 2) let U have the norm $\mag{\ }_u$, 3) let
V have the norm $\mag{\ }_v$. Let the **induced norm** be $\mag{A}_{u,v} =
\sup_{x\neq 0} \frac{\mag{Ax}_v}{\mag{x}_u}$. Theorem: the induced norm is
a norm. Not going to bother showing positive homogeneity and triangle
inequality (trivial in this case). Only going to show last property:
separates points. Essentially, $\mag{A}_{u,v} = 0 \iff A = 0$. The reason
that this is not necessarily trivial is because of the supremum. It's a
complex operator that's trying to maximize this function over an infinite
set of points. It's possible that the supremum does not actually exist at a
finite point.

The first direction is easy: if $A$ is zero, then its norm is 0 (by
definition -- numerator is 0).

The second direction is a hard one. If $\mag{A}_{u,v} = 0$, then given any
$x \neq 0$, it holds that $\frac{\mag{Ax}_u}{\mag{v}_u} \le 0$ (from the
definition of supremum). Denominator must be positive definite (being the
norm of a vector), and numerator must be positive definite (also being a
norm). Thus the norm is also bounded below by zero, which means that the
numerator is zero for all nonzero x. Thus everything is in the nullspace of
$A$, which means that $A$ is zero.

Proposition: the induced norm has (a) $\mag{Ax}_u \le \mag{A}_{u,v}
\mag{x}_u$; (b) $\mag{AB}_{u,v} \le \mag{A}_{u,v} \mag{B}_{u,v}$. (b)
follows from (a).

Not emphasized in Claire's notes: induced norms form a small amount of all
possible norms on matrices.

Examples of induced norms:

* $\mag{A}_{1,1} = \max_j \sum_i \abs{u_{ij}}$: maximum column sum: maximum
  of the sum of columns;
* $\mag{A}_{2,2} = \max_j \sqrt{\lambda_j A^T A}$: max singular value norm;
* $\mag{A}_{\infty, \infty} = \max_i \sum_j \abs{u_{ij}}$: maximum row sum.

Other matrix: special case of Schatten norms. (a) Frobenius norm
$\sqrt{\text{trace}(A^T A)}$. Also square root of singular
values. Convenient way to write nuclear norm.

Statistical regularization; Frobenius norm is analogous to $\ell_2$
regularization; nuclear norm analogous to $\ell_1$ regularization. It is
important to be aware that these other norms exist.

Sensitivity analysis
--------------------
Nice application of norms, but we won't see that it's a nice application
until later.

Computation for numerical linear algebra.

Some algebra can be performed to show that if $Ax_0 = b$ (when $A$
invertible), then for $(A + \delta A)(x + \delta_x) = b + \delta b$, we
have an approximate bound of $\frac{\mag{\delta_x}}{\mag{x_0}} \le
\mag{A}\mag{A^{-1}} \bracks{\frac{\mag{\delta A}}{\mag{A}} +
\frac{\mag{\delta b}}{\mag{b}}}$. Need to engineer computation to improve
situation. Namely, we're perturbing $A$ and $b$ slightly: how much can the
solution vary? In some sense, we have a measure of effect
($\mag{A}\mag{A^{-1}}$) and a measure of perturbation. The first quantity
is important enough that people in linear algebra have defined it and
called it a **condition number**: $\kappa(A) = \mag{A}\mag{A^{-1}} \ge
1$. The best you can do is 1. If you have a condition number of 1, your
system is well-conditioned and very robust to perturbations. Larger
condition number will mean less robustness to perturbation.

More machinery: Inner Product & Hilbert Spaces
----------------------------------------------

Consider a linear space $(H, \mathbb{F})$, and define a function
$\fn{\braket{}{}}{(H, \mathbb{F})}{\mathbb{F}}$. This function is an
inner product if it satisfies the following properties.

* Conjugate symmetry. $\braket{x}{y} = \braket{y}{x}^*$.
* Homogeneity. $\braket{x}{\alpha y} = \alpha \braket{x}{y}$.
* Linearity. $\braket{x}{y + z} = \braket{x}{y} + \braket{x}{z}$.
* Positive definiteness. $\braket{x}{x} \ge 0$, where equality only occurs
  when $x = 0$.

Inner product spaces have a natural norm (might not be the official name),
and that's the norm induced by the inner product.

One can define $\mag{x}^2 = \braket{x}{x}$, which satisfies the axioms of a
norm.

Examples of Hilbert spaces: finite-dimensional vectors. Most of the time,
infinite-dimensional Hilbert spaces match up with finite-dimensional. All
linear operators in finite vector spaces are continuous because they can be
written as a matrix (not always the case with infinite vector
spaces). Suppose I have the field $\mathbb{F}$; $(\mathbb{F}^n,
\mathbb{F})$, where the inner product $\braket{x}{y} = \sum_i \bar{x_i}
y_i$, but another important inner product space is the space of
square-integrable functions, $L^2([t_0, t_1], \mathbb{F}^n
)$. Infinite-dimensional space which actually is the space spanned by
Fourier series. It turns out that the inner product (of functions) is
$\int_{t_0}^{t_1} f(t)^* g(t) dt$.

We're going to power through a little more machinery, but we're getting
very close to the application. Need to go through adjoints and
orthogonality before we can start doing applications.

Adjoints
--------
Consider Hilbert spaces $(U, \mathbb{F}, \braket{}{}_u), V, \mathbb{F},
\braket{}{}_v)$, and let $\fn{A}{U}{V}$ be a continuous linear
function. The **adjoint** of $A$ is denoted $A^*$ and is the map
$\fn{A^*}{V}{U}$ such that $\braket{x}{Ay}_v = \braket{A^*}{y}_u$.

Reasoning? Sometimes you can simplify things. Suppose $A$ maps an
infinite-dimensional space to a finite-dimensional space (e.g. functions to
numbers). In some sense, you can convert that function into something that
goes from real numbers to functions on numbers. Generalization of the
Hermitian transpose.

Consider functions $f, g \in C([t_0, t_1], \Re^n)$. What is the adjoint of
$\fn{A}{C([t_0, t_1], \Re^n)}{\Re}$, where $A = \braket{g}{f}_{C
([t_0, t_1], \Re^n)}$? (aside: this notion of the adjoint will be very
important when we get to observability and reachability)

Observe that $\braket{v}{A}_\Re = v \cdot A = v \braket{g}{f}_C = \braket{v
g}{f}$, and so consequently, we have that the adjoint of $A^*[v] = v g$.

Orthogonality
-------------
With Hilbert spaces, one can define orthogonality in an axiomatic manner (a
more abstract form, rather). Let $(H, \mathbb{F}, \braket{}{})$ be a
Hilbert space. Two vectors $x, y$ are defined to be **orthogonal** if
$\braket{x}{y} = 0$.

Cute example: suppose $c = a + b$ and $a, b$ are orthogonal. In fact,
$\mag{c}^2 = \mag{a + b}^2 = \braket{a + b}{a + b} = \braket{a}{a} +
\braket{b}{b} + \braket{a}{b} + \braket{b}{a} = \mag{a}^2 +
\mag{b}^2$. Cute because the result is the Pythagorean theorem, which we
got just through these axioms.

One more thing: the orthogonal complement of a subspace $M$ in a Hilbert
space is defined as $M^\perp = \set{y \in H}{\forall x \in M
\braket{x}{y}}$.

We are at a point now where we can talk about an important theorem:

Fundamental Theorem of Linear Algebra (partially)
-------------------------------------------------
Let $A \in \Re^{m \times n}$. Then:

* $R(A) \perp N(A^T)$
* $R(A^T) \perp N(A)$
* $R(AA^T) = R(A)$
* $R(A^TA) = R(A^T)$
* $N(AA^T) = N(A)$
* $N(A^TA) = N(A^T)$

Proofs:

* Given any $x \in \Re^n, y \in \Re^m \st A^T y = 0$ ($y \in N(A^T)$),
  consider the quantity $\braket{y}{Ax} = \braket{A^Ty}{x} = 0$.

* Given any $x \in \Re^n, \exists y \in \Re^m \st x = A^T y + z$, where $z
  \in N(A)$(as a result of the decomposition above). Thus $Ax =
  AA^Ty$. Implies that $R(A) \subset R(A A^T)$

Now for the application.

Application: Least Squares
--------------------------
Consider the following problem: minimze $\mag{y - Ax}_2$, where $y \not\in
R(A)$. If $y$ were in the range of A, and A were invertible, the solution
would be trivial ($A^{-1}y$). In many problems, $A \in \Re^{m\times n}$,
where $m \gg n$, $y \in \Re^m$, $x \in \Re^n$.

Since we cannot solve $Ax = y$, we instead solve $Ax = \hat{y}$. According
to our intuition, we would like $y - \hat{y}$ to be orthogonal to
$R(A)$. From the preceding (partial) theorem, this means that $y - \hat{y}
\in N(A^T) \iff A^T(y - y_0) = 0$. Remember: what we really want to solve
is $A^T(y - Ax) = 0 \implies A^T Ax = A^T y \implies x = (A^T A)^{-1} A^T
y$ if $A^T A$ is invertible.

If A has full column-rank (that is, for $A \in \Re^{m \times n}$, we have
$R(A) = n$), then this means that in fact $N(A) = \{0\}$, and the preceding
theorem implies that the dimension of $R(A^T) = n$, which means that the
dimension of $R(A^T A) = n$. However, $A^T A \in \Re^{n \times n}$. Thus,
$A^T A$ is invertible.

Back to condition numbers (special case)
----------------------------------------
Consider a self-adjoint and invertible matrix in $\Re^{n \times
n}$. $\hat{x} = (A^T A)^{-1} A^T y = A^{-1} y$. We have two ways of
determining this value: the overdetermined least-squares solution and the
standard inverse. Let us look at the condition numbers.

$\kappa(A^T A) = \mag{A^T A}\mag{(A^T A)^{-1}} = \mag{A^2}\mag{(A^{-1})^2}
= \bracks{\kappa(A)}^2$. This result is more general: also applies in the
$L^2$ case even if $A$ is not self-adjoint. As you can see, this is worse
than if we simply use the inverse.

Gram-Schmidt orthonormalization
-------------------------------
This is a theoretical toy, not used for computation (numerics are very bad).

More definitions:

A *set* of vectors S is **orthogonal** if $x \perp y \forall x
\neq y$ and $x, y \in S$.

The set is **orthonormal** if also $\mag{x} = 1, \forall x \in S$. Why do we
care about orthonormality? Consider Parseval's theorem. The reason you get
that theorem is that the bases are required to be orthonormal so that you
can get that result. Otherwise it wouldn't be as clean. That's typically
why people like orthonormal bases: you can represent your vectors as just
coefficients (and you don't need to store the length of the vectors).

We conclude with an example of Gram-Schmidt orthonormalization. Consider
the space $L^2([t_0, t_1], \Re)$. Suppose I have $v_1 = 1, v_2 = t, v_3 =
t^2$, $t_0 = 0$, $t_1 = 1$, and $\mag{v_1}^2 = \int_0^1 1 \cdot 1 dt =
1$. The key idea of Gram-Schmidt orthonormalization is the following: start
with $b_1 \equiv \frac{v_1}{\mag{v_1}}$. Then go on with $b_2 = \frac{v_2 -
\braket{v_2}{b_1}b_1}{\mag{v_2 - \braket{v_2}{b_1}b_1}}$, and repeat until
you're done (in essence: you want to preserve only the component that is
orthogonal to the space spanned by the vectors you've computed so far, then
renormalize).

Basically, you get after all this computation that $b_2 = \frac{1}{12} t -
\frac{1}{24}$. Same construction for $b_3$.
