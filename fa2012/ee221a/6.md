Singular Value Decomposition & Introduction to Differential Equations
=====================================================================
September 11, 2012
-----------------

Reviewing the adjoint, suppose we have two vector spaces $U, V$; like we
have with norms, let us associated a field that is either $\Re$ or
$\mathbb{C}$. Assume that these spaces are inner product spaces (we're
associating with each an inner product). Suppose we have a continuous
(linear) map $\fn{\mathcal{A}}{U}{V}$. We define the **adjoint** of this
map to be $\fn{\mathcal{A}^*}{V}{U}$ such that $\braket{u}{\mathcal{A} v} =
\braket{\mathcal{A}^* v}{u}$.

We define **self-adjoint** maps as maps that are equal to their adjoints,
i.e. $\fn{\mathcal{A}}{U_1}{U_2} \st \mathcal{A} = \mathcal{A}^*$.

In finite-dimensional vector spaces, the adjoint of a map is equivalent to
the conjugate transpose of the matrix representation of the map. We refer
to matrices that correspond to self-adjoint maps as **hermitian**.

Unitary matrices
----------------
Suppose that we have $U \in \mathbb{F}^{n\times n}$. $U$ is **unitary** iff
$U^*U = UU^* = I_n$. If $\mathbb{F}$ is $\Re$, the matrix is called
**orthogonal**.

These constructions lead us to something useful: singular value
decomposition. We'll come back to this later when we talk about matrix
operations.

Singular Value Decomposition (SVD)
----------------------------------
Suppose you have a matrix $M \in \mathbb{F}^{m\times m}$. An **eigenvalue**
$\lambda$ of $M$ is a complex number iff there exists a nonzero vector $v$
such that $Mv = \lambda v$ ($v$ is thus called the **eigenvector**
associated to $\lambda$). Now we can think about how to define singular
values of a matrix in terms of these definitions.

Let us think about this in general for a matrix $A \in \mathbb{F}^{m \times
n}$ (which we consider to be a matrix representation of some linear map
with respect to a basis). Note that $A A^* = \mathbb{F}^{m \times m}$,
which will have $m$ eigenvalues $\lambda_i, i = 1 ... m$.

Note that $AA^*$ is hermitian. We note that from the Spectral theorem, we
can decompose the matrix into an orthonormal basis of eigenvectors
corresponding to real eigenvalues. In fact, in this case, the eigenvalues
must be real and non-negative.

If we write the eigenvalues of $AA^*$ as $\lambda_1 \ge \lambda_2 \ge
... \ge \lambda_m$, where the first $r$ are nonzero, note that $r =
\text{rank} AA^*$. We define the **non-zero singular values** of $A$ to be
$\sigma_i = \sqrt{\lambda_i}, i \le r$. The remaining singular values are
zero.

Recall the **induced 2-norm**: let us relate this notion of singular values
back to the induced 2-norm of a matrix $A$ ($\mag{A}_{2,i}$). Consider the
induced norm to be the norm induced by the action of $A$ on the domain of
$A$; thus if we take the induced 2-norm, then this is the $\max (\lambda_i
(A^*A))^{1/2}$, which is simply the maximum singular value.

Now that we know what singular values are, we can do a useful decomposition
called singular value decomposition.

Take $M \in \mathbb{C}^{m \times n}$. We have the following theorem: there
exist unitary matrices $U \in \mathbb{C}^{m \times m}, V \in \mathbb{C}^{n
\times n}$ such that $A = U \Sigma V$, where $\Sigma$ is defined as a
diagonal matrix containing the singular values of $A$. Consider the first
$r$ columns of $U$ to be $U_1$, the first $r$ columns of $V$ to be $V_1$,
and the $r \times r$ block of $\Sigma$ containing the nonzero singular
values to be $\Sigma_r$. Then $A = U \Sigma V = U_1 \sigma_r
V_1^*$.

Consider $AA^*$. With a bit of algebra, we can show that $AA^*U_1 = U_1
\sigma_r^2$. We call the columns $u_i$ of $U_1$ are the eigenvectors of
$AA^*$ associated to eigenvalues $\sigma_i^2$; these are called the
**right-singular vectors**.

Similarly, if we consider $A^*A$, we can show that $A^*A = V_1^* \Sigma_r^2
V_1$ and that $v_i^* A^*A = \Sigma_r^2 v_1^*$; the columns of this matrix
are called the **left-singular vectors**.

Recap
-----
We've covered a lot of ground these past few weeks: we covered functions,
vector spaces, bases, and then we started to consider linearity. And then
we started talking about endowing vector spaces with things like norms,
inner products; induced norms. From that, we went on to talk about
adjoints. We used adjoints, we went on to talk a little about projection
and least-squares optimization. We then went on to talk about Hermitian
matrices and singular value decomposition. I think about this first unit as
having many basic units that we'll use over and over again. Two interesting
applications: least-squares, SVD.

So we have this basis now to build on as we talk about linear
systems. We'll also need to build a foundation on linear differential
equations. We'll spend some time going over the basics: what a solution
means, under what conditions a solution exists (i.e. what properties does
the differential equation need to have?). We'll spend the next couple weeksn
talking about properties of differential equations.

All of what we've done up to now has been covered in appendix A of Callier
& Desoer. For the introduction to differential equations, we'll follow
appendix B of Callier & Desoer. Not the easiest to read, but very
comprehensive background reading.

The existence and uniqueness theorems are in many places, however.

Lecture notes 7.

Differential Equations
----------------------

$$
\dot{x} = f((x(t), t)), x(t_0) = x_0
\\ x \in \Re^n
\\ \fn{f}{\Re^n \times \Re}{\Re^n}
$$

(strictly speaking, $f$ maps $x$ to the tangent space, but for this course,
we're going to consider the two spaces to be equivalent)

Often, we're going to consider the **time-invariant** case (where there is
no dependence on $t$, but rather only on $x$), but this is a time-variant
case. Recall that we consider time to be a privileged variable, i.e. always
"marching forward".

What we're going to talk about now is how we can solve this differential
equation. Rather (for now), under what conditions does there exist a
(unique) solution to the differential equation (with initial condition)?
We're interested in these two properties: existence and uniqueness. The
solution we call $x(t)$ where $x(t_0) = x_0$. We need some understanding of
some properties of that function $f$. We'll talk about continuity,
piecewise continuity, Lipschitz continuity (thinking about the
existence). In terms of uniqueness, we'll be talking about Cauchy
sequences, Banach spaces, Bellman-Gr&ouml;nwall lemma.

A couple of different ways to prove uniqueness and existence; we'll use the
Callier & Desoer method.

We'll finish today's lecture by just talking about some definitions of
continuity. Suppose we have a function $f(x)$ that is said to be
**continuous**: that is, $\forall \epsilon > 0, \exists \delta > 0 \st
\abs{x_1
- x_2} < \delta \implies \abs{f(x_1) - f(x_2)} < \epsilon$
  ($\epsilon$-$\delta$ definition).

Suppose we have $\fn{f(x,t)}{\Re^n \times \Re}{\Re^n}$. $f$ is said to be
piece-wise continuous (w.r.t. $t$), $\forall x$ if $\fn{f(x,
\cdot)}{\Re}{\Re^n}$ is continuous except at a finite number of
(well-behaved) discontinuities in any closed and bounded interval of
time. What I'm not allowing in this definition are functions with
infinitely many points of discontinuity.

Next time we'll talk about Lipschitz continuity.
