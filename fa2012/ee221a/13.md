Eigenvalues, etc.
=================
October 4, 2012
---------------

If $A$ is real, then $d_1, ... d_n \in \Re$, which also tells us that roots
appear in complex conjugate pairs. If the matrix is complex, all bets are
off.

Amazingly, the study of these eigenvalues and eigenvectors is actually a
lot easier if the $\lambda_i$ are **distinct**, i.e. $\lambda_i\lambda_j =
\delta_{ij}$. The first thing to notice is that if you have $n$ distinct
eigenvalues, from the definition of eigenvectors, there's at least one
eigenvector associated with every eigenvalue.

First thing to prove is that the eigenvectors of distinct eigenvectors are
linearly independent. Consider a solution to the homogeneous equation
$\sum_i \alpha_i e_i$. Apply $A - \lambda_i I$ for each $i$ except for one;
as a result, we have $\alpha_k (\lambda_k - \lambda_1)(\lambda_k -
\lambda_2) ... (\lambda k - \lambda_n) e_k$ for some $k$. Thus since the
$\lambda_i$ are all distinct, $\alpha_k$ must be identically $0$ for all
$k$.

Jordan's Theorem
----------------

We have $A\begin{bmatrix}e_1 & e_2 & ... & e_n \end{bmatrix} =
\begin{bmatrix} \lambda_1 & 0 & ... & 0 \\ 0 & \lambda_2 & ... & 0 \\
\vdots & & \ddots & \\ 0 & & ... & 0\end{bmatrix}\begin{bmatrix}e_1 & e_2 &
... & e_n\end{bmatrix} = T^{-1}\Lambda T$. If you look at $A$ in a new
basis given by the eigenvectors, $A$ is a diagonal matrix, where the
diagonal entries are given by the eigenvalues.

Jordan (canonical) form of a matrix.

Let's give a name to the inverse of $T^{-1} = \begin{bmatrix} v_1^T \\
v_2^T \\ \vdots \\ v_n^T \end{bmatrix}$. Looks also like eigenvectors;
these are called the left eigenvectors ($\{e_i\}$ are the right
eigenvectors) -- these are actually row vectors as opposed to column
vectors.

Notion of an outer product. The dyadic (outer) product of the right and
left eigenvectors is sometimes called the residual product (basically, a
projection matrix): $e_i v_i^T = R_i$. If you flipped the order of the
arguments, that would also be the identity, and what that yields is a
scalar: $v_i^T e_j = \delta_{ij}$. How would you normally define a function
$f(A)$? $f(A) = \sum_i \lambda_i R_i$. $f$ is required to be **analytic**
at $\lambda_i$.

Let's try to apply this to (single input, single output) linear
systems. Assume that $A$ has distinct eigenvalues. That means that there
exists $T$ such that $A = T^{-1}\Lambda T$. If I were to rewrite this with
a change of basis with $z = Tx$, then $\dot{z} = \Lambda z + Tbu$, $y =
cT^{-1}z$. If we consider the Laplace transform, we have poles
corresponding to the eigenvalues. No coupling; cannot observe. By putting
systems in modal form, you can actually see which inputs are influencing
the overall transfer function. Just the transient response corresponding to
the initial condition. The multi-input, multi-output matrix is identical,
by linearity.

Just to tell you that this kind of manipulation has some quite interesting
other implications, let me lead you through another calculation and talk
about some popular misconceptions about linear systems. In most
undergraduate courses, you'll be told about the "eigenfunction property":
if you put in an input at some frequency, you'll get an output at the same
frequency. Two problems: is that always true for every frequency, and
secondly, what about the role of the initial condition: what if the system
is unstable?

What I'm going to do is just an exercise in Laplace transforms,
really. (end of lecture 12 notes)

Output generally corresponds to the **force response**, the response to the
input at the same frequency, (by the way, without further assumption, the
eigenfunction property is immediately false if you force it at one of its
**natural modes**, i.e. eigenvalues) and a second component, the
contribution due to the initial condition. If $A$ is stable, then the
second component goes to 0. Is it possible to still use this response by
clever choice of initial condition? Yes: regularization (unique choice of
$x_0$ once $u_0$ is specified and $B$ is given: $x_0 = = (\lambda I -
A)^{-1}Bu_0$). Otherwise, we have a transient response, and if $A$ is
growing exponentially, then this will dominate the system.

The other thing this calculation is good for is that it tells you something
about the zeros of a linear system. The **zeros** are (1) $\exists u_0 \neq
\theta \st H(\lambda) u_0 = \theta$. $\lambda$ is a **zero of
transmission** if at this value, $\hat{H}$ has a nontrivial right
nullspace. Here is a way to think about what a zero of transmission is: it
blocks transmission at that value $\lambda$. If you've chosen the initial
condition properly, you'll see nothing at the output.

What you need to remember: distinct eigenvalues means distinct
eigenvectors; you can decompose a matrix into its Jordan form; we can see
which modes affect the output; and here is a definition of a zero of
transmission: if there is a nontrivial choice of $u_0$ and frequency
$\lambda$ where you can block transmission.

Lecture Notes 13
----------------

Repeated eigenvalue case. Invariant subspaces. If $\fn{A}{(U,
\mathbb{C})}{(U, \mathbb{C})}$, $\mathcal{M} \subset V$ is said to be an
**invariant subspace** if $x \in \mathcal{M} \implies Ax \in
\mathcal{M}$. Aside: $\mathcal{M}$ is also invariant under any function of
$A$. Examples of invariant subspaces: $\mathcal{N}(A)$, $\mathcal{R}(A)$,
$\mathcal{N}(A - \lambda I)$, $m_1 \cap m_2$ (with invariant subspaces
$m_1, m_2$), $m_1 + m_2$ ($+$ is the **sum**). The reason why $m_1 \cup
m_2$ is not an invariant subspace is because it is not a subspace.n
