Linear Quadratic Optimization
=============================
October 23, 2012
----------------

How old is control theory? As old as Troy. Legend goes that Aeneas, when in
Carthage met Queen Dido and discovered the following legend: local
chieftains of Carthage gave her a piece of oxhide and told her she could be
the queen of whatever she could cover with the oxhide. Legend says that she
took the oxhide, cut it into a long thin strip, and she solved the
corresponding isoparametric optimization problem.

Mechanics is a special case of optimal control. Part of what we're not
trying to do is drag you through thousands of years of thinking, but rather
to encapsulate so many years of human thought into two lectures.

Jerry gave you one version of this; different way, to get to where optimal
control is going.

Start by writing down a linear system: $\dot{x} = A(t)x + B(t)u, y(t) =
C(t)x + D(t) u$. Associated with this linear system, one defines the dual
system (also known as the costate) -- last time we represented this with
$p$. Following from the notes, $-\dot{\tilde{x}} = A^*\tilde{x} + C^*(t)
\tilde{u}, \y = B^*\tilde{x} + D^*\tilde{u}$. Recall that these are linear
maps between continuous functions. This is the adjoint (or dual) system,
which plays a large role in optimal control.

In circuit theory or mechanical systems, you might find the dual of a
circuit by replacing capacitors with inductors, series with parallel, etc,
or you might see mass-damper spring systems. And so that's the relationship
between them.

A pretty interesting equality that goes into them is so-called the pairing
lemma, which is a way to associate the initial conditions and final
state. The pairing lemma states that $\braket{\tilde{x}(t)}{x(t)} +
\int_{t_0}^t \braket{\tilde{u}(\tau)}{y(\tau)}d\tau = \braket
{\tilde{x}(t_0)}{x(t_0)} + \int_{t_0}^t \braket{\tilde{y}(\tau)} {u(\tau)}
d\tau$. The proof is not very involved: start with $\braket{\dot{\tilde{x}}
+ A^*\tilde{x} + C^*\tilde{u}}{x} + \braket{-y + B^*\tilde{x} +
D^*\tilde{u}}{u} = 0$; work through algebra. Think of this as a little
algebraic fact; it has some meaning in terms of the language of the
adjoint, but let's not get too sidetracked yet.

Using the pairing lemma, there's a more algebraic proof, compared to the
one on Thursday. In these notes, we set up the optimal control problem.

The optimal control problem that you give yourself is stated for the given
system: minimize, for a certain choice of $u$, the cost function as defined
last time: $J(u) = \frac{1}{2}\bracks{\int_{t_0}^{t_1} (\mag{u(t)}^2_2 +
\mag{C(t)x(t)}^2_2) dt + x^T(t_1)Sx(t_1)}$ (note that $S > 0$, so it's a
Hermitian positive semi-definite matrix). Note: there's a cost on the final
state, cost associated with regulation of state, and in fact, this whole
thing doesn't need you to define an output $y$. Optimal control says that
given some dynamics, maximize this.

The reason you talk about Newton and Calculus of variations is that Newton
considered this. Newton's laws were a consequence of the particles
minimizing the total action ($(\mag{u(t)}^2_2 + \mag{C(t)x(t)}^2_2)$ is the
Lagrangian).

The way you do this is via calculus of variations: $J(u + \epsilon\delta
u(t)) = J(u) + \epsilon\delta J(\delta u) + O(\epsilon)$. If $u$ were the
best choice, $\forall \delta u$ (in some class), $\delta(\delta u) = 0$.

Newton invented calculus to solve these calculus of variations problems
because he wanted to describe how things moved.

So what you get from this is that $\delta u$ causes a $\delta x$, and as
last time, you get $\delta \dot{x} = A\delta x + B \delta u, \delta x(t_0)
= 0$. From this (when you do the first variation), we saw that you get
$\int_{t_0}^{t_1} \braket{u}{\delta u} dt + \int_{t_0}^{t_1}\braket{C^*Cx}
{\delta x}dt + \braket{Sx}{\delta x_1}$. This is a necessary condition for
optimality, but it isn't so explicit. The reason to use this pairing lemma
is to simplify the differential equation. I'm going to associate with the
system $\delta\dot{x} = A\delta x + B\delta u$ a dummy output, i.e. $y =
\delta x$. The dual of this system is simply $-\dot{\tilde{x}} =
A^*\tilde{x} + \tilde{u}, \tilde{y} = B^*\tilde{x}$.

Using the pairing lemma and working through a bit of algebra, we get that
$\int_{t_0}^{t_1} \bracks{\braket{u}{\delta u} + \braket{B^* \tilde{u}}
{\delta u}}dt = 0$, so $\int_{t_0}^{t_1} \bracks{\braket{u +
B^*\tilde{x}}{\delta u}dt = 0$.

(aside: Kalman filtering is the dual of optimal control)

The Hamiltonian system: $\begin{bmatrix}\dot{x} \\ \dot{\tilde{x}}
\end{bmatrix} = \begin{bmatrix}A & -BB^* \\ -C^*C & -A^* \end{bmatrix}
\begin{bmatrix}x \\ \tilde{x} \end{bmatrix}$, $x(t_0) = x_0, \tilde{x}(t_1)
= Sx(t_1)$. In mathematics, we call this a two-point boundary equation. Not
fun to solve, generally.

However, the theorem (of Linear Quadratic Optimal Control) is as follows:
$U = B^*(t) P(t) x(t)$; we have $-\dot{P} = A^*P + PA - PBB^*P + C^*C$,
$P(t_1) = -S$. This has a very famous name associated with it:
Ricatti.

Ricatti did the following: given $x$ and $y$ which were both linear
equations in terms of each other, and you defined the new state variable $z
= y/x$, what differential equation would $z$ satisfy? $\dot{z} = a + bz -
cz + dz^2$. Further, the reason that the original equation looks quadratic
is because $P(t) = \tilde{x} X^{-1} $, where the matrices satisfy the
Hamiltonian system we had before. Amazingly, the answer to this problem is
simply a state feedback law, $u = F(t)x(t)$, where $F(t) =
B^*(t)P(t)$.

If we consider the matrix differential equation $\deriv{}{t}
\begin{bmatrix}X \\ \tilde{X} \end{bmatrix} = \begin{bmatrix}A & -BB^* \\
-C^*C & -A^* \end{bmatrix} \begin{bmatrix}X \\ \tilde{X} \end{bmatrix}$,
$x(t_0) = x_0, \tilde{x}(t_1) = Sx(t_1)$ where $X(t_1) = I, \tilde{X}(t_1)
= S$,

What we want to show is that $u(t) = -B^*\tilde{X}X^{-1}x(t)$. One part of
this proof is to show that $X(t)$ is invertible (page 36 of C&D, so we'll
skip this).

Begin by choosing k such that $X(t_0)k = x_0$, then define $x(t) = X(t) k,
\tilde{x}(t) = \tilde{X}(t) k$. As an artifact of defining this, $x,
\tilde{x}$ satisfy the same expressions, and by the initial conditions,
$x(t_1) = k$, so $\tilde{x}(t_1) = Sx(t_1)$. This shows now that $u(t) =
-B^* \tilde{X} X^{-1} x(t)$, which is our linear feedback system, so we
must show that $\tilde{X} X^{-1} = P$, which is more algebra.

Note: we can find the derivative of $\deriv{}{t}X^{-1}$ by differentiating
$X^{-1} X = I$.

If you take the ratio of two quantities in a differential equation, then
this ratio satisfies the Ricatti equation. People say that the Ricatti
equation is the simplest linear differential equation because it's just a
linear differential equation in disguise.

The person we should really credit for finishing off optimal control
(coming up with this composite story) was R. E. Kalman. This paper (1961)
was rejected by most journals, including IEEE.

There had been a Russian called Pontryagin, who came up with a version of
optimal control: in classical mechanics, it's Hamilton's principle of
minimized action; Pontryagin said that given $\int_{t_0}^{t_1} L(x, u)dt +
V(x(t_1))$ (Pontryagin's maximum principle). Zadeh and Desoer rewrote
Pontryagin's maximum principle. Kalman, of course, to his credit, made
everything quadratic.

Algebraic Ricatti equation: $C^*C + A^*\bar{P} + \bar{P}A^* - \bar{P}BB^*
\bar{P} = 0$, in time-invariant case. To do this right, you'll have to
cover notions of stability, then controllability and observability. in
undergraduate classes, you heard about root-locus, in which you tweak a
parameter and make sure that poles don't end up in the right-half plane.
