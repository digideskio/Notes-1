Proof of Existence and Uniqueness Theorem
=========================================
September 18, 2012
------------------

Today:

* proof of existence and uniqueness theorem.
* [ if time ] introduction to dynamical systems.

First couple of weeks of review to build up basic concepts that we'll be
drawing upon throughout the course. Either today or Thursday we will launch
into linear system theory.

We're going to recall where we were last time. We had the fundamental
theorem of differential equations, which said the following: if we had a
differential equation, $\dot{x} = f(x,t)$, with initial condition $x(t_0) =
x_0$, where $x(t) \in \Re^n$, etc, if $f( \cdot , t)$ is Lipschitz
continuous, and $f(x, \cdot )$ is piecewise continuous, then there exists a
unique solution to the differential equation / initial condition pair (some
function $\phi(t)$) wherever you can take the derivative (may not be
differentiable everywhere: loses differentiability on the points where
discontinuities exist).

We spent quite a lot of time discussing Lipschitz continuity. Job is
usually to test both conditions; first one requires work. We described a
popular candidate function by looking at the mean value theorem and
applying it to $f$: a norm of the Jacobian function provides a candidate
Lipschitz if it works.

We also described local Lipschitz continuity, and often, when using a norm
of the Jacobian, that's fairly easy to show.

Important point to recall: a norm of the Jacobian of $f$ provides a
candidate Lipschitz function.

Another important thing to say here is that we can use any norm we want, so
we can be creative in our choice of norm when looking for a better bound.

We started our proof last day, and we talked a little about the structure
of the proof. We are going to proceed by constructing a sequence of
functions, then show (1) that it converges to a solution, then show (2)
that it is unique.

Proof of Existence
------------------
We are going to construct this sequence of functions as follows:
$x_{m+1}(t) = x_0 + \int_0^t f(x_m(\tau)) d\tau$. Here we're dealing with
an arbitrary interval from $t_1$ to $t_2$, and so $0 \in [t_1, t_2]$. We
want to show that this sequence is a Cauchy sequence, and we're going to
rely on our knowledge that the space these functions are defined in is a
Banach space (hence this sequence converges to something in the space).

We have to put a norm on the set of reals, so we'll use the infinity
norm. Not going to prove it, but rather state it's a Banach space. If we
show that this is a Cauchy sequence, then the limit of that Cauchy sequence
exists in the space. The reason that's interesting is that it's this limit
that provides a candidate for this differential equation.

We will then prove that this limit satisfies the DE/IC pair. That is
adequate to show existence. We'll then go on to prove uniqueness.

Our immediate goal is to show that this sequence is Cauchy, which is, we
should show $\exists m \st (x_{m+p} - x_m) \to 0$ as $m$ gets large.

First let us look at the difference between $x_{m+1}$ and $x_m$. Just
functions of time, and we can compute this. $\mag{x_{m+1} - x_m} =
\int_{t_0}^t (f(x_m, \tau) - f(x_{m+1}, \tau)) d\tau$. Use the fact that f
is Lipschitz continuous, and so it is $\le k(\tau)\mag{x_m(\tau) -
x_{m+1}(\tau)} d\tau$. The function is Lipschitz, so well-defined, and it
has a supremum in this interval. Let $\bar{k}$ be the supremum of $k$ over
the whole interval $[t_1, t_2]$. This means that we can take this
inequality and rewrite as $\mag{x_{m+1} - x_m} \le \bar{k} \int_{t_0}^t
\mag{x_m(\tau) - x_{m+1}(\tau)} d\tau$. Now we have a bound that relates
the bound between $x_m$ and $x_{m+1}$. You can essentially relate the
distance we've just related between two subsequent elements to some further
distance by counting.

Let us do two things: sort out the integral on the right-hand-side, then
look at arbitrary elements beyond an index.

We know that $x_1(t) = x_0 + \int_{t_0}^t f(x_0, \tau) d\tau$, and that $x_1
- x_0 \le \int_{t_0}^{t} \mag{f(x_0, \tau)} d\tau \le \int_{t_1}{t_2}
  \mag{f(x_0, \tau) d\tau} \defequals M$. From the above inequalities,
  $\mag{x_2 - x_1} \le M \bar{k}\abs{t - t_0}$. Now I can look at general
  bounds: $x_3 - x_2 \le \frac{M\bar{k}^2 \abs{t - t_0}^2}{2!}$. In general,
  $x_{m+1} - x_m \le \frac{M\parens{\bar{k} \abs{t - t_0}}^m}{m!}$.

If we look at the norm of $\dot{x}$, that is going to be a function
norm. What I've been doing up to now is look at a particular value $t_1 < t
< t_2$.

Try to relate this to the norm $\mag{x_{m+1} - x_m}_\infty$. Can what we've
done so far give us a bound on the difference between two functions? We
can, because the infinity norm of a function is the maximum value that the
function assumes (maximum vector norm for all points $t$ in the interval
we're interested in). If we let $T$ be the difference between our larger
bound $t_2 - t_1$, we can use the previous result on the pointwise norm,
then a bound on the function norm has to be less than the same
bound, i.e. if a pointwise norm function is less than this bound for all
relevant $t$, then its max value must be less than this bound.

That gets us on the road we want to be, since that now gets us a bound. We
can now go back to where we started. What we're actually interested in is
given an index $m$, we can construct a bound on all later elements in the
sequence.

$\mag{x_{m+p} - x_m}_\infty = \mag{x_{m+p} + x_{m+p-1} - x_{m+p-1} + ... -
x_m} = \mag{\sum_{k=0}^{p-1} (x_{m+k+1} - x_{m+k})} \le M \sum_{k=0}^{p-1}
\frac{(\bar{k}T)^{m+k}}{(m+k)!}$.

We're going to recall a few things from undergraduate calculus: Taylor
expansion of the exponential function and $(m+k)! \ge m!k!$.

With these, we can say that $\mag{x_{m+p} - x_m}_\infty \le
M\frac{(\bar{k}T)^m}{m!} e^{\bar{k} T}$. What we'd like to show is that this
can be made arbitrarily small as $m$ gets large. We study this bound as $m
\to \infty$, and we recall that we can use the Stirling approximation,
which shows that factorial grows faster than the exponential function. That
is enough to show that $\{x_m\}_0^\infty$ is Cauchy. Since it is in a
Banach space (not proving, since beyond our scope), it converges to
something in the space to a function (call it $x^\ell$) in the same
space.

Now we just need to show that the limit $x^\ell$ solves the differential
equation (and initial condition). Let's go back to the sequence that
determines $x^\ell$. $x_{m+1} = x_0 + \int_{t_0}^t f(x_m, \tau)
d\tau$. We've proven that this limit converges to $x^\ell$. What we want to
show is that if we evaluate $f(x^\ell, t)$, then $\int_{t_0}^t f(x_m, \tau)
\to \int_{t_0}^t f(x^\ell, \tau) d\tau$. Would be immediate if we had that
the function were continuous. Clear that it satisfies initial condition by
the construction of the sequence, but we need to show that it satisfies the
differential equation. Conceptually, this is probably more difficult than
what we've just done (establishing bounds, Cauchy sequences). Thinking
about what that function limit is and what it means for it to satisfy that
differential equation.

Now, you can basically use some of the machinery we've been using all along
to show this. Difference between these goes to $0$ as $m$ gets large.

$$\mag{\int_{t_0}^t (f(x_m, \tau) f(x^\ell, \tau)) d\tau}
\\ \le \int_{t_0}^t k(\tau) \mag{x_m - x^\ell} d\tau \le \bar{k}\mag{x_m - x^\ell}_\infty T
\\ \le \bar{k} M e^{\bar{k} T} \frac{(\bar{k} T)^m}{m!}T
$$

Thus $x^\ell$ solves the DE/IC pair. A solution $\Phi$ is $x^\ell$,
i.e. $x^\ell(t) = f(x^\ell, t) \forall [t_1, t_2] - D$ and $x^\ell(t_0) =
x_0$

To show that this solution is unique, we will use the Bellman-Gronwall
lemma, which is very important. Used ubiquitously when you want to show
that functions of time are equal to each other: candidate mechanism to do
that.

Bellman-Gronwall Lemma
----------------------
Let $u, k$ be real-valued positive piece-wise continuous functions of time,
and we'll have a constant $c_1 \ge 0$ and $t_0 \ge 0$. If we have such
constants and functions, then the following is true: if $u(t) \le c_1 +
\int_{t_0}^t k(\tau)u(\tau) d\tau$, then $u(t) \le c_1 e^{\int_{t_0}^t
k(\tau) d\tau}$.

Proof (of B-G)
--------------

$t > t_0$ WLOG.

$$U(t) = c_1 + \int_{t_0}^t k(\tau) u(\tau) d\tau
\\ u(t) \le U(t)
\\ u(t)k(t)e^{\int_{t_0}^t k(\tau) d\tau} \le U(t)k(t)e^{\int_{t_0}^t k(\tau) d\tau}
\\ \deriv{}{t}\parens{U(t)e^{\int_{t_0}^t k(\tau) d\tau}} \le 0 \text{(then integrate this derivative, note that U(t_0) = c_1)}
\\ u(t) \le U(t) \le c_1 e^{\int_{t_0}^t k(\tau) d\tau}
$$

Using this to prove uniqueness of DE/IC solutions
-------------------------------------------------
How we're going to use this to prove B-G lemma.

We have a solution that we constructed $\Phi$, and someone else gives us a
solution $\Psi$, constructed via a different method. Show that these must
be equivalent. Since they're both solutions, they have to satisfy the DE/IC
pair. Take the norm of the difference between the differential equations.

$$\mag{\Phi - \Psi} \le \bar{k} \int_{t_0}^t \mag{\Phi - \Psi} d\tau \forall
t_0, t \in [t_1, t_2]$$

From the Bellman-Gronwall Lemma, we can rewrite this inequality as
$\mag{\Phi - \Psi} \le c_1 e^{\bar{k}(t - t_0)}$. Since $c_1 = 0$, this
norm is less than or equal to 0. By positive definiteness, this norm must
be equal to 0, and so the functions are equal to each other.

Reverse time differential equation
----------------------------------

We think about time as monotonic (either increasing or decreasing, usually
increasing). Suppose that time is decreasing. $\exists \dot{x} =
f(x,t)$. Going backwards in time, explore existence and uniqueness going
backwards in time. Suppose we had a time variable $\tau$ which goes from
$t_0$ backwards, and defined $\tau \defequals t_0 - t$. We want to define
the solution to that differential equation backwards in time as $z(\tau) =
x(t)$ if $t < t_0$. Derive what reverse order time derivative is. Equation
is just $-f$; we're going to use $\bar{f}$ to represent this
function ($\deriv{}{\tau}z = -\deriv{}{t}x = -f(x, t) = -f(z, \tau) =
\bar{f}$).

This equation, if I solve the reverse time differential equation, we'll
have some corresponding backwards solution. Concluding statement: can think
about solutions forwards and backwards in time. Existence of unique
solution forward in time means existence of unique solution backward in
time (and vice versa). You can't have solutions crossing themselves in
time-invariant systems.
