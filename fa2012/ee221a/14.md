Jordan Form
===========
October 9, 2012
---------------

We just started talking about invariant subspaces. Midterm will be up to
and including lecture notes 12 (but we haven't gone through 11). Today I'd
like to continue in this vein that we've been presenting for the past
couple of lectures. What we've been doing is being motivated by our need to
efficiently compute $e^{At}$, and we've been thinking more generally about
how to compute functions of a square matrix $A$. What we've done in our
presentation is divide that problem into two classes: case 1 (where $A$ is
diagonalizable) -- that is, $A$ has $n$ linearly independent eigenvectors,
each of which is an element of $\mathbb{C}^n$. Remember that if $A$ has $n$
distinct eigenvalues, it is given that $A$ has $n$ eigenvectors. However,
we don't need $n$ distinct eigenvalues (consider degeneracy of quantum
states).

What we proved last time was a theorem that in this case, the eigenvectors
form a basis for $\mathbb{C}^n$, and we went ahead and constructed a
similarity transform which transforms $A$ to a diagonal matrix. Consider
$T^{-1}$, whose columns are the eigenvectors (since the columns form a
basis, it is invertible, and $T$ exists). $\Lambda = TAT^{-1}$, and
$\Lambda$, a diagonal matrix containing the corresponding eigenvalues.

Consider a function $\fn{f}{\mathbb{C}^{n\times n}}{\mathbb{C}^{n\times
n}}$. We talked about polynomial functions of a matrix (recall
Cayley-Hamilton theorem). Let us consider a more general case: $f$ is
**analytic**, that is, one that is locally given by a convergent power
series. We have that $f(A) = f(T^{-1}\Lambda T) = T^{-1}f(\Lambda)T$. What
does this mean? This is just $f$ applied to the diagonal matrix.

In general, $A$ is not diagonalizable, and so we have to think about what a
canonical form (a simple form for a general $n \times n$ matrix). We now
are considering case 2, where $A$ is not diagonalizable, i.e. $A$ has
strictly less than $n$ eigenvectors. This can only happen if $A$ has
repeated eigenvalues (note: not guaranteed!).

We will present the "generalization" of the diagonal form, which is called
Jordan form. Instead of using $\Lambda$, we'll use $J$. Basically, it'll be
an upper triangular matrix with the eigenvalues on the diagonal, and on the
off-diagonal, we'll have either ones or zeros (depending on how many
eigenvectors we have for a given eigenvalue). We call this a canonical form
(note that the diagonal form is a special case of the Jordan form when
there are n eigenvectors).

We're going to approach this today (by constructing a similarity
transform). In order to do that, we need a little bit of background: we
need to look in more detail at the eigenstructures of a matrix. We're going
to do that through this concept of $\mathcal{A}$-invariant
subspaces. Suppose you have a linear map $\fn{\mathcal{A}}{V}{V}$. $M$, a
subspace of $V$ is said to be **$\mathcal{A}$-invariant** (or invariant under
$\mathcal{A}$) if $\fn{\mathcal{A}}{M}{M}$.

Example: the nullspace of $\mathcal{A}$ is $\mathcal{A}$-invariant. Very
easy to show that a subspace is $\mathcal{A}$-invariant, typically.

Now consider $N(A - \lambda I)$, where $\lambda$ is an eigenvalue of
$A$. We know that this space contains the eigenvectors of $A$ that
correspond to $\lambda$; we can then show that this is also an invariant
subspace. A few more examples: if you have a polynomial function of a
matrix $A$, then the nullspace of this polynomial function is
$A$-invariant. Further, given two $A$-invariant subspaces, the intersection
of these subspaces is also $A$-invariant. Further, if you define sums of
subspaces in the usual manner, the sum of two $A$-invariant subspaces is
also $A$-invariant.

Dorect sum of subspaces
-----------------------

Again we have a vector space $X$, and we'll have $k$ subspaces $M_1, M_2,
..., M_K$ of $X$. $V$ is said to be a **direct sum** (i.e. $V = \Oplus
M_i$) if $\forall v \in V, \exists! m_i \in M_i \st v = \sum a_i m_i$. In
some sense, this is a generalization of linear independence.

A direct result of the uniqueness constraint of the definition of the
direct sum is that the intersection of any two of these subspaces contains
only the zero vector.

We need two more things before we consider the Jordan form of a matrix.

One way we can relate $A$-invariance and direct sum is going back to the
beginning of the class, if you look at $A \in \mathbb{C}^{n \times n}$ with
$n$ distinct eigenvalues (or even just $n$ linearly independent
eigenvectors), then $\mathbb{C}^n = \Oplus N(A-\lambda_i)$ (we proved last
time that in this case, the eigenvectors form a basis for the space). This
is a **direct sum decomposition** of $\mathbb{C}^n$.

So now let's go on to something quite important, which we generally call
the Second Representation Theorem. First, let's consider what
representation means: we define **representation** as the representation of
a linear map between finite-dimensional vector spaces as a matrix. Aside:
the first representation theorem was that we could construct a matrix
representation for any linear map between finite-dimensional vector
spaces.

Now we consider the case where $\fn{\mathcal{A}}{V}{V}$. Consider $V = M_1
\oplus M_2$, where $V$ has dimension $n$, $M_1$ has dimension $k$, and
$M_2$ has dimension $n-k$. If $M_1$ is $A$-invariant, we can say more about
what this matrix $A$ can look like: $\mathcal{A}$ can be represented as a
block matrix $A$ in which the lower left block is the zero matrix. The
upper left block is $k$-dimensional, and the remaining block sizes can be
inferred from this.

Easy to prove: can use first representation theorem. We'd like to prove
that there exists a basis for $V$ such that $A$ has the stated form. Begin
by constructing a basis for $M_1, M_2$. Let $\{b_1, ..., b_k\}$ be a basis
for $M_1$ and $\{b_{k+1}, ..., b_n\}$ be a basis for $M_2$. The image of
$A$ on the first $k$ basis vectors can be written as linear combinations of
the first $k$ basis vectors (i.e. you don't need any components involving
$b_{k+1}, ..., b_n$), and by uniqueness (guaranteed by this being a direct
sum decomposition), any representation with respect to this basis will
produce the lower-left block of zeros (by the first representation
theorem).

If $M_1, M_2$ are both $A$-invariant, then the top-right block can also be
zero (by symmetry); in fact, the basis we chose earlier will yield this
result. The proof is identical.

Example:

* Suppose $A$ has one eigenvector. I'm going to write then $\mathbb{C}^n
  = N(A - \lambda_1 I)$. Let us just understand this example as we leave
  today's class: $A$ is no longer diagonalizable. Going back to our second
  representation theorem and applying it to this case, we'll derive the
  Jordan form (not on the midterm).

* Suppose $A$ has $n$ eigenvectors. $\mathbb{C}^n = \Oplus_i N(A -
  \lambda_i I)$. By the second representation theorem, we are guaranteed that
  this matrix is diagonalizable.
