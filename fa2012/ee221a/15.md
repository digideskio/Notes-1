Jordan Form
===========
October 11, 2012
----------------

We're going to finish up talking about the case where we don't have $n$
eigenvectors. Recall that if we have $n$ distinct eigenvalues, we are
guaranteed $n$ eigenvectors, but this is not a necessary condition.

We'll then start an example, the linear quadratic regulator.

Recall: we defined the characteristic polynomial $\hat{\chi}_A(s)$ as
$\det(sI-A)$, which we rewrote as $\hat{\psi}_A(s) = \prod_i (s -
\lambda_i)^{d_i}$. We're using $d_i$ to represent the (algebraic)
multiplicity of eigenvalue $\lambda_i$. The Cayley-Hamilton theorem tells
us that the characteristic polynomial evaluated on the matrix $A$ is the
$n$-by-$n$ zero matrix.

Now we're going to define something new. We've been talking about the
characteristic polynomial, and we know its properties. The roots of the
characteristic polynomial are the eigenvalues of the matrix, and by
Cayley-Hamilton, every matrix satisfies its characteristic polynomial.

Let's now define something called the minimal polynomial of $A$, The
**minimal polynomial** $\hat{\psi}_A(s)$ is the polynomial of least-degree
such that $\hat{\psi}_A(A) = \theta_{n \times n}$. The question is whether
there is a polynomial of degree less than $n$ such that this identity is
satisfied. It is the structure of the characteristic polynomial and that of
the minimal polynomial that allows us to explore the Jordan form.

Just by definition, we have the result that $\hat{\psi}_A(s)$ divides
$\hat{\chi}_A(s)$. Proof: by the definition of "divides", we have a
remainder term $\hat{r}$; by linearity, $\hat{r}(A) = 0$. $\hat{r}$ must
either be lesser degree than $\hat{\psi}$ (which would violate our
definition), or it is identically the zero polynomial.

Thus, we can write $\hat{\psi}_A(s) = \prod_i (s - \lambda_i)^{m_i}$, $m_i
\le d_i$.

It turns out that the exponents $m_i$ in the minimal polynomial are
determined by the largest Jordan block associated with $\lambda_i$. (We
define a **Jordan block** to be a block of the Jordan form that contains
only ones as the super-diagonal elements.) In general, given $A$, how do we
find its minimal polynomial?

(thoughts: consider each Jordan block of $A - \lambda_i$ to be analogous to
a linear feedback shift register, with no new inputs; as such, it shows
that the largest Jordan block has nilpotency corresponding to the number of
registers, and all other Jordan blocks for the eigenvalue reach 0
earlier. Alternately, repeat the above while considering raising and
lowering operators)

**Theorem**: $\mathbb{C}^n = \Oplus_i N(A - \lambda_i I)^{m_i}$, where $N(A -
\lambda_i I)^{m_i}$ is of dimension $d_i$.

Recall: if there were $d_i$ eigenvectors associated to $\lambda_i$ for all
$i$, then the matrix would be diagonalizable. Generally, the things you add
to this form are called generalized eigenvectors.

Proof: We have the minimal polynomial $\hat{\psi}_A(s)$. Consider that
$\hat{\psi}_A(s) = \frac{1}{\prod_i (s - \lambda_i)^{m_i}} = \sum_j
\frac{\hat{n}_i(s)}{(s-\lambda_i)^{m_1}}}$. Now, multiplying both sides by
the minimal polynomial, we end up with $1 = \sum \hat{n}_1(s)\hat{p}_i(s)$,
as in the notes. Evaluating this polynomial at the matrix $A$, we get $I =
\sum \hat{n}_i(A)\hat{p_i}(A)$. Multiplying by an arbitrary $x \in
\mathbb{C}^n$, $x = \hat{n}_1(s)\hat{p}_i(s)x$, which we've written as the
sum of $\sigma$ terms.

In general, $x_i = \hat{n}_i(A)\hat{p}_i(A)x$. Since $\hat{p}_i(s)
=\frac{\hat{\psi}_A(s)}{(s - \lambda_i)^{m_i}}$, we know that $(A -
\lambda_i I)^{m_i}x_i = \theta$ since from the above, $x_i \in N(A -
\lambda_i I)^{m_i}$.

In order to prove the theorem, we must next show that this decomposition is
**unique** (meaning that for any vector $x$, there is not another way you
can break it up into $\sum x_i$; see notes), and that the nullity of $(A
- \lambda_i I)^{m_i} = d_i$, which is the multiplicity of $(s - \lambda_i)$ in
the characteristic polynomial.

As a concrete but slightly abstract example, we're going to look at the
geometric structure of eigenspaces.

Geometric Structure of Eigenspaces
----------------------------------
Consider $A$ with just one eigenvalue, whose characteristic polynomial is
given as defined above. In this case, $d_1 = n, 1 \le m_1 \le d_1$.

Suppose $n=6, m=3$. One question is whether you can uniquely determine a
Jordan form (no; we'll see this in a second). What we're saying from the
theorem we just proved is that we can decompose $\mathbb{C}^6 = N(A -
\lambda I)^3$. We don't have enough information yet to determine what the
Jordan form looks like, and how the characteristic and minimal polynomials
lead to the Jordan form.

Suppose, in addition, that $N(A - \lambda I) = \mathrm{span}\{e_1, e_2,
e_3\}$ (where $e_i$ is an eigenvector of $A$ associated to the eigenvalue
$\lambda$). Further, suppose that $N(A - \lambda I)^2 = N(A - \lambda I)
\oplus v_1 \oplus v_2$, where $v_1, v_2$ are linearly
independent solutions of $(A - \lambda I)$.

Then, suppose without loss of generality that $(A = \lambda I) v_1 = e_1$,
and $(A = \lambda I) v_2 = e_2$.

Finally, suppose $N(A - \lambda I)^3 = N(A - \lambda I)^2 \oplus w_1$,
where $(A - \lambda I)w_1 = v_1$ (without loss of generality, again).

In this case (and in general), $e_1$ is an eigenvector associated to
$\lambda$, $v_i$ is a generalized eigenvector (of degree 1) associated to
$\lambda, e_1$, and $w_1$ is a generalized eigenvector (of degree 2)
associated to $\lambda, e_1, v_1$.

$e_2$ is also an eigenvector associated to $\lambda$, and $v_2$ is also a
generalized eigenvector corresponding to $\lambda, e_1$.

Finally, $e_3$ is an eigenvector associated to $\lambda$.

As such, through these definitions, we can construct a similarity
transformation that gives us a canonical form for the matrix $A$, the
Jordan form.

Constructing the Jordan Form
----------------------------

$$
   Ae_1 = \lambda e_1
\\ Ae_2 = \lambda e_2
\\ Ae_3 = \lambda e_3
\\ Av_1 = \lambda v_1 + e_1
\\ Av_2 = \lambda v_2 + e_2
\\ Aw_1 = \lambda w_1 + v_1
$$

I know I can always construct these (in a linearly independent
fashion). Now, consider a similarity transform $J = TAT^{-1}$, where
$T^{-1} = \begin{pmatrix} e_1 & v_1 & w_1 & e_2 & v_2 & e_3
\end{pmatrix}$. I'm putting my eigenvectors with their generalized
eigenvectors. $J$ is the Jordan form of $A$, which is effectively a
diagonal matrix with some ones in the super-diagonal. This Jordan form
might have been moved around, so it's not unique. However, the Jordan form
is unique up to permutations of the Jordan blocks.

Note: we can't have multiple generalized eigenvectors that correspond to
the same (generalized) eigenvector, or else they are no longer linearly
independent with the generalized eigenvectors established thus far; we
require this chain structure.

What we've done is define a construct called the generalized
eigenvector. We'll start next Thursday's class with computing $e^{Jt}$.
