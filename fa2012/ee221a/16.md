Functions of Jordan Form, Introduction to Optimal Control
=========================================================
October 18, 2012
----------------

Alternative derivation of linear quadratic regulator; intro to optimal
control via LQR (Dr. Jerry Ding).

Today's a bit of a different lecture: finish up the last note we have in
this functions of a matrix. If we want to compute an arbitrary analytic
function of a matrix (which we showed we could generally transform to its
Jordan canonical form), how do we do that? We'll then go on to optimal
control.

For the first five or ten minutes, let's talk about this Jordan form and
where we ended last day.

Functions of a matrix ($n\times n$) in Jordan Form
--------------------------------------------------

From what we derived last day, we can write down a minimal polynomial, and
recall that the exponent corresponds to the largest Jordan block for that
polynomial. This Jordan form could have come (generally) from a matrix $A$,
and we could have computed $J$ through the similarity transform we
discussed last day. We recall that the columns of $T^{-1}$ are the
eigenvectors and generalized eigenvectors. The number of Jordan blocks
tells you the number of eigenvectors of the matrix.

$f(A) = f(T^{-1} J T) = T^{-1} f(J) T$. We claim that if you have an
analytic function $f$ (i.e. locally given by a convergent power series). If
we have the Jordan form and an analytic function, we have the result that
if we compute $f(J)$ (which we can do by applying $f$ to each of the Jordan
blocks), we can use these results to compute $f(A)$.

So we look at each Jordan block, and when we compute $f$ of each Jordan
block, the diagonal elements are just the function applied to the
eigenvalues, they're all diagonal matrices, where all entries to the right
of the diagonal are nth derivatives of $f$, evaluated at the eigenvalues.

If in general you were looking at a Jordan block, the claim is as follows:
the elements are of the following structure:

$$(T^{-1}_k)_{ij} = f^{[j - i]}(\lambda_k)\theta(j - i + \epsilon)$$

In the notes, we prove it -- this is a very interesting proof, but it's
very involved and doesn't add much to our discussion. I'll refer you to the
notes; we use the minimal polynomial structure and what's called the method
of interpolating polynomials.

This claim gives us an important theorem, which we can pull off from this
result, the **spectral mapping theorem**, which tells us something about
the eigenvalues of a function of a matrix. Recall we used $\sigma$ to
represent the spectrum (set of eigenvalues). This theorem says that
$\sigma(f(J)) = f(\sigma(J))$. Specifically we see this from the Jordan
form, and since we know that we can always construct this matrix, we can
consider that this applies to all matrices.

This is probably one of our most powerful tools: we can look at the
eigenvalues of the matrix, and by the spectral mapping theorem, we can
consider the effect of the function of the eigenvalues to figure something
out about the matrix.

Another thing: 10:18 is apparently the California shake-down day. We
practice what we're supposed to do in the event of an earthquake.

Linear Quadratic Optimization
-----------------------------

You have some linear time-varying system $x(t) = A(t)x(t) + B(t)u(t)$ with
initial condition $x(0) = x_0$. Consider our state to be in $\Re^n, u \in
\Re^{n_i}, t \in [t_0, t_1]$. Our objective function in this case will be
the integral of a penalty on control plus a penalty on state plus a
terminal cost, i.e. $J(u) = \frac{1}{2}\bracks{\int_{t_0}^{t_1}
(\mag{u(t)}^2_2 + \mag{C(t)x(t)}^2_2) dt + x^T(t_1)Sx(t_1)}$. In some
sense, we want to penalize the deviation from the system.

Our optimization space $U$ here is a set of piecewise-continuous
functions. Obviously not differentiable at points of discontinuity, so it
satisfies the differential equation almost everywhere.

The quadratic optimization problem is this: compute the optimal cost of our
objective function, subject to LTV, i.e. $J^* = \min_{u \in U} J(u)$.

What are the applications? e.g. stabilization and minimum energy control,
set point tracking. More interestingly, you can do trajectory tracking (one
slight problem is you also have some dynamics in the trajectory that you
want to track; this changes the structure of the controller a little bit,
but all the ideas are the same).

Finally, an interesting problem is LQG (linear quadratic gaussian)
control. Suppose you have a plant, which is linear, pluss some gaussian
noise. With some input, we get some output (and in general you don't want
to observe the full state vector). Estimate system's state based on
measurements and compute control. With the LQG problem, you're minimizing
the control. We have a Kalman filter fed directly into a LQR. This is one
of the famous instances of the equivalence principle, i.e. you can do the
control completely separately. This is one of the few cases in stochastic
control where you actually have this equivalence principle.

There are typically two types of solutions: either go by dynamic
programming or optimality condition. So what does optimality condition
mean?

Suppose we have a continuously differentiable scalar function on Euclidean
space, and we want to solve $\min_{x \in \Re^n} \phi(x)$. If $x$ is
optimal, then the gradient of the cause function is 0 (i.e. $\nabla \phi(x)
= 0$. Your optimal point must be one of your critical points; you don't
know which one you're working with. In a lot of numerical optimization
problems, what you're trying to do is find the gradient; you're trying to
proceed in the direction of least cost. However, you can get stuck in a
local minimum.

Equivalently, a similar condition should hold. Equivalent form: the
directional derivative $\nabla_v\phi(x) = 0 \forall v \in \Re^n$ (what
we're doing is taking an $\epsilon$ deviation along this direction and
figuring out what the perturbation is). Ideally, you'd like to find a
similar derivative for the cost function, but you've got a few problems:
(1) perturbation in $u$. The nice thing about $U$ is that it's a vector
space. We can now define our perturbation as follows: $u_\epsilon = u +
\epsilon\delta u, \delta u \in U$.

Next question: how does the cause vector change as a result of control? The
trajectory of the system with this input is $x_2(t) \Phi(t,t_0)\x_0 +
\int_{t_0}^t \Phi(t, \tau) B(\tau)(u(\tau) + \epsilon\delta u(\tau))d\tau =
x(t) + \epsilon \int_{t_0}^{t_1} \Phi(t,\tau)B(\tau) \delta u(\tau) d\tau$;
we can call this second term $\delta x(t)$, which we then refer to as our
perturbation of the state trajectory. Basically, this is the zero-state
response.

$\delta \dot{x} = A(t) \delta x(t) + B(t) \delta u(t); \delta x(0) = 0$. 

$$J(u + \epsilon\delta u = \frac{1}{2}\bracks{\int_{t_0}^{t_1}\mag{u +
\epsilon \delta u}^2_2 + \mag{C(x + \epsilon \delta x)} dt + (x(t_1) +
\epsilon \delta x(t_1))^T S (x(t_1) + \epsilon \delta x(t_1))} = J(u) +
\epsilon F(u, \delta u) + \epsilon^2 Q(\delta u)$$

What you see is that if you introduce an $\epsilon$ perturbation in your
input, you get an $\epsilon$ perturbation in your output. As such, the
directional derivative is well-defined, and can be found explicitly as
$F(u, \delta u)$. This is the first variation of $J$. This first variation
we can write down in an analogous manner to the static analysis we talked
about.

$u$ is optimal $\iff \nabla_{\delta u}J(u) = 0, \forall \delta u \in U$
(this is two-way because it is a convex optimization problem).

Two parts: first assume $u$ is optimal. This implies that $\forall \delta
u, \epsilon, J(u + \epsilon\delta u) \ge J(u)$. which means that $J(u +
\epsilon \delta u) - J(u) = \epsilon(F(u, \delta u) + \epsilon Q(\delta u))
\ge 0$. Case 1: $\epsilon > 0$, so $\nabla_{\delta u}J(u) + \epsilon
Q(\delta u) \ge 0 \implies \nabla_{\delta u} J(u) \ge 0$. In case 2,
$\epsilon < 0 \implies \nabla_{\delta u} J(u) + \epsilon Q(\delta u) \le 0
\implies \nabla_{\delta u} J(u) \le 0 \implies \nabla_{\delta u} J(u) = 0$.

For the other direction, assume $\nabla_{\delta u} J(u) = 0, \forall \delta
u$. $J(u + \epsilon \delta u) = J(u) + \epsilon \nabla_{\delta u}J(u) +
\epsilon^2 Q(\delta u) \ge J(u)$ ($\forall \delta u, \epsilon$), so $u$ is
optimal.

This is where we deviate from the approach in Callier and Desoer. Because
of this result, what we're really interested in is this first
variation. Now I'll write down more explicitly what that cross-term
is. $\nabla_{\delta u} J(u) = \int_{t_0}^{t_1} \bracks{u(t)^T \delta u(t) +
x(t)^T C(t)^TC(t)\delta x(t)} dt + x(t_1)^T S\delta x(t_1)$.

We'll begin by introducing a quantity that we call the control Hamiltonian:
$H(x, p, u, t) = p^T(A(t)x + B(t)u) + t_2(u^Tu + x^TC(t)^T C(t)x)$. $p$ is
basically your costate. The thing to note here is that your original system
equation came from $\dot{x} = \nabla_p H(x, p, u, t)$. The costate is
basically if you took the negative gradient with respect to $x$, i.e. $p =
-\nabla _x H(x, p, u, t)$. This comes from classical mechanics and
conservation of energy. It's sort of intuitive as an optimal control
problem that there's a coupling relationship between your dynamics and your
system, and you're trying to descend to the state of lowest cost
(minimizing the action). Instead of solving the dynamic optimization
problem, all we're going to minimize is our Hamiltonian.

I'll try to give an informal derivation for now. Here, $\dot{p}(t) =
-A^T(t) p(t) - C(t)^TC(t) x(t)$; $p(t_1) = \nabla_x \parens{\frac{1}{2}
x^T(t_1) S(x(t_1))} = Sx(t_1)$. What we do now is take a time derivative of
$p(t)^T \delta x(t)$. When you evaluate this derivative, something magical
sort of happens: you get $p(t)^T \delta x(t) + p(t)^T\delta x(t)$. What you
can show from this is if you just plug this in, $= -x(t)^T C(t)^T C(t)
\delta x(t) + p(t)^T B(t) \delta u(t)$. If you integrate both sides of this
expression from $t_0$ to $t_1$, you get $p(t_1)^T \delta x(t_1) - p(t_0)^T
\delta x(t_0) = \int_{t_0}^{t_1} {p(t)^T B(t) \delta u(t) - x(t)^T C(t)^T
C(t) \delta x(t)} dt$.

Rearranging terms, this basically becomes $x(t_1)^T S x(t_1) +
\int_{t_0}^{t_1} x(t)^T C(t)^T C(t) \delta x(t) dt = \int{t_0}^{t_1} p(t)^T
B(t) \delta u(t) dt$. Essentially, we can rewrite the first variation as
$\nabla_{\delta u} J(u) = \int_{t_0}^{t_1} \parens{u(t)^T + p(t)^T B(t)}
\delta u(t) dt$. If $u$ is optimal, then $\int_{t_0}^{t_1} \mag{u(t)^T +
p(t)^T B(t)}^2_2 dt = 0$, namely $\nabla_u H(x, p, u, t) = 0$. This
informal proof doesn't tell you what the critical points are.

If you go through a more formal proof, we'd apply Pontryagin's minimum
principle: if $u$ is optimal, let $(x,p)$ be corresponding state and
costate trajectories. Along this trajectory, $H(x(t), p(t), u(t), t) =
\min_{u \in \Re^{n_i}} H(x(t), p(t), u,t)$. Basically, you get a feedback
function in terms of $x$. However, this is only a necessary condition,
so you only have a set of possible solutions.

Applying the minimum principle, the unique solution to this problem $\min_u
H(x, p, u, t)$ is $U^* = -B^T p$. Basically, what this says is by the
minimum principle, if $u$ is optimal, then it must have this form. If you
knew nothing about optimal control, and you just wrote down the
Hamiltonian, this gives you the form, if it exists at all.

You then have to go back and see which value works, and then going back to
the other problem (which is both necessary and sufficient, since this is a
convex optimization problem), then you can verify the solution.

This is basically the main result for this linear quadratic optimization
problem. Theorem 2.1.157 from Callier and Desoer: 

**The optimal solution to (LQ) is $u(t) = -B(t)^Tp(t)$, for almost every $t
  \in [t_0, t_1]$, where (x,p) is the solution of $\dot{x} = A(t)x(t) +
  B(t)B(t)^Tp(t), x(t_0) = x_0$ and $\dot{p} = C(t)^TC(t)x(t) - A(t)^T
  p(t), p(t_1) = Sx(t_1)$. So this is basically the structure of the
  controller.**

The rest of the proof tries to find a more tractable form that you can
compute. Because in general this two-point boundary problem is hard to
solve, you want to turn this into an initial value or terminal condition
problem. You can represent your co-state as a linear function of your state
$p(t) = P(t)x(t)$; what you get out is a linear state feedback law: $u(t) =
-B(t)^TP(t) x(t)$. This matrix then evolves as follows: $-\dot{P}(t) =
A(t)^T P(t) + P(t) A(t) - P(t)B(t)B(t)^T P(t) + C(t)^T C(t), P(t_1) =
S$.

The thing to note is that this equation is derived from the theorem.
