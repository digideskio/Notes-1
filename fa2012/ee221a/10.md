EE 221A: Linear System Theory
=============================
September 25, 2012
------------------
Linear time-varying systems
---------------------------
Recall the state transition function is given some function of the current
time with initial state, initial time, and inputs, Suppose you have a
differential equation; how do you acquire the state transition function?
Solve the differential equation.

For a general dynamical system, there are different ways to get the state
transition function. This is an instantiation of a dynamical system, and
we're going to ge thte state transition function by solving the
differential equation / initial condition pair. 

We're going to call $\dot{x}(t) = A(t)x(t) + B(t)u(t)$ a vector
differential equation with initial condition $x(t_0) = x_0$.

So that requires us to think about solving that differential equation. Do a
dimension check, to make sure we know the dimensions of the matrices. $x
\in \Re^n$, so $A \in \Re^{n_0 \times n}$. We could define the matrix
function $A$, which takes intervals of the real line and maps them over to
matrices. As a function, $A$ is piecewise continuous matrix function in
time.

The entries are piecewise-continuous scalars in time. We would like to get
at the state transition function; to do that, we need to solve the
differential equation.

Let's assume for now that $A, B, U$ are given (part of the system
definition).

Piece-wise continuous is trivial; we can use the induced norm of $A$ for a
Lipschitz condition. Since this induced norm is piecewise-continuous in
time, this is a fine bound. Therefore $f$ is globally Lipschitz continuous.

We're going to back off for a bit and introduce the state transition
matrix. Background for solving the VDE. We're going to introduce a matrix
differential equation, $\dot{X} = A(t) X$ (where $A(t)$ is same as before).

I'm going to define $\Phi(t, t_0)$ as the solution to the matrix
differential equation (MDE) for the initial condition $\Phi(t_0, t_0) =
1_{n \times n}$. I'm going to define $\Phi$ as the solution to the $n
\times n$ matrix when my differential equation starts out in the identity
matrix.

Let's first talk about properties of this matrix $\Phi$ just from the
definition we have.

 * If you go back to the vector differential equation, and let's just drop
   the term that depends on $u$ (either consider $B$ to be 0, or the input
   to be 0), the solution of $\cdot{x} = A(t)x(t)$ is given by $x(t) =
   \Phi(t, t_0)x_0$.
 * This is what we call the semigroup property, since it's reminiscent of
   the semigroup axiom. $\Phi(t, t_0) = \Phi(t, t_1) \Phi(t_1, t_0) \forall
   t, t_0, t_1 \in \Re^+$
 * $\Phi^{-1}(t, t_0) = \Phi(t_0, t)$.
 * $\text{det} \Phi(t, t_0) = \exp\parens{\int_{t_0}^t \text{tr} \parens{A
   (\tau)} d\tau}$.

Here's let's talk about some machinery we can now invoke when
we want to show that two functions of time are equal to each other when
they're both solutions to the differential equation. You can simply show by
the existence and uniqueness theorem (assuming it applies) that they
satisfy the same initial condition and the same differential
equation. That's an important point, and we tend to use it a lot.

(i.e. when faced with showing that two functions of time are equal to each
other, you can show that they both satisfy the same initial condition and
the same differential equation [as long as the differential equation
satisfies the hypotheses of the existence and uniqueness theorem])

Obvious, but good to state.

Note: the initial condition doesn't have to be the initial condition given;
it just has to hold at one point in the interval. Pick your point in time
judiciously.

Proof of (2): check $t=t_1$. (3) follows directly from (2). (4) you can
look at if you want. Gives you a way to compute $\Phi(t, t_0)$. We've
introduced a matrix differential equation and an abstract solution.

Consider (1). $\Phi(t, t_0)$ is a map that takes the initial state and
transitions to the new state. Thus we call $\Phi$ the **state transition
matrix** because of what it does to the states of this vector differential
equation: it transfers them from their initial value to their final value,
and it transfers them through matrix multiplication.

Let's go back to the original differential equation. Claim that the
solution to that differential equation has the following form: $x(t) =
\Phi(t, t_0)x_0 + \int_{t_0}^t \Phi(t, \tau)B(\tau)u(\tau) d\tau$. Proof:
we can use the same machinery. If someone gives you a candidate solution,
you can easily show that it is the solution.

Recall the Leibniz rule, which we'll state in general as follows:
$\pderiv{}{z} \int_{a(z)}^{b^z} f(x, z) dx = \int_{a(z)}^{b^z}
\pderiv{}{x}f(x, z) dx + \pderiv{b}{z} f(b, z) - \pderiv{a}{z} f(a, z}$.

$$
\dot{x}(t) & = A(t) \Phi(t, t_0) x_0 + \int_{t_0}^t
\pderiv{}{t} \parens{\Phi(t, \tau)B(\tau)u(\tau)} d\tau +
\pderiv{t}{t}\parens{\Phi(t, t)B(t)u(t)} - \pderiv{t_0}{t}\parens{...}
\\ & = A(t)\Phi(t, t_0)x_0 + \int_{t_0}^t A(t)\Phi(t,\tau)B(\tau)u(\tau)d\tau + B(t)u(t)
\\ & = A(\tau)\Phi(t, t_0) x_0 + A(t)\int_{t_0}^t \Phi(t, \tau)B(\tau)
u(\tau) d\tau + B(t) u(t)
\\ & = A(\tau)\parens{\Phi(t, t_0) x_0 + \int_{t_0}^t \Phi(t, \tau)B(\tau)
u(\tau) d\tau} + B(t) u(t)
$$

$x(t) = \Phi(t,t_0)x_0 + \int_{t_0}^t \Phi(t,\tau)B(\tau)u(\tau) d\tau$ is
good to remember.


Not surprisingly, it depends on the input function over an interval of
time.

The differential equation is changing over time, therefore the system
itself is time-varying. No way in general that will be time-invariant,
since the equation that defines its evolution is changing. You test
time-invariance or time variance through the response map. But is it
linear? You have the state transition function, so we can compute the
response function (recall: readout map composed with the state transition
function) and ask if this is a linear map.
