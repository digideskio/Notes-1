CS 262A: Advanced Topics in Computer Systems
============================================
September 12, 2012
------------------
ARIES: Logging and Recovery
---------------------------

Guest speaker: Dr. Fekete. Researcher in transaction management (collection
of techniques one puts inside the database to support the app program by
having a notion of ACID transactions. Recovery not focus (mostly on
concurrency control).

Also does not know everything in this paper. Immensely complicated and
hairy paper. Very strange because at the time this was written, many people
thought that transaction management was a solved problem. People had
written textbooks, gone through all the exposition -- five years before
this paper's initial version, so eight years before it appeared in the
journals. Panel discussion in this period where some db researchers were
wondering whether they should give up db research (is it done?). This paper
came along, but it said that "yes, we've got solutions, but they're
nonoptimal." Radically different way of doing things. This is what people
now think about, not previous techniques.

The paper was written from a situation where there were all of these
techniques out there (some in industry, others in textbooks, and the two
communities did not communicate much). This was written entirely by
industry people.

The trick that people learn in uni regarding raising the level of
abstraction does not happen here.

What I'm going to try to do in this account is try to show you what (I
think) the key ideas are, and some optimizations they added on.

First, a few things. System R was a very influential research prototype,
but it was never turned into production. Not the basis. This paper talks a
lot about shadow pages. Nobody built shadow pages in production. System R
people realized that this was not the right way to do things, and they
threw it out. Everyone went ahead to write-ahead logging.

The key thing we need to have is the picture of the db platform as split
into two parts: at the top, the query engine -- the bit that gets SQL that
understands it and parses it and optimizes; a lower level, called the
storage engine; and the interface across here is typically record / read /
write / insert / scan. It's basically what we now call a key-value
store. Essentially tables accessed by primary key, and an additional
operation scan, which can scan through one of those tables.

Storage engine doesn't know about SQL: not declarative; very much
procedural. This paper is talking about that part of the system. Another
thing: role of the transaction. Transaction is concept provided to app
layer. Grouping bunch of SQL statements and logic into a single
control. Properties (i.e. ACID). These are passed on down to storage.

What the storage engine is doing is implementation of transaction
information. How to provide transactions to the higher layer.

ACID properties: atomicity, consistency, isolation, durability. Not even
real because consistency is different because of the others; no part of the
system guarantees this: application's job to guarantee this.

What's this paper looking at? Dealing with two failure modes; make sure
system fails well. One is failure of a single application program (may have
executed DIV by 0, done something silly), and the capacity to do that (if
there is a failure) to set everything back as if nothing ever happened is
the I in ACID. The other failure is the possibility that the system
crashes. DBs are large and complicated; they crash, and the DB is running
on an OS (also large and complicated); it crashes. The goal here is that
the transactions that have committed even after crash should still be there
(D). The ones that had not completed should be like aborted
transactions. So application program doesn't have to worry about data in
inconsistent state halfway through. Job is at end, data is
consistent. Whether there's a failure in the transaction, or something at
the end, it should be consistent at the end.

Turning that into a spec, if you will, of an algorithm (recovery manager,
more generally) is that the value of each data item should be what was put
there by the most recent update among all of the committed transactions.

This system design is based on a number of sort of architectural
assumptions, the first is that updates are happening in-place. Many DB
systems now are multi-version approaches. The System R shadow page approach
was also like that. If someone modifies data, it's modified
destructively. Records with keys are close together? Will still be the case
after these records have been updated. Assume that enough concurrency
control is happening so that we have essentially the write locks are taken
on items. Turns out we don't need to worry about read locks, but the write
locks are there.

Finally, this sort of overall architecture that we have is that the data
locations are on-disk, but for performance, the actual operations are done
in buffering memory. Operated in memory, at some point flushed back to disk
if modified. Turns out that this aspect (data with authoritative state on
disk but most recent state in memory) is what makes recovery hard.

Challenges: REDO, UNDO. Schemes that predated ARIES that people talked
about as ways of doing things. A scheme described as Force, which says that
when the transaction commits, you take every page the transaction had
written, you push that back to disk. Means you certainly don't have to do
any redo, the trouble is that commits will be incredibly slow. Suppose
you've written dozen of items, you'll be seeking all over the disk, and
that will be a disaster. The other is called No steal or pinning; you don't
do this until after the commit (prevent flush from happening before
commit). Poor performance, however. Desire is to have neither of these:
decouple commit and flushing.

Basic idea: logging. Record REDU and UNDO information, for every update, in
a log. Just put into the log a description of what's going on. One of the
nice things is that the log is a sequential data structure (no issues with
random access to disk). In the log, you say which transactions do what
(change which data).

Write-ahead logging is a bunch of rules (again, this long predates ARIES)
is that to be able to REDO and UNDO, 

* the log record must be forced to the disk before the data page in which
  that update happens goes to the disk.
* must write all log records for a transaction before commit.

First (undo) rule allows system to have atomicity; second (redo) rule
allows system to have durability.

These are here because if you didn't have them, you wouldn't necessarily be
able to have an undo (or redo) when you needed it.

Key ideas of ARIES (italicized are novel to ARIES):

* Log every change (*even undos during transaction abort*)
* In restart, *first repeat history* without backtracking.
   ** *Even redo the actions of loser transactions.*
* *Then undo* actions of losers.
* LSNs (log sequence numbers) in pages used to coordinate state between
  log, buffer, disk.

One slight complexity that we should mention is that the log is mostly kept
on disk except for the part that's currently active (in memory), and
there's a background process that moves stuff between memory and disk. Some
indication that process has gotten far enough; notion of synchronizing. Can
turn WAL constraints into rules about how LSNs line up. (path on log
up-to-date enough to reflect everything on a page). Details don't matter:
crucial thing is that each log failed gives you information about what
happened.

As mentioned earlier, crucial idea is to log undos. The record that
describes that undo is a CLR (compensation log record). Log is
sequential. You want to be able to trace by what happened in a previous
transaction; transaction is the link that lets you step between states.

Also table that shows what is kept in memory; dirty page table. Standard OS
stuff, but again, you want to keep track of the LSNs. The page itself has
an LSN (the last modification to happen). You also keep something called
the record LSN (which says what first made a page dirty).

So. Checkpointing is something the paper spends a lot of time on. It is an
optimization which is done to enable the restart process to be
faster. Checkpointing is a system-wide thing that keeps track of which
pages are where; save-pointing is a per-transaction thing that allows
partial rollback (allow us to rollback part of a transaction).

People say the log is the real truth about the system. Notion of building
structures into log. Even in systems that don't go that far, log is still
ultimate truth. You have data on disk, memory where the buffers are and
bookkeeping information.

Remember, one of the keys to ARIES is that you log undos. You need these
CLRs if you're going to repeat history properly. What we do here is what
we've described for transaction reboot. These undos happen. In the reset
process, you repeat the compensation exactly at the same place in the
sequence.

Monotonically increasing CLRs is important. Shows some pathological crash
cases, but if you crash and start undoing stuff, you can start undoing
undoes and redoing redoes, etc. Several rapid crashes during recovery can
be problematic if you don't have the CLRs. Can guarantee that you will
never undo anything more than once.

What people often do is "group commit", i.e. buffering commits together so
you can push several transactions at once (reduce delay).

Redo happens in three phases: analysis phase (works out what's dirty, which
transactions are losers), redo (repeat history --go to earliest page that
needs to be cleaned up), undo phase.

Analysis phase is not necessary, but it turns out to give you a lot of
efficiencies, which means that you don't have to do as much work in the
redo phase.

Redo phase includes repeating everything. If the LSN is further into the
future than this log record, don't do anything; else, apply log record and
increase LSN up. Each page moves forward in time as you go through this,
and the LSN gives you this key invariant (where you're at).

At end of log, every page is up to date.

Undo phase: take all losers and roll them back. Can get lots of parallelism
out of this; eventually you want to make sure you undo something only
once. As you write the CLRs, as you write the compensation, if you then
have to undo this thing, you can do multiple undos at once.

Gives you this property that you can ensure that you've made progress in
the case of multiple crashes. Taken from paper's pseudocode.

Does deal with a bunch of cases of crashes. Need to think about all of the
places where you could crash, and it turns out that the authors did all of
this.

All of these algorithms are designed to be internally multithreaded. Whole
notion of latching. "Locks" in databases are generally used to prevent
transactions from being concurrent. "Latches" are what OS people generally
call locks (used to prevent system threads from being concurrent).

Filesystem is assumed to act on pages atomically.

This algorithm has a sort of simple core, but they've thought about a lot
of things to make sure that it should perform well. A lot of parallelism
can happen, and a lot of latching to protect all those threads going on.

The last thing I want to take about: state. Logical state. If I increment a
variable, what happens? The simplest form of logging is simply physical
logging. You describe the old state, new state. Redo set to new, undo set
to old. Or just delta (old xor new). One thing I want you to be thinking
about: a key thing we need is idempotence. xor is not idempotent; why is
this okay? xor is guarded by the check on the LSN makes this
acceptable. Logical logging describes the operation and arguments; each of
these needs an inverse. When I say program, I don't mean application
program; someone has to have written an inverse program.

What's very common is actually what people call physiological
logging. Physical to page, logical within page. Similar to design in DBs
where things move around within page. That works really quite well: doing
logical redo can get quite hairy; doing everything purely physically, log
records can get quite big. Most commonly in ARIES, redo is physiologically,
and undo is done logically.

Typical means is that item might not be in same place: might have moved
between pages. Fine because when you repeat history, the data is in the
right place.

Couple of things to think about: if you read the paper, it's not just a
recovery algorithm. Designed with deep awareness of access methods
(e.g. B-trees) and concurrency control. Systems mesh together. Tangled ball
of string there. Buffer management is almost separated (fairly nice API
there where you can say "don't let that page out until this flush has
occurred", and that's all you need, but access method and everything else,
all entangled.

Reason for repeating history is to make fine-grained locking work. To make
mdofiying records work concurrently. Done purely for performance. If not
done, you can have significant performance penalties. You want to have
that. And the simplicity of repeating history gives you the capacity to do
that. Two novel things about this paper: repeating history is a huge
change, and these types of CLRs made for a high-performing (note: no actual
performance evaluation) system.

So how would you do an experiment to try to evaluate this? What are you
comparing it to? This is a complicated basic algorithm with a whole bunch
of optimizations. You could compare it to itself (with optimizations turned
on or off), but how do you compare it to something not in this class at
all? Very hard to do.
